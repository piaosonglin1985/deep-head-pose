Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@incollection{Reinholtz2009,
author = {Reinholtz, Charles and Hong, Dennis and Wicks, Al and Bacha, Andrew and Bauman, Cheryl and Faruque, Ruel and Fleming, Michael and Terwelp, Chris and Alberi, Thomas and Anderson, David and Cacciola, Stephen and Currier, Patrick and Dalton, Aaron and Farmer, Jesse and Hurdus, Jesse and Kimmel, Shawn and King, Peter and Taylor, Andrew and {Van Covern}, David and Webster, Mike},
doi = {10.1007/978-3-642-03991-1_4},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reinholtz et al. - 2009 - Odin Team VictorTango's Entry in the DARPA Urban Challenge(2).pdf:pdf},
pages = {125--162},
publisher = {Springer Berlin Heidelberg},
title = {{Odin: Team VictorTango's Entry in the DARPA Urban Challenge}},
url = {http://link.springer.com/10.1007/978-3-642-03991-1{\_}4},
year = {2009}
}
@article{Mitzel2011,
abstract = {Classical tracking-by-detection approaches require a robust object detector that needs to be executed in each frame. However the detector is typically the most computationally expensive component, especially if more than one object class needs to be detected. In this paper we investigate how the usage of the object detector can be reduced by using stereo range data for following detected objects over time. To this end we propose a hybrid tracking framework consisting of a stereo based ICP (Iterative Closest Point) tracker and a high-level multi-hypothesis tracker. Initiated by a detector response, the ICP tracker follows individual pedestrians over time using just the raw depth information. Its output is then fed into the high-level tracker that is responsible for solving long-term data association and occlusion handling. In addition, we propose to constrain the detector to run only on some small regions of interest (ROIs) that are extracted from a 3D depth based occupancy map of the scene. The ROIs are tracked over time and only newly appearing ROIs are evaluated by the detector. We present experiments on real stereo sequences recorded from a moving camera setup in urban scenarios and show that our proposed approach achieves state of the art performance.},
author = {Mitzel, Dennis and Leibe, Bastian},
doi = {10.1109/ICCVW.2011.6130357},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitzel, Leibe - 2011 - Real-time multi-person tracking with detector assisted structure propagation(2).pdf:pdf},
isbn = {9781467300629},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {974--981},
title = {{Real-time multi-person tracking with detector assisted structure propagation}},
year = {2011}
}
@article{Xiang,
abstract = {In this work, we focus on the problem of tracking objects un- der significant viewpoint variations, which poses a big challenge to tradi- tional object tracking methods. We propose a novel method to track an object and estimate its continuous pose and part locations under severe viewpoint change. In order to handle the change in topological appear- ance introduced by viewpoint transformations, we represent objects with 3D aspect parts and model the relationship between viewpoint and 3D aspect parts in a part-based particle filtering framework. Moreover, we show that instance-level online-learned part appearance can be incorpo- rated into our model, which makes it more robust in difficult scenarios with occlusions. Experiments are conducted on a new dataset of chal- lenging YouTube videos and a subset of the KITTI dataset [14] that include significant viewpoint variations, as well as a standard sequence for car tracking.We demonstrate that our method is able to track the 3D aspect parts and the viewpoint of objects accurately despite significant changes in viewpoint.},
author = {Xiang, Yu and Song, Changkyu and Mottaghi, Roozbeh and Savarese, Silvio},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiang et al. - Unknown - Monocular Multiview Object Tracking with 3D Aspect Parts(2).pdf:pdf},
keywords = {3d aspect part representation,multiview object tracking},
title = {{Monocular Multiview Object Tracking with 3D Aspect Parts}}
}
@incollection{Rosten2006,
author = {Rosten, Edward and Drummond, Tom},
booktitle = {Proceedings of the 9th European conference on Computer Vision - Volume Part I},
doi = {10.1007/11744023_34},
pages = {430--443},
publisher = {Springer-Verlag},
title = {{Machine Learning for High-Speed Corner Detection}},
url = {http://link.springer.com/10.1007/11744023{\_}34},
year = {2006}
}
@article{Zhang2015,
abstract = {layer filtering 중간에 low-level feature를 결합하여 decision tree를 boost시켜 pedestrian을 detect 하는 방법 제안},
archivePrefix = {arXiv},
arxivId = {1501.05759v1},
author = {Zhang, Shanshan and Benenson, Rodrigo and Schiele, Bernt},
doi = {10.1109/CVPR.2015.7298784},
eprint = {1501.05759v1},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Benenson, Schiele - 2015 - Filtered Channel Features for Pedestrian Detection(2).pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Cvpr},
pages = {1751--1760},
title = {{Filtered Channel Features for Pedestrian Detection}},
year = {2015}
}
@article{Benenson2015,
abstract = {Quantitative measures of the space an individual can reach is essential for tracking the progression of a disease and the effects of therapeutic intervention. The reachable workspace can be used to track an individuals' ability to perform activities of daily living, such as feeding and grooming. There are few methods for quantifying upper limb performance, none of which are able to generate a reachable workspace volume from motion capture data. We introduce a method to estimate the reachable workspace volume for an individual by capturing their observed joint limits using a low cost depth camera. This method is then tested on seven individuals with varying upper limb performance. Based on these initial trials, we found that the reachable workspace volume decreased as muscular impairment increased. This shows the potential for this method to be used as a quantitative clinical assessment tool.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.4304v1},
author = {Benenson, Rodrigo and Omran, Mohamed and Hosang, Jan and Schiele, Bernt},
doi = {10.1007/978-3-319-16181-5_47},
eprint = {arXiv:1411.4304v1},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Benenson et al. - 2015 - Ten years of pedestrian detection, what have we learned(2).pdf:pdf},
isbn = {9783319161808},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {613--627},
title = {{Ten years of pedestrian detection, what have we learned?}},
volume = {8926},
year = {2015}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, David G},
doi = {http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94},
eprint = {0112017},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowe - 2004 - Distinctive image features from scale invariant keypoints(2).pdf:pdf},
isbn = {1568811012},
issn = {0920-5691},
journal = {Int'l Journal of Computer Vision},
pages = {91--11020042},
pmid = {20064111},
primaryClass = {cs},
title = {{Distinctive image features from scale invariant keypoints}},
url = {http://portal.acm.org/citation.cfm?id=996342},
volume = {60},
year = {2004}
}
@article{TehraniNiknejad2012,
author = {{Tehrani Niknejad}, Hossein and Takeuchi, Akihiro and Mita, Seiichi and McAllester, David},
doi = {10.1109/TITS.2012.2187894},
issn = {1524-9050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
month = {jun},
number = {2},
pages = {748--758},
title = {{On-Road Multivehicle Tracking Using Deformable Object Model and Particle Filter With Improved Likelihood Estimation}},
url = {http://ieeexplore.ieee.org/document/6172683/},
volume = {13},
year = {2012}
}
@article{Romera2016,
abstract = {Autonomous driving is a challenging topic that requires complex solutions in perception tasks such as recognition of road, lanes, traffic signs or lights, vehicles and pedestrians. Through years of research, computer vision has grown capable of tackling these tasks with monocular detectors that can provide remarkable detection rates with relatively low processing times. However, the recent appearance of Convolutional Neural Networks (CNNs) has revolutionized the computer vision field and has made possible approaches to perform full pixel-wise semantic segmentation in times close to real time (even on hardware that can be carried on a vehicle). In this paper, we propose to use full image segmentation as an approach to simplify and unify most of the detection tasks required in the perception module of an autonomous vehicle, analyzing major concerns such as computation time and detection performance.},
archivePrefix = {arXiv},
arxivId = {1607.00971},
author = {Romera, Eduardo and Bergasa, Luis M. and Arroyo, Roberto},
eprint = {1607.00971},
month = {jul},
title = {{Can we unify monocular detectors for autonomous driving by using the pixel-wise semantic segmentation of CNNs?}},
url = {http://arxiv.org/abs/1607.00971},
year = {2016}
}
@article{Paden2016,
author = {Paden, Brian and Cap, Michal and Yong, Sze Zheng and Yershov, Dmitry and Frazzoli, Emilio},
doi = {10.1109/TIV.2016.2578706},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paden et al. - 2016 - A Survey of Motion Planning and Control Techniques for Self-Driving Urban Vehicles(2).pdf:pdf},
issn = {2379-8904},
journal = {IEEE Transactions on Intelligent Vehicles},
month = {mar},
number = {1},
pages = {33--55},
title = {{A Survey of Motion Planning and Control Techniques for Self-Driving Urban Vehicles}},
url = {http://ieeexplore.ieee.org/document/7490340/},
volume = {1},
year = {2016}
}
@inproceedings{Hassan2005,
address = {Reston, Virigina},
author = {Hassan, Rania and Cohanim, Babak and de Weck, Olivier and Venter, Gerhard},
booktitle = {46th AIAA/ASME/ASCE/AHS/ASC Structures, Structural Dynamics and Materials Conference},
doi = {10.2514/6.2005-1897},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hassan et al. - 2005 - A Comparison of Particle Swarm Optimization and the Genetic Algorithm(2).pdf:pdf},
isbn = {978-1-62410-065-9},
month = {apr},
publisher = {American Institute of Aeronautics and Astronautics},
title = {{A Comparison of Particle Swarm Optimization and the Genetic Algorithm}},
url = {http://arc.aiaa.org/doi/abs/10.2514/6.2005-1897},
year = {2005}
}
@inproceedings{Bay2006,
author = {Bay, Herbert and Tuytelaars, Tinne and {Van Gool}, Luc},
booktitle = {Eccv},
doi = {10.1007/11744023_32},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bay, Tuytelaars, Van Gool - 2006 - SURF Speeded Up Robust Features(2).pdf:pdf},
pages = {404--417},
publisher = {Springer Berlin Heidelberg},
title = {{SURF: Speeded Up Robust Features}},
url = {http://link.springer.com/10.1007/11744023{\_}32},
year = {2006}
}
@article{Maresca2013,
author = {Maresca, Mario Edoardo and Petrosino, Alfredo and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and {Pandu Rangan}, C and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y and Weikum, Gerhard},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maresca et al. - 2013 - MATRIOSKA A Multi-level Approach to Fast Tracking by Learning(2).pdf:pdf},
isbn = {978-3-642-41183-0 978-3-642-41184-7},
journal = {Image Analysis and Processing – ICIAP 2013},
keywords = {Cited},
number = {i},
pages = {419--428},
title = {{MATRIOSKA: A Multi-level Approach to Fast Tracking by Learning}},
url = {http://link.springer.com/10.1007/978-3-642-41184-7{\_}43},
volume = {8157},
year = {2013}
}
@article{Kittipanya-Ngam2011,
author = {Kittipanya-Ngam, Panachit and Lung, Eng How},
doi = {10.1007/978-3-642-22822-3_15},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kittipanya-Ngam, Lung - 2011 - HOG-based descriptors on rotation invariant human detection(2).pdf:pdf},
isbn = {9783642228216},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART1},
pages = {143--152},
title = {{HOG-based descriptors on rotation invariant human detection}},
volume = {6468 LNCS},
year = {2011}
}
@article{Ruiz2017,
abstract = {Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the target face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detection performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to determine pose by training a multi-loss convolutional neural network on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from image intensities through joint binned pose classification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose methods. We open-source our training and testing code as well as release our pre-trained models.},
archivePrefix = {arXiv},
arxivId = {1710.00925},
author = {Ruiz, Nataniel and Chong, Eunji and Rehg, James M.},
eprint = {1710.00925},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruiz, Chong, Rehg - 2017 - Fine-Grained Head Pose Estimation Without Keypoints.pdf:pdf},
month = {oct},
title = {{Fine-Grained Head Pose Estimation Without Keypoints}},
url = {http://arxiv.org/abs/1710.00925},
year = {2017}
}
@article{Gao2014,
author = {Gao, Jin and Ling, Haibin and Hu, Weiming and Xing, Junliang},
doi = {10.1007/978-3-319-10578-9_13},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gao et al. - 2014 - Transfer Learning Based Visual Tracking with Gaussian Process Regression(2).pdf:pdf},
isbn = {9783319105772},
issn = {16113349},
journal = {Eccv},
pages = {188--203},
title = {{Transfer Learning Based Visual Tracking with Gaussian Process Regression}},
year = {2014}
}
@article{Hariyono2014,
abstract = {This paper presents a pedestrian detection method from a moving vehicle using optical flows and histogram of oriented gradients (HOG). A moving object is extracted from the relative motion by segmenting the region representing the same optical flows after compensating the egomotion of the camera. To obtain the optical flow, two consecutive images are divided into grid cells 14 × 14 pixels; then each cell is tracked in the current frame to find corresponding cell in the next frame. Using at least three corresponding cells, affine transformation is performed according to each corresponding cell in the consecutive images, so that conformed optical flows are extracted. The regions of moving object are detected as transformed objects, which are different from the previously registered background. Morphological process is applied to get the candidate human regions. In order to recognize the object, the HOG features are extracted on the candidate region and classified using linear support vector machine (SVM). The HOG feature vectors are used as input of linear SVM to classify the given input into pedestrian/nonpedestrian. The proposed method was tested in a moving vehicle and also confirmed through experiments using pedestrian dataset. It shows a significant improvement compared with original HOG using ETHZ pedestrian dataset.},
author = {Hariyono, Joko and Hoang, Van-Dung and Jo, Kang-Hyun},
doi = {10.1155/2014/196415},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hariyono, Hoang, Jo - 2014 - Moving object localization using optical flow for pedestrian detection from a moving vehicle(2).pdf:pdf},
issn = {1537-744X},
journal = {TheScientificWorldJournal},
keywords = {gram of oriented gradients,histo-,motion segmentation,optical flow,pedestrian detection},
pages = {196415},
pmid = {25114955},
title = {{Moving object localization using optical flow for pedestrian detection from a moving vehicle.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4121190{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {2014},
year = {2014}
}
@article{Broggi2013,
abstract = {This paper presents the vision of the Artificial Vision and Intelligent Systems Laboratory (VisLab) on future automated vehicles, ranging from sensor selection up to their extensive testing. VisLab's design choices are explained using the BRAiVE autonomous vehicle prototype as an example. BRAiVE, which is specifically designed to develop, test, and demonstrate advanced safety applications with different automation levels, features a high integration level and a low-cost sensor suite, which are mainly based on vision, as opposed to many other autonomous vehicle implementations based on expensive and invasive sensors. The importance of performing extensive tests to validate the design choices is considered to be a hard requirement, and different tests have been organized, including an intercontinental trip from Italy to China. This paper also presents the test, the main challenges, and the vehicles that have been specifically developed for this test, which was performed by four autonomous vehicles based on BRAiVE's architecture. This paper also includes final remarks on VisLab's perspective on future vehicles' sensor suite.},
author = {Broggi, Alberto and Buzzoni, Michele and Debattisti, Stefano and Grisleri, Paolo and Laghi, Maria Chiara and Medici, Paolo and Versari, Pietro},
doi = {10.1109/TITS.2013.2262331},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Broggi et al. - 2013 - Extensive tests of autonomous driving technologies(2).pdf:pdf},
isbn = {1524-9050 VO  - 14},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {ADAS,HMI,autonomous vehicles,robot navigation,sensor integration,vision-based guidance},
number = {3},
pages = {1403--1415},
title = {{Extensive tests of autonomous driving technologies}},
volume = {14},
year = {2013}
}
@article{Nebehay2014,
abstract = {We propose a novel keypoint-based method for long-term model-free object tracking in a combined matching-and- tracking framework. In order to localise the object in every frame, each keypoint casts votes for the object center. As erroneous keypoints are hard to avoid, we employ a novel consensus-based scheme for outlier detection in the voting behaviour. To make this approach computationally feasible, we propose not to employ an accumulator space for votes, but rather to cluster votes directly in the image space. By transforming votes based on the current keypoint constel- lation, we account for changes of the object in scale and rotation. In contrast to competing approaches, we refrain from updating the appearance information, thus avoiding the danger of making errors. The use of fast keypoint detec- tors and binary descriptors allows for our implementation to run in real-time. We demonstrate experimentally on a diverse dataset that is as large as 60 sequences that our method outperforms the state-of-the-art when high accu- racy is required and visualise these results by employing a variant of success plots.},
author = {Nebehay, Georg and Pflugfelder, Roman},
doi = {10.1109/WACV.2014.6836013},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nebehay, Pflugfelder - 2014 - Consensus-based matching and tracking of keypoints for object tracking(2).pdf:pdf},
isbn = {9781479949854},
journal = {2014 IEEE Winter Conference on Applications of Computer Vision, WACV 2014},
pages = {862--869},
title = {{Consensus-based matching and tracking of keypoints for object tracking}},
year = {2014}
}
@article{Broggi1999,
author = {Broggi, Alberto and Broggi, Alberto and Bertozzi, Massimo and Fascioli, Alessandra and Guarino, Corrado and Bianco, Lo and Piazzi, Aurelio},
journal = {INTERNATIONAL JOURNAL OF INTELLIGENT CONTROL AND SYSTEMS},
pages = {409----441},
title = {{The argo autonomous vehicle's vision and control systems}},
year = {1999}
}
@article{Saffari2010,
abstract = {Online boosting is one of the most successful online learning algorithms in computer vision. While many challenging online learning problems are inherently multi-class, online boosting and its variants are only able to solve binary tasks. In this paper, we present Online Multi-Class LPBoost (OMCLP) which is directly applicable to multi-class problems. From a theoretical point of view, our algorithm tries to maximize the multi-class soft-margin of the samples. In order to solve the LP problem in online settings, we perform an efficient variant of online convex programming, which is based on primal-dual gradient descent-ascent update strategies. We conduct an extensive set of experiments over machine learning benchmark datasets, as well as, on Caltech 101 category recognition dataset. We show that our method is able to outperform other online multi-class methods. We also apply our method to tracking where, we present an intuitive way to convert the binary tracking by detection problem to a multi-class problem where background patterns which are similar to the target class, become virtual classes. Applying our novel model, we outperform or achieve the state-of-the-art results on benchmark tracking videos.},
author = {Saffari, Amir and Godec, Martin and Pock, Thomas and Leistner, Christian and Bischof, Horst},
doi = {10.1109/CVPR.2010.5539937},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saffari et al. - 2010 - Online multi-class LPBoost(2).pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3570--3577},
title = {{Online multi-class LPBoost}},
year = {2010}
}
@article{Stein2010,
abstract = {This paper presents a top-down approach to stereo for use in driver assistance systems. We introduce an asymmetric configuration where monocular object detection and range estimation is performed in the primary camera and then that image patch is aligned and matched in the secondary camera. The stereo distance measure from the matching assists in target verification and improved distance measurements. This approach, Stereo-Assist, shows significant advantages over the classical bottom-up stereo approach which relies on first computing a dense depth map and then using the depth map for object detection. The new approach can provide increased object detection range, reduced computational load, greater flexibility in camera configurations (we are no longer limited to side-by-side stereo configurations), greater robustness to obstructions in part of the image and mixed camera modalities FIR/VIS can be used. We show results with two novel configurations and illustrate how monocular object detection allows for simple online calibration of the stereo rig.},
author = {Stein, Gideon P. and Gdalyahu, Yoram and Shashua, Amnon},
doi = {10.1109/IVS.2010.5548019},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stein, Gdalyahu, Shashua - 2010 - Stereo-assist Top-down stereo for driver assistance systems(2).pdf:pdf},
isbn = {9781424478668},
issn = {1931-0587},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
pages = {723--730},
title = {{Stereo-assist: Top-down stereo for driver assistance systems}},
year = {2010}
}
@inproceedings{Pellegrini2009,
author = {Pellegrini, S and Ess, A and Schindler, K and van Gool, L},
booktitle = {2009 IEEE 12th International Conference on Computer Vision},
doi = {10.1109/ICCV.2009.5459260},
isbn = {978-1-4244-4420-5},
month = {sep},
pages = {261--268},
publisher = {IEEE},
title = {{You'll never walk alone: Modeling social behavior for multi-target tracking}},
url = {http://ieeexplore.ieee.org/document/5459260/},
year = {2009}
}
@misc{VOLVO2014,
author = {VOLVO},
title = {{City safety}},
url = {https://en.wikipedia.org/wiki/City{\_}safety},
year = {2014}
}
@article{Benenson2011,
abstract = {Mobile robots require object detection and classification for safe and smooth navigation. Stereo vision improves such detection by doubling the views of the scene and by giving indirect access to depth information. This depth information can also be used to reduce the set of candidate detection windows. Up to now, most algorithms compute a depth map to discard unpromising detection windows. We propose a novel approach where a stixel world model is computed directly from the stereo images, without computing an intermediate depth map. We experimentally demonstrate that such approach can considerably reduce the set of candidate detection windows at a fraction of the computation cost of previous approaches.},
author = {Benenson, Rodrigo and Timofte, Radu and {Van Gool}, Luc},
doi = {10.1109/ICCVW.2011.6130495},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Benenson, Timofte, Van Gool - 2011 - Stixels estimation without depth map computation(2).pdf:pdf},
isbn = {9781467300629},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2010--2017},
title = {{Stixels estimation without depth map computation}},
year = {2011}
}
@article{Xu2011,
abstract = {Classification-based pedestrian detection systems (PDSs) are currently a hot research topic in the field of intelligent transportation. A PDS detects pedestrians in real time on moving vehicles. A practical PDS demands not only high detection accuracy but also high detection speed. However, most of the existing classification-based approaches mainly seek for high detection accuracy, while the detection speed is not purposely optimized for practical application. At the same time, the performance, particularly the speed, is primarily tuned based on experiments without theoretical foundations, leading to a long training procedure. This paper starts with measuring and optimizing detection speed, and then a practical classification-based pedestrian detection solution with high detection speed and training speed is described. First, an extended classification/detection speed metric, named feature-per-object (fpo), is proposed to measure the detection speed independently from execution. Then, an fpo minimization model with accuracy constraints is formulated based on a tree classifier ensemble, where the minimum fpo can guarantee the highest detection speed. Finally, the minimization problem is solved efficiently by using nonlinear fitting based on radial basis function neural networks. In addition, the optimal solution is directly used to instruct classifier training; thus, the training speed could be accelerated greatly. Therefore, a rapid and accurate classification-based detection technique is proposed for the PDS. Experimental results on urban traffic videos show that the proposed method has a high detection speed with an acceptable detection rate and a false-alarm rate for onboard detection; moreover, the training procedure is also very fast.},
author = {Xu, Yanwu and Cao, Xianbin and Qiao, Hong},
doi = {10.1109/TSMCB.2010.2046890},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Cao, Qiao - 2011 - An efficient tree classifier ensemble-based approach for pedestrian detection(2).pdf:pdf},
issn = {10834419},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
keywords = {Efficient classification,false-positive rate (FPR),pedestrian detection,performance evaluation,radial basis function (RBF) neural network},
number = {1},
pages = {107--117},
pmid = {20457550},
title = {{An efficient tree classifier ensemble-based approach for pedestrian detection}},
volume = {41},
year = {2011}
}
@article{Yan2014,
author = {Yan, Xu and Kakadiaris, Ioannis A and Shah, Shishir K},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yan, Kakadiaris, Shah - 2014 - What Do I See Modeling Human Visual Perception for Multi-person Tracking(2).pdf:pdf},
isbn = {978-3-319-10605-2; 978-3-319-10604-5},
journal = {Eccv},
pages = {1--16},
title = {{What Do I See? Modeling Human Visual Perception for Multi-person Tracking}},
year = {2014}
}
@inproceedings{Geiger2012a,
author = {Geiger, A. and Lenz, P. and Urtasun, R.},
booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6248074},
isbn = {978-1-4673-1228-8},
month = {jun},
pages = {3354--3361},
publisher = {IEEE},
title = {{Are we ready for autonomous driving? The KITTI vision benchmark suite}},
url = {http://ieeexplore.ieee.org/document/6248074/},
year = {2012}
}
@article{Andriyenko2012,
abstract = {The problem of multi-target tracking is comprised of two distinct, but tightly coupled challenges: (i) the naturally discrete problem of data association, i.e. assigning image observations to the appropriate target; (ii) the naturally continuous problem of trajectory estimation, i.e. recovering the trajectories of all targets. To go beyond simple greedy solutions for data association, recent approaches often perform multi-target tracking using discrete optimization. This has the disadvantage that trajectories need to be pre-computed or represented discretely, thus limiting accuracy. In this paper we instead formulate multi-target tracking as a discrete-continuous optimization problem that handles each aspect in its natural domain and allows leveraging powerful methods for multi-model fitting. Data association is performed using discrete optimization with label costs, yielding near optimality. Trajectory estimation is posed as a continuous fitting problem with a simple closed-form solution, which is used in turn to update the label costs. We demonstrate the accuracy and robustness of our approach with state-of-the-art performance on several standard datasets.},
author = {Andriyenko, Anton and Schindler, Konrad and Roth, Stefan},
doi = {10.1109/CVPR.2012.6247893},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andriyenko, Schindler, Roth - 2012 - Discrete-continuous optimization for multi-target tracking(2).pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {June},
pages = {1926--1933},
title = {{Discrete-continuous optimization for multi-target tracking}},
year = {2012}
}
@article{Dehghan2015,
abstract = {Data association is the backbone to many multiple ob- ject tracking (MOT) methods. In this paper we formulate data association as a Generalized Maximum Multi Clique problem (GMMCP). We show that this is the ideal case of modeling tracking in real world scenario where all the pair- wise relationships between targets in a batch of frames are taken into account. Previous works assume simplified ver- sion of our tracker either in problem formulation or prob- lem optimization. However, we propose a solution using GMMCPwhere no simplification is assumed in either steps. We show that the NP hard problem of GMMCP can be for- mulated through Binary-Integer Program where for small and medium size MOT problems the solution can be found efficiently. We further propose a speed-up method, employ- ing Aggregated Dummy Nodes for modeling occlusion and miss-detection, which reduces the size of the input graph without using any heuristics. We show that, using the speed- up method, our tracker lends itself to real-time implementa- tion which is plausible in many applications. We evaluated our tracker on six challenging sequences of Town Center, TUD-Crossing, TUD-Stadtmitte, Parking-lot 1, Parking-lot 2 and Parking-lot pizza and show favorable improvement against state of art. 1.},
author = {Dehghan, Afshin and {Modiri Assari}, Shayan and Shah, Mubarak},
doi = {10.1109/CVPR.2015.7299036},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dehghan, Modiri Assari, Shah - 2015 - GMMCP Tracker Globally Optimal Generalized Maximum Multi Clique Problem for Multiple Object Tra(2).pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{GMMCP Tracker: Globally Optimal Generalized Maximum Multi Clique Problem for Multiple Object Tracking}},
year = {2015}
}
@inproceedings{Bradski1998,
author = {Bradski, G.R.},
booktitle = {Proceedings Fourth IEEE Workshop on Applications of Computer Vision. WACV'98 (Cat. No.98EX201)},
doi = {10.1109/ACV.1998.732882},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bradski - 1998 - Real time face and object tracking as a component of a perceptual user interface(2).pdf:pdf},
isbn = {0-8186-8606-5},
keywords = {CAMSHIFT,Computer vision,Face detection,Games,Histograms,Humans,Layout,Probability distribution,Robustness,Time sharing computer systems,User interfaces,computer vision,graphical user interfaces,histogram based methods,human faces,mean shift algorithm,perceptual user interface,probability distributions,real time face and object tracking,real-time systems},
pages = {214--219},
publisher = {IEEE Comput. Soc},
title = {{Real time face and object tracking as a component of a perceptual user interface}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=732882},
year = {1998}
}
@article{Dalal2005,
author = {Dalal, Navneet and Triggs, Bill},
doi = {10.1109/CVPR.2005.177},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dalal, Triggs - 2005 - Histograms of oriented gradients for human detection(2).pdf:pdf},
issn = {1063-6919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords = {feature extraction;gradient methods;object detecti},
pages = {886--893},
title = {{Histograms of oriented gradients for human detection}},
volume = {1},
year = {2005}
}
@article{Breitenstein2009,
abstract = {We propose a novel approach for multi-person tracking-by-detection in a particle filtering framework. In addition to final high-confidence detections, our algorithm uses the continuous confidence of pedestrian detectors and online trained, instance-specific classifiers as a graded observation model. Thus, generic object category knowledge is complemented by instance-specific information. A main contribution of this paper is the exploration of how these unreliable information sources can be used for multi-person tracking. The resulting algorithm robustly tracks a large number of dynamically moving persons in complex scenes with occlusions, does not rely on background modeling, and operates entirely in 2D (requiring no camera or ground plane calibration). Our Markovian approach relies only on information from the past and is suitable for online applications. We evaluate the performance on a variety of datasets and show that it improves upon state-of-the-art methods.},
author = {Breitenstein, Michael D. and Reichlin, Fabian and Leibe, Bastian and Koller-Meier, Esther and {Van Gool}, Luc},
doi = {10.1109/ICCV.2009.5459278},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breitenstein et al. - 2009 - Robust tracking-by-detection using a detector confidence particle filter(2).pdf:pdf},
isbn = {9781424444205},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
number = {Iccv},
pages = {1515--1522},
title = {{Robust tracking-by-detection using a detector confidence particle filter}},
year = {2009}
}
@article{Teixeira2010,
abstract = {An increasingly common requirement of computer systems is to extract information regarding the people present in an environment. In this article, we provide a comprehensive, multi-disciplinary survey of the existing literature, focusing mainly on the extraction of five commonly needed spatio- temporal properties: namely presence, count, location, track and identity. We discuss a new taxonomy of observable human properties and physical traits, along with the sensing modalities that can be used to extract them. We compare active vs. passive sensors, and single-modality vs. sensor fusion approaches, in instrumented vs. uninstrumented settings, surveying sensors as diverse as cameras, motion sensors, pressure pads, radars, electric field sensors, and wearable inertial sensors, among others. The goal of this work is to summarize the existing solutions from various disciplines, to guide the creation of new systems and point toward future research directions.},
author = {Teixeira, Thiago and Dublon, Gershon and Savvides, Andreas},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Teixeira, Dublon, Savvides - 2010 - A Survey of Human-Sensing Methods for Detecting Presence, Count, Location, Track, and Identity(2).pdf:pdf},
issn = {01468030},
journal = {ACM Computing Surveys},
pages = {1--35},
title = {{A Survey of Human-Sensing: Methods for Detecting Presence, Count, Location, Track, and Identity}},
volume = {5},
year = {2010}
}
@article{SzegedyREA14,
archivePrefix = {arXiv},
arxivId = {1412.1441},
author = {Szegedy, Christian and Reed, Scott E and Erhan, Dumitru and Anguelov, Dragomir},
eprint = {1412.1441},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2014 - Scalable, High-Quality Object Detection(2).pdf:pdf},
journal = {CoRR},
title = {{Scalable, High-Quality Object Detection}},
url = {http://arxiv.org/abs/1412.1441},
volume = {abs/1412.1},
year = {2014}
}
@article{Milanes2012,
abstract = {Intelligent systems designed to reduce highway fatalities have been widely applied in the automotive sector in the last decade. Of all users of transport systems, pedestrians are the most vulnerable in crashes as they are unprotected. This paper deals with an autonomous intelligent emergency system designed to avoid collisions with pedestrians. The system consists of a fuzzy controller based on the time-to-collision estimate - obtained via a vision-based system - and the wheel-locking probability - obtained via the vehicle's CAN bus - that generates a safe braking action. The system has been tested in a real car - a convertible Citro??n C3 Pluriel - equipped with an automated electro-hydraulic braking system capable of working in parallel with the vehicle's original braking circuit. The system is used as a last resort in the case that an unexpected pedestrian is in the lane and all the warnings have failed to produce a response from the driver. ?? 2012 Elsevier Ltd. All rights reserved.},
author = {Milan{\'{e}}s, Vicente and Llorca, David F. and Villagr{\'{a}}, Jorge and P{\'{e}}rez, Joshue and Parra, Ignacio and Gonz{\'{a}}lez, Carlos and Sotelo, Miguel A.},
doi = {10.1016/j.eswa.2012.03.047},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Milan{\'{e}}s et al. - 2012 - Vision-based active safety system for automatic stopping(2).pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Accident detection,Anti-lock braking system,Collision avoidance,Intelligent control,Pedestrian protection,Stereovision},
number = {12},
pages = {11234--11242},
title = {{Vision-based active safety system for automatic stopping}},
volume = {39},
year = {2012}
}
@article{Nam2014,
author = {Nam, Hyeonseob and Hong, Seunghoon and Han, Bohyung},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nam, Hong, Han - 2014 - Online Graph-Based Tracking(2).pdf:pdf},
isbn = {978-3-319-10602-1; 978-3-319-10601-4},
journal = {Eccv},
keywords = {online},
pages = {112--126},
title = {{Online Graph-Based Tracking}},
volume = {8693},
year = {2014}
}
@article{Farneb2003,
abstract = {This paper presents a novel two-frame motion estimation algorithm. The first step is to approximate each neighborhood of both frames by quadratic polynomials, which can be done efficiently using the polynomial expansion transform. From observing how an exact polynomial transforms under translation a method to estimate displacement fields from the polynomial expansion coefficients is derived and after a series of refinements leads to a robust algorithm. Evaluation on the Yosemite sequence shows good results.},
author = {Farneb, Gunnar},
doi = {10.1007/3-540-45103-X_50},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Farneb - 2003 - Two-Frame Motion Estimation Based on Polynomial Expansion(2).pdf:pdf},
isbn = {978-3-540-40601-3},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
number = {1},
pages = {363--370},
title = {{Two-Frame Motion Estimation Based on Polynomial Expansion}},
volume = {2749},
year = {2003}
}
@article{Liu2015,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300\backslashtimes 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500\backslashtimes 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
archivePrefix = {arXiv},
arxivId = {1512.02325},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {1512.02325},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2015 - SSD Single Shot MultiBox Detector(2).pdf:pdf},
month = {dec},
title = {{SSD: Single Shot MultiBox Detector}},
url = {http://arxiv.org/abs/1512.02325 http://dx.doi.org/10.1007/978-3-319-46448-0{\_}2},
year = {2015}
}
@article{Ess2009a,
author = {Ess, A and Schindler, K and Leibe, B and Gool, L Van},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ess et al. - 2009 - Improved Multi-Person Tracking with Active Occlusion Handling(2).pdf:pdf},
journal = {ICRA 2009 workshop},
title = {{Improved Multi-Person Tracking with Active Occlusion Handling}},
year = {2009}
}
@article{Dollar2014,
abstract = {Multi-resolution image features may be approximated via extrapolation from nearby scales, rather than being computed explicitly. This fundamental insight allows us to design object detection algorithms that are as accurate, and considerably faster, than the state-of-the-art. The computational bottleneck of many modern detectors is the computation of features at every scale of a finely- sampled image pyramid. Our key insight is that one may compute finely sampled feature pyramids at a fraction of the cost, without sacrificing performance: for a broad family of features we find that features computed at octave-spaced scale intervals are sufficient to approximate features on a finely-sampled pyramid. Extrapolation is inexpensive as compared to direct feature computation. As a result, our approximation yields considerable speedups with negligible loss in detection accuracy. We modify three diverse visual recognition systems to use fast feature pyramids and show results on both pedestrian detection (measured on the Caltech, INRIA, TUD-Brussels and ETH datasets) and general object detection (measured on the PASCAL VOC). The approach is general and is widely applicable to vision algorithms requiring fine-grained multi-scale analysis. Our approximation is valid for images with broad spectra (most natural images) and fails for images with narrow band-pass spectra (e.g. periodic textures). Index},
author = {Doll{\'{a}}r, Piotr and Appel, Ron and Belongie, Serge and Perona, Pietro},
doi = {10.1109/TPAMI.2014.2300479},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doll{\'{a}}r et al. - 2014 - Fast feature pyramids for object detection(2).pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Visual features,image pyramids,natural image statistics,object detection,pedestrian detection,real-time systems},
number = {8},
pages = {1532--1545},
pmid = {26353336},
title = {{Fast feature pyramids for object detection}},
volume = {36},
year = {2014}
}
@article{Hare2012,
abstract = {Efﬁcient keypoint-based object detection methods are$\backslash$r$\backslash$nused in many real-time computer vision applications. These$\backslash$r$\backslash$napproaches often model an object as a collection of keypoints and associated descriptors, and detection then involves ﬁrst constructing a set of correspondences between$\backslash$r$\backslash$nobject and image keypoints via descriptor matching, and$\backslash$r$\backslash$nsubsequently using these correspondences as input to a robust geometric estimation algorithm such as RANSAC to$\backslash$r$\backslash$nﬁnd the transformation of the object in the image. In such$\backslash$r$\backslash$napproaches, the object model is generally constructed of-$\backslash$r$\backslash$nﬂine, and does not adapt to a given environment at runtime.$\backslash$r$\backslash$nFurthermore, the feature matching and transformation estimation stages are treated entirely separately. In this paper,$\backslash$r$\backslash$nwe introduce a new approach to address these problems by$\backslash$r$\backslash$ncombining the overall pipeline of correspondence generation and transformation estimation into a single structured$\backslash$r$\backslash$noutput learning framework.$\backslash$r$\backslash$nFollowing the recent trend of using efﬁcient binary descriptors for feature matching, we also introduce an approach to approximate the learned object model as a collection of binary basis functions which can be evaluated very$\backslash$r$\backslash$nefﬁciently at runtime. Experiments on challenging video$\backslash$r$\backslash$nsequences show that our algorithm signiﬁcantly improves$\backslash$r$\backslash$nover state-of-the-art descriptor matching techniques using$\backslash$r$\backslash$na range of descriptors, as well as recent online learning$\backslash$r$\backslash$nbased approaches.},
author = {Hare, Sam and Saffari, Amir and Torr, Philip H S},
doi = {10.1109/CVPR.2012.6247889},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hare, Saffari, Torr - 2012 - Efficient online structured output learning for keypoint-based object tracking(2).pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1894--1901},
title = {{Efficient online structured output learning for keypoint-based object tracking}},
year = {2012}
}
@article{Horn1981,
author = {Horn, Berthold K.P. and Schunck, Brian G.},
doi = {10.1016/0004-3702(81)90024-2},
issn = {00043702},
journal = {Artificial Intelligence},
month = {aug},
number = {1-3},
pages = {185--203},
publisher = {Elsevier},
title = {{Determining optical flow}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0004370281900242},
volume = {17},
year = {1981}
}
@article{Wu2016,
abstract = {The trend towards increasingly deep neural networks has been driven by a general observation that increasing depth increases the performance of a network. Recently, however, evidence has been amassing that simply increasing depth may not be the best way to increase performance, particularly given other limitations. Investigations into deep residual networks have also suggested that they may not in fact be operating as a single deep network, but rather as an ensemble of many relatively shallow networks. We examine these issues, and in doing so arrive at a new interpretation of the unravelled view of deep residual networks which explains some of the behaviours that have been observed experimentally. As a result, we are able to derive a new, shallower, architecture of residual networks which significantly outperforms much deeper models such as ResNet-200 on the ImageNet classification dataset. We also show that this performance is transferable to other problem domains by developing a semantic segmentation approach which outperforms the state-of-the-art by a remarkable margin on datasets including PASCAL VOC, PASCAL Context, and Cityscapes. The architecture that we propose thus outperforms its comparators, including very deep ResNets, and yet is more efficient in memory use and sometimes also in training time. The code and models are available at https://github.com/itijyou/ademxapp},
archivePrefix = {arXiv},
arxivId = {1611.10080},
author = {Wu, Zifeng and Shen, Chunhua and van den Hengel, Anton},
eprint = {1611.10080},
month = {nov},
title = {{Wider or Deeper: Revisiting the ResNet Model for Visual Recognition}},
url = {http://arxiv.org/abs/1611.10080},
year = {2016}
}
@article{Choi2013,
abstract = {In this paper, we present a general framework for tracking multiple, possibly interacting, people from a mobile vision platform. To determine all of the trajectories robustly and in a 3D coordinate system, we estimate both the camera's ego-motion and the people's paths within a single coherent framework. The tracking problem is framed as finding the MAP solution of a posterior probability, and is solved using the reversible jump Markov chain Monte Carlo (RJ-MCMC) particle filtering method. We evaluate our system on challenging datasets taken from moving cameras, including an outdoor street scene video dataset, as well as an indoor RGB-D dataset collected in an office. Experimental evidence shows that the proposed method can robustly estimate a camera's motion from dynamic scenes and stably track people who are moving independently or interacting.},
author = {Choi, Wongun and Pantofaru, Caroline and Savarese, Silvio},
doi = {10.1109/TPAMI.2012.248},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Pantofaru, Savarese - 2013 - A general framework for tracking multiple people from a moving camera(2).pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Multitarget tracking,RJ-MCMC particle filtering,people tracking,person detection},
number = {7},
pages = {1577--1591},
pmid = {23681988},
title = {{A general framework for tracking multiple people from a moving camera}},
volume = {35},
year = {2013}
}
@article{Sun2003,
author = {Sun, Y and Fisher, R},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun, Fisher - 2003 - Object-based attention for computer vision(2).pdf:pdf},
journal = {Artificial Intelligence},
keywords = {grouping salience,hierarchical selectivity,integrated competition,object-based visual attention,visual attention},
number = {1},
pages = {77--123},
title = {{Object-based attention for computer vision}},
volume = {146},
year = {2003}
}
@inproceedings{Cho2010a,
author = {Cho, Hyunggi and Rybski, Paul E. and Zhang, Wende},
booktitle = {2010 IEEE Intelligent Vehicles Symposium},
doi = {10.1109/IVS.2010.5548063},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho, Rybski, Zhang - 2010 - Vision-based bicyclist detection and tracking for intelligent vehicles(2).pdf:pdf},
isbn = {978-1-4244-7866-8},
keywords = {cyclist},
mendeley-tags = {cyclist},
month = {jun},
pages = {454--461},
publisher = {IEEE},
title = {{Vision-based bicyclist detection and tracking for intelligent vehicles}},
url = {http://ieeexplore.ieee.org/document/5548063/},
year = {2010}
}
@article{Chen2015,
abstract = {Integer ambiguity resolution is a challenging technical issue that exists in real-time kinematic (RTK) global positioning system (GPS) navigation. Once the integer vector is resolved, centimeter-level positioning estimation accuracy can be achieved using the GPS carrier phase measurements. Recently, a real-time sliding window Bayesian estimation approach to RTK GPS and inertial navigation was proposed to provide reliable centimeter accurate-state estimation, via integer ambiguity resolution utilizing a prior along with all inertial measurement unit and GPS measurements within the time window. One challenge to implementing that approach in practice is the high computation cost. This paper proposes a novel implementation approach with significantly lower computational requirements and includes a thorough theoretical analysis. The implementation results show that the proposed method resolves an integer vector identical to that of the original method and achieves state estimation with centimeter global positioning accuracy.},
author = {Chen, Yiming and Zhao, Sheng and Farrell, Jay A.},
doi = {10.1109/TCST.2015.2501352},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Zhao, Farrell - 2015 - Computationally Efficient Carrier Integer Ambiguity Resolution in Multiepoch GPSINS A Common-Position-Sh(2).pdf:pdf},
isbn = {1063-6536 VO - PP},
issn = {10636536},
journal = {IEEE Transactions on Control Systems Technology},
number = {5},
pages = {1541--1556},
title = {{Computationally Efficient Carrier Integer Ambiguity Resolution in Multiepoch GPS/INS: A Common-Position-Shift Approach}},
volume = {24},
year = {2015}
}
@misc{Friedman2000,
abstract = {Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications.},
archivePrefix = {arXiv},
arxivId = {0804.2330},
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
booktitle = {The Annals of Statistics},
doi = {10.1214/aos/1016218223},
eprint = {0804.2330},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman, Hastie, Tibshirani - 2000 - Additive Logistic Regression(2).pdf:pdf},
isbn = {00905364},
issn = {0090-5364},
number = {2},
pages = {337--374},
pmid = {2565644},
title = {{Additive Logistic Regression}},
volume = {28},
year = {2000}
}
@misc{Bouguet2000,
author = {Bouguet, Jean-yves},
booktitle = {Intel Corporation, Microprocessor Research Labs},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bouguet - 2000 - Pyramidal implementation of the Lucas Kanade feature tracker(2).pdf:pdf},
title = {{Pyramidal implementation of the Lucas Kanade feature tracker}},
year = {2000}
}
@article{Vazquez,
author = {V{\'{a}}zquez, David and Ger{\'{o}}nimo, David and L{\'{o}}pez, Antonio M.},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/V{\'{a}}zquez, Ger{\'{o}}nimo, L{\'{o}}pez - Unknown - Detecting small pedestrians(2).pdf:pdf},
keywords = {adas,hog,pedestrian detection},
title = {{Detecting small pedestrians}}
}
@misc{Stricker,
author = {Stricker, Didier and Bleser, Gabriele},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stricker, Bleser - Unknown - • Introduction and Examples • Background Subtraction Methods(2).pdf:pdf},
pages = {1--45},
title = {{• Introduction and Examples • Background Subtraction Methods}}
}
@misc{WABCO2017,
author = {WABCO},
title = {{Advanced Driver Assistance Systems From WABCO}},
url = {http://www.wabco-auto.com/products/category-type/advanced-driver-assistance-systems},
year = {2017}
}
@article{Oron2014,
abstract = {The Lucas-Kanade (LK) method is a classic tracking algorithm exploiting target structural constraints thorough template matching. Extended Lucas Kanade or ELK casts the original LK algorithm as a maximum likelihood optimization and then extends it by considering pixel object / background likelihoods in the optimization. Template matching and pixel-based object / background segregation are tied together by a unified Bayesian framework. In this framework two log-likelihood terms related to pixel object / background affiliation are introduced in addition to the standard LK template matching term. Tracking is performed using an EM algorithm, in which the E-step corresponds to pixel object/background inference, and the M-step to parameter optimization. The final algorithm, implemented using a classifier for object / background modeling and equipped with simple template update and occlusion handling logic, is evaluated on two challenging data-sets containing 50 sequences each. The first is a recently published benchmark where ELK ranks 3rd among 30 tracking methods evaluated. On the second data-set of vehicles undergoing severe view point changes ELK ranks in 1st place outperforming state-of-the-art methods.},
author = {Oron, Shaul and Bar-Hille, Aharon and Avidan, Shai},
doi = {10.1007/978-3-319-10602-1_10},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oron, Bar-Hille, Avidan - 2014 - Extended Lucas-Kanade tracking(2).pdf:pdf},
isbn = {978-3-319-10601-4 978-3-319-10602-1},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 5},
pages = {142--156},
title = {{Extended Lucas-Kanade tracking}},
volume = {8693 LNCS},
year = {2014}
}
@article{Dollar2009,
author = {Doll{\'{a}}r, Piotr and Wojek, Christian and Schiele, Bernt and Perona, Pietro},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doll{\'{a}}r et al. - 2009 - Pedestrian Detection A Benchmark (to appear)(2).pdf:pdf},
isbn = {9781424439911},
journal = {Proc. CVPR},
title = {{Pedestrian Detection: A Benchmark (to appear)}},
year = {2009}
}
@article{Munkres1957,
author = {Munkres, James},
doi = {10.1137/0105003},
issn = {0368-4245},
journal = {Journal of the Society for Industrial and Applied Mathematics},
month = {mar},
number = {1},
pages = {32--38},
title = {{Algorithms for the Assignment and Transportation Problems}},
url = {http://epubs.siam.org/doi/abs/10.1137/0105003},
volume = {5},
year = {1957}
}
@article{Baumgartner2013,
abstract = {Current pedestrian tracking approaches ignore important aspects of human behavior. Humans are not moving independently, but they closely interact with their environment, which includes not only other persons, but also different scene objects. Typical everyday scenarios include people moving in groups, pushing child strollers, or pulling luggage. In this paper, we propose a probabilistic approach for classifying such person-object interactions, associating objects to persons, and predicting how the interaction will most likely continue. Our approach relies on stereo depth information in order to track all scene objects in 3D, while simultaneously building up their 3D shape models. These models and their relative spatial arrangement are then fed into a probabilistic graphical model which jointly infers pairwise interactions and object classes. The inferred interactions can then be used to support tracking by recovering lost object tracks. We evaluate our approach on a novel dataset containing more than 15,000 frames of person-object interactions in 325 video sequences and demonstrate good performance in challenging real-world scenarios.},
author = {Baumgartner, Tobias and Mitzel, Dennis and Leibe, Bastian},
doi = {10.1109/CVPR.2013.469},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baumgartner, Mitzel, Leibe - 2013 - Tracking people and their objects(2).pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
pages = {3658--3665},
title = {{Tracking people and their objects}},
year = {2013}
}
@book{Goldberg1989,
author = {Goldberg, David E.},
isbn = {0201157675},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
title = {{Genetic Algorithms in Search, Optimization and Machine Learning}},
year = {1989}
}
@article{DBLP:journals/corr/Garcia-GarciaOO17,
archivePrefix = {arXiv},
arxivId = {1704.06857},
author = {Garcia-Garcia, Alberto and Orts-Escolano, Sergio and Oprea, Sergiu and Villena-Martinez, Victor and Rodr$\backslash$'$\backslash$iguez, Jos{\'{e}} Garc$\backslash$'$\backslash$ia},
eprint = {1704.06857},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garcia-Garcia et al. - 2017 - A Review on Deep Learning Techniques Applied to Semantic Segmentation(4).pdf:pdf},
journal = {CoRR},
title = {{A Review on Deep Learning Techniques Applied to Semantic Segmentation}},
url = {http://arxiv.org/abs/1704.06857},
volume = {abs/1704.0},
year = {2017}
}
@article{McKenna2007,
abstract = {Bayesian particle filters have become popular for tracking human motion in cluttered scenes. The most commonly used filters suffer from two drawbacks. First, the prior used for the filtering step is often poor due to relatively large, poorly modelled inter-frame motion. Second, the use of the prior as an importance function results in inefficient sampling of the posterior. The use of the auxiliary particle filter (APF) and the novel iterated likelihood weighting filter (ILW) are proposed here in order to help address these problems. Experimental results comparing the filters' accuracy and consistency are presented for a scenario in which a person is tracked in an overhead view using an ellipse model. A likelihood model based on combined region (colour) and boundary (gradient) cues is motivated and used. The ILW filter is shown to outperform both Condensation and the APF on typical sequences from this scenario. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {McKenna, S. J. and Nait-Charif, H.},
doi = {10.1016/j.imavis.2006.06.003},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McKenna, Nait-Charif - 2007 - Tracking human motion using auxiliary particle filters and iterated likelihood weighting(2).pdf:pdf},
isbn = {0262-8856},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Head tracking,Human tracking,Iterated likelihood weighting,Particle filters,Supportive environments},
number = {6},
pages = {852--862},
title = {{Tracking human motion using auxiliary particle filters and iterated likelihood weighting}},
volume = {25},
year = {2007}
}
@article{Benenson2013,
abstract = {The current state of the art solutions for object detection describe each class by a set of models trained on discovered sub-classes (so called "components"), with each model itself composed of collections of interrelated parts (deformable models). These detectors build upon the now classic Histogram of Oriented Gradients+linear SVM combo. In this paper we revisit some of the core assumptions in HOG+SVM and show that by properly designing the feature pooling, feature selection, preprocessing, and training methods, it is possible to reach top quality, at least for pedestrian detections, using a single rigid component. Abstract We provide experiments for a large design space, that give insights into the design of classifiers, as well as relevant information for practitioners. Our best detector is fully feed-forward, has a single unified architecture, uses only histograms of oriented gradients and colour information in monocular static images, and improves over 23 other methods on the INRIA, ETH and Caltech-USA datasets, reducing the average miss-rate over HOG+SVM by more than 30{\%}.},
author = {Benenson, Rodrigo and Mathias, Markus and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1109/CVPR.2013.470},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Benenson et al. - 2013 - Seeking the strongest rigid detector(2).pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
keywords = {objects detection,pedestrian detection},
pages = {3666--3673},
title = {{Seeking the strongest rigid detector}},
year = {2013}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition(2).pdf:pdf},
month = {sep},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@phdthesis{Benenson2008,
abstract = {The development of driverless vehicles capable of moving on urban roads could pro- vide important benefits in accidents reductions, life comfort and costs reductions. In this document we discuss how to create a perception system allowing robots to drive on roads,without the need to adapt the infrastructure,without requiring previous visits, and considering possible the presence of pedestrians and cars. We argue that the perception process is application specific and by nature needs to be able to dealwith uncertainties in the knowledge of the world.We analyse the particular problem of perception for safe driving in the urban environments and propose a novel solution where the perception process is essentially seen as an optimization process. Also we propose that the perception process could benefit from collaboration between nearby vehicles. We examine this problem and provide a solution adapted to the con- straints encountered in the urban scenario. Here the core issue is formulated as a data association problem. Both the new perception and collaboration mechanism were integrated into a full- fledged driverless vehicle system. The results are supported by real world full scale experiments on our automated electric vehicles, the Cycabs.},
author = {Benenson, Rodrigo},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Benenson - 2008 - Perception for urban driverless vehicles design and implementation(2).pdf:pdf},
title = {{Perception for urban driverless vehicles: design and implementation}},
year = {2008}
}
@misc{wabco2016,
author = {WABCO},
title = {{WABCO}},
url = {http://www.wabco-auto.com/wabco/},
year = {2016}
}
@inproceedings{Kubota2007,
abstract = {A fast and robust stereo algorithm for on-board$\backslash$n$\backslash$nobstacle detection systems is proposed. The proposed method$\backslash$n$\backslash$nfinds the optimum road-obstacle boundary which provides the$\backslash$n$\backslash$nmost consistent interpretation of the input stereo image pair.$\backslash$n$\backslash$nGlobal optimization combined with a robust matching measure$\backslash$n$\backslash$nenables stable detection of obstacles under various circumstances,$\backslash$n$\backslash$nsuch as heavy rain and severe lighting conditions. The$\backslash$n$\backslash$nprocessing time for VGA size image pair is about 15msec on$\backslash$n$\backslash$na 3.6GHz pentium IV processor, which is fast enough for realtime$\backslash$n$\backslash$napplications.},
author = {Kubota, S and Nakano, T and {Y. Okamoto [Corporate Research {\&} Development Center}, TOSHIBA Corporation]},
booktitle = {IEEE Intelligent Vehicles Symposium},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kubota, Nakano, Y. Okamoto Corporate Research {\&} Development Center - 2007 - A Global Optimization Algorithm for Real-Time On-Board St(2).pdf:pdf},
title = {{A Global Optimization Algorithm for Real-Time On-Board Stereo Obstacle Detection Systems}},
year = {2007}
}
@article{Yangb,
author = {Yang, Meng and Zhang, David},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Zhang - Unknown - Robust Sparse Coding for Face Recognition CVPR11.pdf(2).pdf:pdf},
number = {1},
title = {{Robust Sparse Coding for Face Recognition CVPR11.pdf}}
}
@article{Song2015,
abstract = {The problem of real-time multiclass object recognition is of great practical importance in object recognition. In this paper, we describe a framework that simultaneously utilizes shared representation, reconstruction sparsity, and parallelism to enable real-time multiclass object detection with deformable part models at 5Hz on a laptop computer with almost no decrease in task performance. Our framework is trained in the standard structured output prediction formulation and is generically applicable for speeding up object recognition systems where the computational bottleneck is in multiclass, multi-convolutional inference. We experimentally demonstrate the efficiency and task performance of our method on PASCAL VOC, subset of ImageNet, Caltech101 and Caltech256 dataset.},
author = {Song, Hyun Oh and Girshick, Ross and Zickler, Stefan and Geyer, Christopher and Felzenszwalb, Pedro and Darrell, Trevor},
doi = {10.1109/TPAMI.2014.2353631},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Song et al. - 2015 - Generalized sparselet models for real-time multiclass object recognition(2).pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,deformable part models,real-time vision,sparse coding},
number = {5},
pages = {1001--1012},
pmid = {26353324},
title = {{Generalized sparselet models for real-time multiclass object recognition}},
volume = {37},
year = {2015}
}
@article{Dagan2004,
abstract = { The large number of rear end collisions due to driver inattention has been identified as a major automotive safety issue. Even a short advance warning can significantly reduce the number and severity of the collisions. This paper describes a vision based forward collision warning (FCW) system for highway safety. The algorithm described in this paper computes time to contact (TTC) and possible collision course directly from the size and position of the vehicles in the image - which are the natural measurements for a vision based system - without having to compute a 3D representation of the scene. The use of a single low cost image sensor results in an affordable system which is simple to install. The system has been implemented on real-time hardware and has been test driven on highways. Collision avoidance tests have also been performed on test tracks.},
author = {Dagan, E. and Mano, O. and Stein, G.P. and Shashua, a.},
doi = {10.1109/IVS.2004.1336352},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dagan et al. - 2004 - Forward collision warning with a single camera(2).pdf:pdf},
isbn = {0-7803-8310-9},
journal = {IEEE Intelligent Vehicles Symposium, 2004},
pages = {37--42},
title = {{Forward collision warning with a single camera}},
year = {2004}
}
@article{Kuo2010,
author = {Kuo, Cheng-hao and Huang, Chang and Nevatia, Ramakant},
doi = {10.1109/CVPR.2010.5540148},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuo, Huang, Nevatia - 2010 - Multi-Target Tracking by On-Line Learned Discriminative Appearance Models(2).pdf:pdf},
isbn = {978-1-4244-6984-0},
issn = {10636919},
pages = {685--692},
pmid = {15466619},
title = {{Multi-Target Tracking by On-Line Learned Discriminative Appearance Models}},
year = {2010}
}
@article{Navarro-Serment2008,
abstract = {The approach investigated in this work employs LADAR measurements to detect and track pedestrians over time. The algorithm can process range measurements from both line and 3D scanners. The use of line scanners allows detection and tracking at rates up to 75 Hz. However, this type of sensor may not always perform satisfactorily in uneven terrains. A 3D LADAR is used to improve operation in uneven terrains, by first estimating the local ground elevation, and then performing the detection using the measurements corresponding to a certain height above the ground. The information pipeline used to feed sensor data into the algorithm is the same for both types of sensors. The perceptual capabilities described aim to form the basis for safe and robust navigation in robotic vehicles, necessary to safeguard pedestrians operating in the vicinity of a moving robotic vehicle.},
author = {Navarro-Serment, Luis E and Mertz, Christoph and Vandapel, Nicolas and Hebert, Martial},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Navarro-Serment et al. - 2008 - LADAR-based Pedestrian Detection and Tracking(2).pdf:pdf},
journal = {Workshop on Human Detection from Mobile Platforms IEEE International Conference on Robotics and Automation ICRA},
pages = {1--6},
title = {{LADAR-based Pedestrian Detection and Tracking}},
url = {http://www.ri.cmu.edu/pub{\_}files/pub4/navarro{\_}serment{\_}luis{\_}ernesto{\_}2008{\_}1/navarro{\_}serment{\_}luis{\_}ernesto{\_}2008{\_}1.pdf},
year = {2008}
}
@book{Buehler2009,
author = {Buehler, Martin and Iagnemma, Karl and Singh, Sanjiv},
edition = {1st},
isbn = {3642039901, 9783642039904},
publisher = {Springer Publishing Company, Incorporated},
title = {{The DARPA Urban Challenge: Autonomous Vehicles in City Traffic}},
year = {2009}
}
@article{Broggi2011,
abstract = {Obstacle detection by means of stereo-vision is a fundamental task in computer vision, which has spurred a lot of research over the years, especially in the field of vehicular robotics. The information provided by this class of algorithms is used both in driving assistance systems and in autonomous vehicles, so the quality of the results and the processing times become critical, as detection failures or delays can have serious consequences. The obstacle detection system presented in this paper has been extensively tested during VIAC, the VisLab Intercontinental Autonomous Challenge [1], [2], which has offered a unique chance to face a number of different scenarios along the roads of two continents, in a variety of conditions; data collected during the expedition has also become a reference benchmark for further algorithm improvements.},
author = {Broggi, Alberto and Buzzoni, Michele and Felisa, Mirko and Zani, Paolo},
doi = {10.1109/IROS.2011.6048211},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Broggi et al. - 2011 - Stereo obstacle detection in challenging environments The VIAC experience(2).pdf:pdf},
isbn = {9781612844541},
issn = {2153-0858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {1599--1604},
title = {{Stereo obstacle detection in challenging environments: The VIAC experience}},
year = {2011}
}
@article{Grabner2008,
abstract = {Recently, on-line adaptation of binary classifiers for tracking have been investigated. On-line learning allows for simple classifiers since only the current view of the object from its surrounding background needs to be discriminiated. However, on-line adaption faces one key problem: Each update of the tracker may introduce an error which, finally, can lead to tracking failure (drifting). The contribution of this paper is a novel on-line semi-supervised boosting method which significantly alleviates the drifting problem in tracking applications. This allows to limit the drifting problem while still staying adaptive to appearance changes. The main idea is to formulate the update process in a semi-supervised fashion as combined decision of a given prior and an on-line classifier. This comes without any parameter tuning. In the experiments, we demonstrate real-time tracking of our SemiBoost tracker on several challenging test sequences where our tracker outperforms other on-line tracking methods.},
author = {Grabner, Helmut and Leistner, Christian and Bischof, Horst},
doi = {10.1007/978-3-540-88682-2-19},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grabner, Leistner, Bischof - 2008 - Semi-supervised on-line boosting for robust tracking(2).pdf:pdf},
isbn = {3540886818},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {234--247},
title = {{Semi-supervised on-line boosting for robust tracking}},
volume = {5302 LNCS},
year = {2008}
}
@article{Scaramuzza2008,
abstract = {In this paper, we describe a real-time algorithm for computing the ego-motion of a vehicle relative to the road. The algorithm uses as only input images provided by a single omnidirectional camera mounted on the roof of the vehicle. The front ends of the system are two different trackers. The first one is a homography-based tracker that detects and matches robust scale invariant features that most likely belong to the ground plane. The second one uses an appearance based approach and gives high resolution estimates of the rotation of the vehicle. This planar pose estimation method has been successfully applied to videos from an automotive platform. We give an example of camera trajectory estimated purely from omnidirectional images over a distance of 400 meters. For performance evaluation, the estimated path is superimposed onto a satellite image. In the end, we use image mosaicing to obtain a textured 2D reconstruction of the estimated path.},
author = {Scaramuzza, Davide and Siegwart, Roland},
doi = {10.1109/TRO.2008.2004490},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scaramuzza, Siegwart - 2008 - Appearance-guided monocular omnidirectional visual odometry for outdoor ground vehicles(2).pdf:pdf},
isbn = {3540795464},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Appearance,Homography,Omnidirectional camera,Scale-invariant feature transform (SIFT) features,Vehicle ego-motion estimation,Visual odometry},
number = {5},
pages = {1015--1026},
title = {{Appearance-guided monocular omnidirectional visual odometry for outdoor ground vehicles}},
volume = {24},
year = {2008}
}
@article{Revised2006,
author = {Revised, Last},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Revised - 2006 - Bipartite Matching {\&} the Hungarian Method(2).pdf:pdf},
title = {{Bipartite Matching {\&} the Hungarian Method}},
year = {2006}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2015 - Deep Residual Learning for Image Recognition(2).pdf:pdf},
month = {dec},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@inproceedings{Ozuysal2007,
author = {Ozuysal, Mustafa and Fua, Pascal and Lepetit, Vincent},
booktitle = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2007.383123},
isbn = {1-4244-1179-3},
month = {jun},
pages = {1--8},
publisher = {IEEE},
title = {{Fast Keypoint Recognition in Ten Lines of Code}},
url = {http://ieeexplore.ieee.org/document/4270148/},
year = {2007}
}
@article{Sudowe2011,
author = {Sudowe, Patrick and Leibe, Bastian},
doi = {10.1007/978-3-642-23968-7_2},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sudowe, Leibe - 2011 - Efficient use of geometric constraints for sliding-window object detection in video(2).pdf:pdf},
isbn = {978-3-642-23967-0},
journal = {Proceedings of the 8th international conference on Computer vision systems},
pages = {11--20},
title = {{Efficient use of geometric constraints for sliding-window object detection in video}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-23968-7{\_}2{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2045399.2045402},
year = {2011}
}
@article{Tsochantaridis2005,
abstract = {Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.},
author = {Tsochantaridis, Ioannis and Joachims, Thorsten and Hofmann, Thomas and Altun, Yasemin},
doi = {10.1007/s10994-008-5071-9},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsochantaridis et al. - 2005 - Large Margin Methods for Structured and Interdependent Output Variables(2).pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {maximum margin markove network},
pages = {1453--1484},
title = {{Large Margin Methods for Structured and Interdependent Output Variables}},
volume = {6},
year = {2005}
}
@article{Wedel2009,
author = {Wedel, Andreas and Mei{\ss}ner, Annemarie and Rabe, Clemens and Franke, Uwe and Cremers, Daniel},
doi = {10.1007/978-3-642-03641-5_2},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wedel et al. - 2009 - Detection and segmentation of independently moving objects from dense scene flow(2).pdf:pdf},
isbn = {3642036406},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {14--27},
title = {{Detection and segmentation of independently moving objects from dense scene flow}},
volume = {5681 LNCS},
year = {2009}
}
@inproceedings{Zambrano-Bigiarini2013,
author = {Zambrano-Bigiarini, Mauricio and Clerc, Maurice and Rojas, Rodrigo},
booktitle = {2013 IEEE Congress on Evolutionary Computation},
doi = {10.1109/CEC.2013.6557848},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zambrano-Bigiarini, Clerc, Rojas - 2013 - Standard Particle Swarm Optimisation 2011 at CEC-2013 A baseline for future PSO improvement(2).pdf:pdf},
isbn = {978-1-4799-0454-9},
keywords = {Algorithm design and analysis,Benchmark testing,CEC-2013,Convergence,Equations,Optimization,SPSO- 2011,SPSO-2011,Standards,Topology,adaptive random topology,benchmark testing,convergence,evolutionary computation,fast convergence,future PSO improvements,global minimum,global optimum,local minima,optimization,particle swarm optimisation,particle swarm optimization,random processes,random topology,real-parameter single objective optimisation,rotated multimodal functions,rotational invariance,separable test functions,standard particle swarm optimisation,topology,unimodal test functions},
month = {jun},
pages = {2337--2344},
publisher = {IEEE},
title = {{Standard Particle Swarm Optimisation 2011 at CEC-2013: A baseline for future PSO improvements}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6557848},
year = {2013}
}
@article{Bernardin2008,
abstract = {Simultaneous tracking of multiple persons in real-world environments is an active research field and several approaches have been proposed, based on a variety of features and algorithms. Recently, there has been a growing interest in organizing systematic evaluations to compare the various techniques. Unfortunately, the lack of common metrics for measuring the performance of multiple object trackers still makes it hard to compare their results. In this work, we introduce two intuitive and general metrics to allow for objective comparison of tracker characteristics, focusing on their precision in estimating object locations, their accuracy in recognizing object configurations and their ability to consistently label objects over time. These metrics have been extensively used in two large-scale international evaluations, the 2006 and 2007 CLEAR evaluations, to measure and compare the performance of multiple object trackers for a wide variety of tracking tasks. Selected performance results are presented and the advantages and drawbacks of the presented metrics are discussed based on the experience gained during the evaluations.},
author = {Bernardin, Keni and Stiefelhagen, Rainer},
doi = {10.1155/2008/246309},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bernardin, Stiefelhagen - 2008 - Evaluating multiple object tracking performance The CLEAR MOT metrics(2).pdf:pdf},
issn = {16875176},
journal = {Eurasip Journal on Image and Video Processing},
title = {{Evaluating multiple object tracking performance: The CLEAR MOT metrics}},
volume = {2008},
year = {2008}
}
@article{Montemerlo2008,
author = {Montemerlo, Michael and Becker, Jan and Bhat, Suhrid and Dahlkamp, Hendrik and Dolgov, Dmitri and Ettinger, Scott and Haehnel, Dirk and Hilden, Tim and Hoffmann, Gabe and Huhnke, Burkhard and Johnston, Doug and Klumpp, Stefan and Langer, Dirk and Levandowski, Anthony and Levinson, Jesse and Marcil, Julien and Orenstein, David and Paefgen, Johannes and Penny, Isaac and Petrovskaya, Anna and Pflueger, Mike and Stanek, Ganymed and Stavens, David and Vogt, Antone and Thrun, Sebastian},
doi = {10.1002/ROB.V25:9},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Montemerlo et al. - 2008 - Junior The Stanford entry in the Urban Challenge(2).pdf:pdf},
issn = {15564959},
journal = {Journal of Field Robotics},
number = {9},
pages = {569--597},
publisher = {John Wiley and Sons Ltd.},
title = {{Junior: The Stanford entry in the Urban Challenge}},
volume = {25},
year = {2008}
}
@article{Viola2004,
author = {Viola, Paul and Jones, Michael J.},
doi = {10.1023/B:VISI.0000013087.49260.fb},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Viola, Jones - 2004 - Robust Real-Time Face Detection(2).pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {may},
number = {2},
pages = {137--154},
publisher = {Kluwer Academic Publishers},
title = {{Robust Real-Time Face Detection}},
url = {http://link.springer.com/10.1023/B:VISI.0000013087.49260.fb},
volume = {57},
year = {2004}
}
@inproceedings{Grabner2006,
author = {Grabner, H. and Bischof, H.},
booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1 (CVPR'06)},
doi = {10.1109/CVPR.2006.215},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grabner, Bischof - 2006 - On-line Boosting and Vision(2).pdf:pdf},
isbn = {0-7695-2597-0},
keywords = {Application software,Boosting,Computer graphics,Computer vision,Feature extraction,Machine learning,Machine learning algorithms,Object detection,Support vector machines,Training data},
pages = {260--267},
publisher = {IEEE},
title = {{On-line Boosting and Vision}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1640768},
volume = {1},
year = {2006}
}
@inproceedings{Song2015a,
address = {New York, New York, USA},
author = {Song, Xiaona and Rui, Ting and Zha, Zhengjun and Wang, Xinqing and Fang, Husheng},
booktitle = {Proceedings of the 7th International Conference on Internet Multimedia Computing and Service - ICIMCS '15},
doi = {10.1145/2808492.2808497},
isbn = {9781450335287},
keywords = {AdaBoost,CNN,shallow CNN feature-extractor,vehicle detection},
pages = {1--5},
publisher = {ACM Press},
title = {{The AdaBoost algorithm for vehicle detection based on CNN features}},
url = {http://dl.acm.org/citation.cfm?doid=2808492.2808497},
year = {2015}
}
@article{Broggi2009,
abstract = {This paper presents an application of a pedestrian-detection system aimed at localizing potentially dangerous situations under specific urban scenarios. The approach used in this paper differs from those implemented in traditional pedestrian-detection systems, which are designed to localize all pedestrians in the area in front of the vehicle. Conversely, this approach searches for pedestrians in critical areas only. The environment is reconstructed with a standard laser scanner, whereas the following check for the presence of pedestrians is performed due to the fusion with a vision system. The great advantages of such an approach are that pedestrian recognition is performed on limited image areas, therefore boosting its timewise performance, and no assessment on the danger level is finally required before providing the result to either the driver or an onboard computer for automatic maneuvers. A further advantage is the drastic reduction of false alarms, making this system robust enough to control nonreversible safety systems.},
author = {Broggi, Alberto and Cerri, Pietro and Ghidoni, Stefano and Grisleri, Paolo and Jung, Ho Gi},
doi = {10.1109/TITS.2009.2032770},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Broggi et al. - 2009 - A new approach to urban pedestrian detection for automatic braking(2).pdf:pdf},
isbn = {1524-9050},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Artificial intelligence (AI),Computer vision,Fuzzy logic,Image processing,Pattern recognition,Pedestrian detection},
number = {4},
pages = {594--605},
title = {{A new approach to urban pedestrian detection for automatic braking}},
volume = {10},
year = {2009}
}
@article{Ess2009,
author = {Ess, a and Leibe, B and Schindler, K and {Van Gool}, L},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ess et al. - 2009 - Robust multiperson tracking from a mobile platform(2).pdf:pdf},
journal = {Pattern Analysis and Machine Intelligence},
pages = {1--14},
title = {{Robust multiperson tracking from a mobile platform}},
volume = {31},
year = {2009}
}
@article{Sunb,
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun, Wang, Tang - Unknown - Goup{\_}Reading{\_}22Dec15{\_}-{\_}Deep{\_}Learning{\_}Face{\_}Representation{\_}From{\_}Predicting{\_}10{\_}000{\_}Classes(2).pdf:pdf},
title = {{Goup{\_}Reading{\_}22Dec15{\_}-{\_}Deep{\_}Learning{\_}Face{\_}Representation{\_}From{\_}Predicting{\_}10{\_}000{\_}Classes}}
}
@inproceedings{Zhou2016,
author = {Zhou, Yiren and Nejati, Hossein and Do, Thanh-Toan and Cheung, Ngai-Man and Cheah, Lynette},
booktitle = {2016 IEEE International Conference on Digital Signal Processing (DSP)},
doi = {10.1109/ICDSP.2016.7868561},
isbn = {978-1-5090-4165-7},
month = {oct},
pages = {276--280},
publisher = {IEEE},
title = {{Image-based vehicle analysis using deep neural network: A systematic study}},
url = {http://ieeexplore.ieee.org/document/7868561/},
year = {2016}
}
@inproceedings{girpr2012.viac,
address = {Pontignano, Siena, Italy},
author = {Bertozzi, Massimo and Broggi, Alberto and Cardarelli, Elena and Cattani, Stefano and Laghi, Maria Chiara},
booktitle = {Procs.{\~{}}GIRPR 2012},
title = {{Equipment and Capabilities of the Vehicles for the VisLab Intercontinental Autonomous Challenge}},
year = {2012}
}
@article{Cortes1995,
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1023/A:1022627411411},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cortes, Vapnik - 1995 - Support-Vector Networks(2).pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
number = {3},
pages = {273--297},
publisher = {Kluwer Academic Publishers-Plenum Publishers},
title = {{Support-Vector Networks}},
url = {http://link.springer.com/10.1023/A:1022627411411},
volume = {20},
year = {1995}
}
@inproceedings{Zitnick2014a,
author = {Zitnick, C. Lawrence and Doll{\'{a}}r, Piotr},
booktitle = {Eccv},
doi = {10.1007/978-3-319-10602-1_26},
pages = {391--405},
publisher = {Springer, Cham},
title = {{Edge Boxes: Locating Object Proposals from Edges}},
url = {http://link.springer.com/10.1007/978-3-319-10602-1{\_}26},
year = {2014}
}
@inproceedings{Tian2010,
author = {Tian, Tai-Peng and Sclaroff, Stan},
booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5540227},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tian, Sclaroff - 2010 - Fast globally optimal 2D human detection with loopy graph models(2).pdf:pdf},
isbn = {978-1-4244-6984-0},
keywords = {Approximation algorithms,Approximation error,Biological system modeling,Cost function,Graphical models,Humans,Inference algorithms,Iterative algorithms,Kinematics,Tree graphs,branch and bound algorithm,data structures,dynamic programming,globally optimal 2D human figure detection,graph theory,iterative parsing dataset,loopy graph models,object detection,query processing,range minimum query data structures,tree model,tree searching},
month = {jun},
pages = {81--88},
publisher = {IEEE},
title = {{Fast globally optimal 2D human detection with loopy graph models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5540227},
year = {2010}
}
@article{Dollar2009a,
abstract = {We study the performance of integral channel features for image classification tasks, focusing in particular on pedestrian detection. The general idea behind integral channel features is that multiple registered image channels are computed using linear and non-linear transformations of the input image, and then features such as local sums, histograms, and Haar features and their various generalizations are efficiently computed using integral images. Such features have been used in recent literature for a variety of tasks indeed, variations appear to have been invented independently multiple times. Although integral channel features have proven effective, little effort has been devoted to analyzing or optimizing the features themselves. In this work we present a unified view of the relevant work in this area and perform a detailed experimental evaluation. We demonstrate that when designed properly, integral channel features not only outperform other features including histogram of oriented gradient (HOG), they also (1) naturally integrate heterogeneous sources of information, (2) have few parameters and are insensitive to exact parameter settings, (3) allow for more accurate spatial localization during detection, and (4) result in fast detectors when coupled with cascade classifiers.},
author = {Doll{\'{a}}r, Piotr and Tu, Zhuowen and Perona, Pietro and Belongie, Serge},
doi = {10.5244/C.23.91},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doll{\'{a}}r et al. - 2009 - Integral Channel Features(2).pdf:pdf},
isbn = {1-901725-39-1},
issn = {1901725391},
journal = {BMVC 2009 London England},
pages = {1--11},
title = {{Integral Channel Features}},
url = {http://www.loni.ucla.edu/{~}ztu/publication/dollarBMVC09ChnFtrs{\_}0.pdf},
year = {2009}
}
@inproceedings{Kokkinos2011,
author = {Kokkinos, Iasonas},
booktitle = {NIPS 2011},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kokkinos - 2011 - Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound(2).pdf:pdf},
pages = {2681--2689},
title = {{Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound}},
year = {2011}
}
@article{Dollar2010,
abstract = {Our key insight is that for a broad family of features, including gradient histograms, the feature responses computed at a single scale can be used to approximate feature responses at nearby scales.},
author = {Dollar, Piotr and Belongie, Serge and Perona, Pietro},
doi = {10.5244/C.24.68},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dollar, Belongie, Perona - 2010 - The Fastest Pedestrian Detector in the West(2).pdf:pdf},
isbn = {1-901725-40-5},
journal = {Procedings of the British Machine Vision Conference 2010},
pages = {68.1--68.11},
title = {{The Fastest Pedestrian Detector in the West}},
url = {http://www.bmva.org/bmvc/2010/conference/paper68/index.html},
year = {2010}
}
@phdthesis{Detection2009,
author = {Ger{\'{o}}nimo, David},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ger{\'{o}}nimo - 2009 - A Global Approach to Vision-Based Pedestrian Detection for Advanced Driver Assistance Systems(2).pdf:pdf},
title = {{A Global Approach to Vision-Based Pedestrian Detection for Advanced Driver Assistance Systems}},
year = {2009}
}
@article{Breitenstein2011,
abstract = {In this paper, we address the problem of automatically detecting and tracking a variable number of persons in complex scenes using a monocular, potentially moving, uncalibrated camera. We propose a novel approach for multiperson tracking-by-detection in a particle filtering framework. In addition to final high-confidence detections, our algorithm uses the continuous confidence of pedestrian detectors and online-trained, instance-specific classifiers as a graded observation model. Thus, generic object category knowledge is complemented by instance-specific information. The main contribution of this paper is to explore how these unreliable information sources can be used for robust multiperson tracking. The algorithm detects and tracks a large number of dynamically moving people in complex scenes with occlusions, does not rely on background modeling, requires no camera or ground plane calibration, and only makes use of information from the past. Hence, it imposes very few restrictions and is suitable for online applications. Our experiments show that the method yields good tracking performance in a large variety of highly dynamic scenarios, such as typical surveillance videos, webcam footage, or sports sequences. We demonstrate that our algorithm outperforms other methods that rely on additional information. Furthermore, we analyze the influence of different algorithm components on the robustness.},
author = {Breitenstein, Michael D. and Reichlin, Fabian and Leibe, Bastian and Koller-Meier, Esther and {Van Gool}, Luc},
doi = {10.1109/TPAMI.2010.232},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breitenstein et al. - 2011 - Online multiperson tracking-by-detection from a single, uncalibrated camera(2).pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Multi-object tracking,detector confidence,detector confidence particle filter,online learning,particle filtering,pedestrian detection,sequential Monte Carlo estimation,sports analysis,surveillance,tracking-by-detection,traffic safety},
number = {9},
pages = {1820--1833},
pmid = {21173441},
title = {{Online multiperson tracking-by-detection from a single, uncalibrated camera}},
volume = {33},
year = {2011}
}
@article{Gordon2006,
abstract = {Many applications of 3D object recognition, such as aug- mented reality or robotic manipulation, require an accurate solution for the 3D pose of the recognized objects. This is best accomplished by building a metrically accurate 3D model of the object and all its fea- ture locations, and then fitting this model to features detected in new images. In this chapter, we describe a system for constructing 3D met- ric models from multiple images taken with an uncalibrated handheld camera, recognizing these models in new images, and precisely solving for object pose. This is demonstrated in an augmented reality applica- tion where objects must be recognized, tracked, and superimposed on new images taken from arbitrary viewpoints without perceptible jitter. This approach not only provides for accurate pose, but also allows for integration of features from multiple training images into a single model that provides for more reliable recognition.},
author = {Gordon, Iryna and Lowe, David G},
doi = {10.1007/11957959_4},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gordon, Lowe - 2006 - What and Where 3D Object Recognition with Accurate Pose(2).pdf:pdf},
isbn = {978-3-540-68794-8},
issn = {15347362},
journal = {Toward Category-Level Object Recognition},
pages = {67--82},
pmid = {12678625},
title = {{What and Where : 3D Object Recognition with Accurate Pose}},
year = {2006}
}
@inproceedings{Tian2015,
author = {Tian, Yonglong and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.221},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tian et al. - 2015 - Deep Learning Strong Parts for Pedestrian Detection(2).pdf:pdf},
isbn = {978-1-4673-8391-2},
keywords = {ConvNet,DeepParts,Detectors,Feature extraction,ImageNet,IoU positive proposals,Proposals,Prototypes,Semantics,Training,Training data,convolution,convolutional neural network,extensive part detectors,learning (artificial intelligence),massive general object categories,neural nets,object detection,occlusion handling,pedestrian bounding boxes,pedestrian detection,pedestrian images,pedestrians},
month = {dec},
pages = {1904--1912},
publisher = {IEEE},
title = {{Deep Learning Strong Parts for Pedestrian Detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7410578},
year = {2015}
}
@article{QuanYuan2011,
author = {{Quan Yuan} and Thangali, Ashwin and Ablavsky, Vitaly and Sclaroff, Stan},
doi = {10.1109/TPAMI.2010.117},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quan Yuan et al. - 2011 - Learning a Family of Detectors via Multiplicative Kernels(2).pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {mar},
number = {3},
pages = {514--530},
title = {{Learning a Family of Detectors via Multiplicative Kernels}},
url = {http://ieeexplore.ieee.org/document/5487524/},
volume = {33},
year = {2011}
}
@phdthesis{Stavens2011,
author = {Stavens, David},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stavens - 2011 - Learning To Drive Perception for Autonomous Cars a Dissertation Submitted To the Department of Computer Science and (2).pdf:pdf},
number = {May},
school = {Stanford University},
title = {{Learning To Drive : Perception for Autonomous Cars a Dissertation Submitted To the Department of Computer Science and the Committee on Graduate Studies of Stanford University in Partial Fulfillment of the Requirements for the Degree of}},
year = {2011}
}
@inproceedings{Ess2009c,
author = {Ess, A. and Leibe, B. and Schindler, K. and van Gool, L.},
booktitle = {2009 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2009.5152884},
isbn = {978-1-4244-2788-8},
month = {may},
pages = {56--63},
publisher = {IEEE},
title = {{Moving obstacle detection in highly dynamic scenes}},
url = {http://ieeexplore.ieee.org/document/5152884/},
year = {2009}
}
@article{Tucker2005,
abstract = {This paper details the design and implementation of an embedded real-time system to fuse data from two 77 GHz radars, three 24 GHz radars and a video lane detection system fitted to a vehicle to track the movement of other vehicles in the local environment. The system demonstrates excellent tracking of vehicles as they pass through regions covered by single and multiple sensors as well as regions with no sensor coverage where tracking continues despite no new measurements. Such a system facilitates future driver assistance functions such as lane change support and blind spot detection.},
author = {Tucker, Mark and Heenan, Adam and Buchanan, Alastair},
doi = {10.1109/ITSC.2005.1520115},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tucker, Heenan, Buchanan - 2005 - Real time embedded sensor fusion for driver assistance(2).pdf:pdf},
isbn = {0780392159},
journal = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
pages = {596--601},
title = {{Real time embedded sensor fusion for driver assistance}},
volume = {2005},
year = {2005}
}
@article{Stein2000,
abstract = {We describe a robust method for computing the ego-motion of the$\backslash$nvehicle relative to the road using input from a single camera mounted$\backslash$nnext to the rear view mirror. Since feature points are unreliable in$\backslash$ncluttered scenes we use direct methods where image values in the two$\backslash$nimages are combined in a global probability function. Combined with the$\backslash$nuse of probability distribution matrices, this enables the formulation$\backslash$nof a robust method that can ignore large number of outliers as one would$\backslash$nencounter in real traffic situations. The method has been tested in real$\backslash$nworld environments and has been shown to be robust to glare, rain and$\backslash$nmoving objects in the scene},
author = {Stein, G.P. and Mano, O. and Shashua, a.},
doi = {10.1109/IVS.2000.898370},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stein, Mano, Shashua - 2000 - A robust method for computing vehicle ego-motion(2).pdf:pdf},
isbn = {0-7803-6363-9},
journal = {Proceedings of the IEEE Intelligent Vehicles Symposium 2000 (Cat. No.00TH8511)},
number = {Mi},
pages = {362--368},
title = {{A robust method for computing vehicle ego-motion}},
year = {2000}
}
@article{Saffari2009,
abstract = {Random Forests (RFs) are frequently used in many computer vision and machine learning applications. Their popularity is mainly driven by their high computational efficiency during both training and evaluation while achieving state-of-the-art results. However, in most applications RFs are used off-line. This limits their usability for many practical problems, for instance, when training data arrives sequentially or the underlying distribution is continuously changing. In this paper, we propose a novel on-line random forest algorithm. We combine ideas from on-line bagging, extremely randomized forests and propose an on-line decision tree growing procedure. Additionally, we add a temporal weighting scheme for adaptively discarding some trees based on their out-of-bag-error in given time intervals and consequently growing of new trees. The experiments on common machine learning data sets show that our algorithm converges to the performance of the off-line RF. Additionally, we conduct experiments for visual tracking, where we demonstrate real-time state-of-the-art performance on well-known scenarios and show good performance in case of occlusions and appearance changes where we outperform trackers based on on-line boosting. Finally, we demonstrate the usability of on-line RFs on the task of interactive real-time segmentation.},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1023{\%}2FA{\%}3A1010933404324},
author = {Saffari, Amir and Leistner, Christian and Santner, Jakob and Godec, Martin and Bischof, Horst},
doi = {10.1109/ICCVW.2009.5457447},
eprint = {/dx.doi.org/10.1023{\%}2FA{\%}3A1010933404324},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saffari et al. - 2009 - On-line random forests(2).pdf:pdf},
isbn = {9781424444427},
issn = {08856125},
journal = {2009 IEEE 12th International Conference on Computer Vision Workshops (ICCV Workshops 2009)},
pages = {1393--1400},
pmid = {20142443},
primaryClass = {http:},
title = {{On-line random forests}},
year = {2009}
}
@article{Geronimo2010a,
abstract = {During the next decade, on-board pedestrian detection systems will play a key role in the challenge of increasing traffic safety. The main target of these systems, to detect pedestrians in urban scenarios, implies overcoming difficulties like processing outdoor scenes from a mobile platform and searching for aspect-changing objects in cluttered environments. This makes such systems combine techniques in the state-of-the-art Computer Vision. In this paper we present a three module system based on both 2D and 3D cues. The first module uses 3D information to estimate the road plane parameters and thus select a coherent set of regions of interest (ROIs) to be further analyzed. The second module uses Real AdaBoost and a combined set of Haar wavelets and edge orientation histograms to classify the incoming ROIs as pedestrian or non-pedestrian. The final module loops again with the 3D cue in order to verify the classified ROIs and with the 2D in order to refine the final results. According to the results, the integration of the proposed techniques gives rise to a promising system. ?? 2010 Elsevier Inc. All rights reserved.},
author = {Ger{\'{o}}nimo, David and Sappa, Angel D. and Ponsa, Daniel and L{\'{o}}pez, Antonio M.},
doi = {10.1016/j.cviu.2009.07.008},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ger{\'{o}}nimo et al. - 2010 - 2D-3D-based on-board pedestrian detection system(2).pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Advanced Driver Assistance Systems,Edge orientation histograms,Haar wavelets,Horizon line,Pedestrian detection},
number = {5},
pages = {583--595},
publisher = {Elsevier Inc.},
title = {{2D-3D-based on-board pedestrian detection system}},
url = {http://dx.doi.org/10.1016/j.cviu.2009.07.008},
volume = {114},
year = {2010}
}
@article{Wu2003,
abstract = {Face detection is a canonical example of a rare event detection problem, in which target patterns occur with much lower frequency than non-targets. Out of millions of face-sized windows in an input image, for example, only a few will typically contain a face. Viola and Jones recently proposed a cascade architecture for face detection which successfully addresses the rare event nature of the task. A central part of their method is a feature selection algorithm based on AdaBoost. We present a novel cascade learning algorithm based on forward feature selection which is two orders of magnitude faster than the Viola-Jones approach and yields classifiers of similar quality. This faster method could be used for more demanding classification tasks, such as on-line learning or searching the space of classifier structures. Our experimental results highlight the dominant role of the feature set in the success of the cascade approach. 1},
author = {Wu, Jianxin and Rehg, Jm and Mullin, Md},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Rehg, Mullin - 2003 - Learning a Rare Event Detection Cascade by Direct Feature Selection(2).pdf:pdf},
isbn = {0-262-20152-6},
issn = {1049-5258},
journal = {Nips},
pages = {1--17},
title = {{Learning a Rare Event Detection Cascade by Direct Feature Selection.}},
url = {http://smartech.gatech.edu/handle/1853/3228{\%}5Cnhttps://papers.nips.cc/paper/2353-learning-a-rare-event-detection-cascade-by-direct-feature-selection.pdf},
year = {2003}
}
@article{Bae2014,
abstract = {In this paper, we consider a multiobject tracking problem in complex scenes. Unlike batch tracking systems using detections of the entire sequence, we propose a novel online multiobject tracking system in order to build tracks sequentially using online provided detections. To track objects robustly even under frequent occlusions, the proposed system consists of three main parts: 1) visual tracking with a novel data association with a track existence probability by associating online detections with the corresponding tracks under partial occlusions; 2) track management to associate terminated tracks for linking tracks fragmented by long-term occlusions; and 3) online model learning to generate discriminative appearance models for successful associations in other two parts. Experimental results using challenging public data sets show the obvious performance improvement of the proposed system, compared with other state-of-the-art tracking systems. Furthermore, extensive performance analysis of the three main parts demonstrates effects and usefulness of the each component for multiobject tracking.},
author = {Bae, Seung-Hwan and Yoon, Kuk-Jin},
doi = {10.1109/TIP.2014.2320821},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bae, Yoon - 2014 - Robust Online Multiobject Tracking With Data Association and Track Management(2).pdf:pdf},
issn = {1057-7149},
journal = {Image Processing, IEEE Transactions on},
keywords = {learning (artificial intelligence);object tracking},
number = {7},
pages = {2820--2833},
title = {{Robust Online Multiobject Tracking With Data Association and Track Management}},
volume = {23},
year = {2014}
}
@article{Kristan2013,
abstract = {Visual tracking has attracted a significant attention in the last few decades. The recent surge in the number of publications on tracking-related problems have made it almost impossible to follow the developments in the field. One of the reasons is that there is a lack of commonly accepted annotated data-sets and standardized evaluation protocols that would allow objective comparison of different tracking methods. To address this issue, the Visual Object Tracking (VOT) workshop was organized in conjunction with ICCV2013. Researchers from academia as well as industry were invited to participate in the first VOT2013 challenge which aimed at single-object visual trackers that do not apply pre-learned models of object appearance (model-free). Presented here is the VOT2013 benchmark dataset for evaluation of single-object visual trackers as well as the results obtained by the trackers competing in the challenge. In contrast to related attempts in tracker benchmarking, the dataset is labeled per-frame by visual attributes that indicate occlusion, illumination change, motion change, size change and camera motion, offering a more systematic comparison of the trackers. Furthermore, we have designed an automated system for performing and evaluating the experiments. We present the evaluation protocol of the VOT2013 challenge and the results of a comparison of 27 trackers on the benchmark dataset. The dataset, the evaluation tools and the tracker rankings are publicly available from the challenge website (http://votchallenge.net).},
author = {Kristan, Matej and Pflugfelder, Roman and Leonardis, Ale{\v{s}} and Matas, Jiri and Porikli, Fatih and {\v{C}}ehovin, Luka and Nebehay, Georg and Fernandez, Gustavo and Voj{\'{i}}ř, Tom{\'{a}}{\v{s}} and Gatt, Adam and Khajenezhad, Ahmad and Salahledin, Ahmed and Soltani-Farani, Ali and Zarezade, Ali and Petrosino, Alfredo and Milton, Anthony and Bozorgtabar, Behzad and Li, Bo and Chan, Chee Seng and Heng, Cherkeng and Ward, Dale and Kearney, David and Monekosso, Dorothy and Karaimer, Hakki Can and Rabiee, Hamid R. and Zhu, Jianke and Gao, Jin and Xiao, Jingjing and Zhang, Junge and Xing, Junliang and Huang, Kaiqi and Lebeda, Karel and Cao, Lijun and Maresca, Mario Edoardo and Lim, Mei Kuan and ELHelw, Mohamed and Felsberg, Michael and Remagnino, Paolo and Bowden, Richard and Goecke, Roland and Stolkin, Rustam and Lim, Samantha Yue Ying and Maher, Sara and Poullot, Sebastien and Wong, Sebastien and Satoh, Shin'Ichi and Chen, Weihua and Hu, Weiming and Zhang, Xiaoqin and Li, Yang and Niu, Zhiheng},
doi = {10.1109/ICCVW.2013.20},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kristan et al. - 2013 - The visual object tracking VOT2013 challenge results(2).pdf:pdf},
isbn = {9781479930227},
issn = {16113349},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
keywords = {VOT2013,Visual object tracking challenge},
pages = {98--111},
title = {{The visual object tracking VOT2013 challenge results}},
year = {2013}
}
@phdthesis{Heikkila1997,
abstract = {In this thesis, computational methods are developed for measuring three-dimensional structure from image sequences. The measurement process contains several stages, in which the intensity information obtained from a moving video camera is transformed into three-dimensional spatial coordinates. The proposed approach utilizes either line or circular features, which are automatically observed from each camera position. The two-dimensional data gathered from a sequence of digital images is then integrated into a three-dimensional model. This process is divided into three major computational issues: data acquisition, geometric camera calibration, and 3-D structure estimation. The purpose of data acquisition is to accurately locate the features from individual images. This task is performed by first determining the intensity boundary of each feature with subpixel precision, and then fitting a geometric model of the expected feature type into the boundary curve. The resulting parameters fully describe the two-dimensional location of the feature with respect to the image coordinate system. The feature coordinates obtained can be used as input data both in camera calibration and 3-D structure estimation. Geometric camera calibration is required for correcting the spatial errors in the images. Due to various error sources video cameras do not typically produce a perfect perspective projection. The feature coordinates determined are therefore systematically distorted. In order to correct the distortion, both a comprehensive camera model and a procedure for computing the model parameters are required. The calibration procedure proposed in this thesis utilizes circular features in the computation of the camera parameters. A new method for correcting the image coordinates is also presented. Estimation of the 3-D scene structure from image sequences requires the camera position and orientation to be known for each image. Thus, camera motion estimation is closely related to the 3- D structure estimation, and generally, these two tasks must be performed in parallel causing the estimation problem to be nonlinear. However, if the motion is purely translational, or the rotation component is known in advance, the motion estimation process can be separated from 3-D structure estimation. As a consequence, linear techniques for accurately computing both camera motion and 3- D coordinates of the features can be used. A major advantage of using an image sequence based measurement technique is that the correspondence problem of traditional stereo vision is mainly avoided. The image sequence can be captured with short inter-frame steps causing the disparity between successive images to be so small that the correspondences can be easily determined with a simple tracking technique. Furthermore, if the motion is translational, the shapes of the features are only slightly deformed during the sequence.},
author = {Heikkil{\"{a}}, Janne},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heikkil{\"{a}} - 1997 - Accurate camera calibration and feature based 3-D reconstruction from monocular image sequences(2).pdf:pdf},
title = {{Accurate camera calibration and feature based 3-D reconstruction from monocular image sequences}},
year = {1997}
}
@misc{odin2007,
author = {Odin},
title = {{Odin at DARPA}},
url = {http://thefutureofthings.com/3232-boss-wins-darpas-urban-challenge/},
year = {2007}
}
@article{Wang2007,
abstract = {... 26, No. 09, September 2007, pp. 889–916 DOI: 10.1177 / 0278364907081229 c1SAGE Publications 2007 Los Angeles, London, New Delhi and Singapore Figures 1, 2, 16, 18, 21, 23–25, 27–31, 33, 35–37 appear in color online: http://ijr.sagepub.com and the robot. ...},
author = {Wang, C.-C. C and Thorpe, C. and Thrun, S. and Hebert, M. and Durrant-Whyte, H.},
doi = {10.1177/0278364907081229},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2007 - Simultaneous localization, mapping and moving object tracking(2).pdf:pdf},
isbn = {978-1-4244-5038-1},
issn = {0278-3649},
journal = {... Journal of Robotics {\ldots}},
number = {9},
pages = {889--916},
title = {{Simultaneous localization, mapping and moving object tracking}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364907081229{\%}5Cnhttp://ijr.sagepub.com/content/26/9/889.short},
volume = {26},
year = {2007}
}
@inproceedings{Ioffe2015,
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {448--456},
publisher = {JMLR.org},
series = {ICML'15},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://dl.acm.org/citation.cfm?id=3045118.3045167},
year = {2015}
}
@article{Held2012,
abstract = {Detecting cars in real-world images is an important task for autonomous driving, yet it remains unsolved. The system described in this paper takes advantage of context and scale to build a monocular single-frame image-based car detector that significantly outperforms the baseline. The system uses a probabilistic model to combine multiple forms of evidence for both context and scale to locate cars in a real-world image. We also use scale filtering to speed up our algorithm by a factor of 3.3 compared to the baseline. By using a calibrated camera and localization on a road map, we are able to obtain context and scale information from a single image without the use of a 3D laser. The system outperforms the baseline by an absolute 9.4{\%} in overall average precision and 11.7{\%} in average precision for cars smaller than 50 pixels in height, for which context and scale cues are especially important.},
author = {Held, David and Levinson, Jesse and Thrun, Sebastian},
doi = {10.1109/ICRA.2012.6224722},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Held, Levinson, Thrun - 2012 - A Probabilistic Framework for Car Detection in Images using Context and Scale(2).pdf:pdf},
isbn = {9781467314053},
issn = {1050-4729},
journal = {International Conference on Robotics and Automation},
pages = {1628--1634},
title = {{A Probabilistic Framework for Car Detection in Images using Context and Scale}},
year = {2012}
}
@inproceedings{Bautista2016a,
author = {Bautista, Carlo Migel and Dy, Clifford Austin and Manalac, Miguel Inigo and Orbe, Raphael Angelo and Cordel, Macario},
booktitle = {2016 IEEE Region 10 Symposium (TENSYMP)},
doi = {10.1109/TENCONSpring.2016.7519418},
isbn = {978-1-5090-0931-2},
month = {may},
pages = {277--281},
publisher = {IEEE},
title = {{Convolutional neural network for vehicle detection in low resolution traffic videos}},
url = {http://ieeexplore.ieee.org/document/7519418/},
year = {2016}
}
@article{Choi2012,
author = {Choi, Changhyun and Christensen, H I},
doi = {10.1109/IROS.2012.6386067},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Christensen - 2012 - 3D pose estimation of daily objects using an RGB-D camera(2).pdf:pdf},
isbn = {9781467317368},
issn = {2153-0858},
journal = {Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on},
keywords = {cameras;computer vision;feature extraction;image c},
pages = {3342--3349},
title = {{3D pose estimation of daily objects using an RGB-D camera}},
year = {2012}
}
@article{Zhang2013,
abstract = {Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation of our structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.},
author = {Zhang, Lu and van der Maaten, L},
doi = {10.1109/CVPR.2013.240},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, van der Maaten - 2013 - Structure Preserving Object Tracking(2).pdf:pdf},
isbn = {1063-6919 VO  -},
issn = {1063-6919},
journal = {Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on},
keywords = {Bismuth,Detectors,Feature extraction,Mathematical model,SPOT,Support vector machines,Target tracking,bounding-box annotation,computer vision,multiobject model-free tracker,multiple object tracking,object detection,object detectors,object tracking,online structured SVM algorithm,single-object trackers,spatial constraints,structure preserving object tracking,support vector machines},
number = {1},
pages = {1838--1845},
title = {{Structure Preserving Object Tracking}},
year = {2013}
}
@inproceedings{XiaofeiLi2016,
author = {{Xiaofei Li} and Flohr, Fabian and {Yue Yang} and {Hui Xiong} and Braun, Markus and Pan, Shuyue and {Keqiang Li} and Gavrila, Dariu M.},
booktitle = {2016 IEEE Intelligent Vehicles Symposium (IV)},
doi = {10.1109/IVS.2016.7535515},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiaofei Li et al. - 2016 - A new benchmark for vision-based cyclist detection(2).pdf:pdf},
isbn = {978-1-5090-1821-5},
keywords = {cyclist},
mendeley-tags = {cyclist},
month = {jun},
pages = {1028--1033},
publisher = {IEEE},
title = {{A new benchmark for vision-based cyclist detection}},
url = {http://ieeexplore.ieee.org/document/7535515/},
year = {2016}
}
@techreport{Leal-Taixe2015,
abstract = {In the recent past, the computer vision community has developed centralized benchmarks for the performance evaluation of a variety of tasks, including generic object and pedestrian detection, 3D reconstruction, optical flow, single-object short-term tracking, and stereo estimation. Despite potential pitfalls of such benchmarks, they have proved to be extremely helpful to advance the state of the art in the respective area. Interestingly, there has been rather limited work on the standardization of quantitative benchmarks for multiple target tracking. One of the few exceptions is the well-known PETS dataset, targeted primarily at surveillance applications. Despite being widely used, it is often applied inconsistently, for example involving using different subsets of the available data, different ways of training the models, or differing evaluation scripts. This paper describes our work toward a novel multiple object tracking benchmark aimed to address such issues. We discuss the challenges of creating such a framework, collecting existing and new data, gathering state-of-the-art methods to be tested on the datasets, and finally creating a unified evaluation system. With MOTChallenge we aim to pave the way toward a unified evaluation framework for a more meaningful quantification of multi-target tracking.},
archivePrefix = {arXiv},
arxivId = {1504.01942},
author = {Leal-Taix{\'{e}}, Laura and Milan, Anton and Reid, Ian and Roth, Stefan and Schindler, Konrad},
eprint = {1504.01942},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leal-Taix{\'{e}} et al. - 2015 - MOTChallenge 2015 Towards a Benchmark for Multi-Target Tracking(2).pdf:pdf},
pages = {1--15},
title = {{MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking}},
url = {http://arxiv.org/abs/1504.01942},
year = {2015}
}
@inproceedings{Yan2014a,
author = {Yan, Junjie and Lei, Zhen and Wen, Longyin and Li, Stan Z.},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.320},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yan et al. - 2014 - The Fastest Deformable Part Model for Object Detection(2).pdf:pdf},
isbn = {978-1-4799-5118-5},
keywords = {1D correlations,2D correlation,Acceleration,Accuracy,Correlation,DPM,Deformable models,Feature extraction,HOG feature extraction,Pascal VOC,Table lookup,Training,aggressive pruning,approximation theory,cascade part pruning,cascade version,correlation methods,deformable part model,face detection,face detection task,feature extraction,feature map,filtering theory,first order approximation,frame-rate speed,gradient methods,hypothesis pruning,linear combination,look-up tables,matrix algebra,matrix index operations,neighborhood aware cascade,object detection,orientation partition,pedestrian detection task,proximal gradient algorithm,rank filter,root filter},
month = {jun},
pages = {2497--2504},
publisher = {IEEE},
title = {{The Fastest Deformable Part Model for Object Detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909716},
year = {2014}
}
@misc{junior2007,
author = {Junior},
title = {{Junior at DARPA}},
url = {http://www.ros.org/news/2010/03/robots-using-ros-stanfords-junior.html},
year = {2007}
}
@article{Lenz2011,
abstract = {Modern driver assistance systems such as collision avoidance or intersection assistance need reliable information on the current environment. Extracting such information from camera-based systems is a complex and challenging task for inner city traffic scenarios. This paper presents an approach for object detection utilizing sparse scene flow. For consecutive stereo images taken from a moving vehicle, corresponding interest points are extracted. Thus, for every interest point, disparity and optical flow values are known and consequently, scene flow can be calculated. Adjacent interest points describing a similar scene flow are considered to belong to one rigid object. The proposed method does not rely on object classes and allows for a robust detection of dynamic objects in traffic scenes. Leading vehicles are continuously detected for several frames. Oncoming objects are detected within five frames after their appearance.},
author = {Lenz, Philip and Ziegler, Julius and Geiger, Andreas and Roser, Martin},
doi = {10.1109/IVS.2011.5940558},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lenz et al. - 2011 - Sparse scene flow segmentation for moving object detection in urban environments(2).pdf:pdf},
isbn = {9781457708909},
issn = {1931-0587},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
pages = {926--932},
pmid = {5940558},
title = {{Sparse scene flow segmentation for moving object detection in urban environments}},
year = {2011}
}
@article{Wofk2019,
abstract = {Depth sensing is a critical function for robotic tasks such as localization, mapping and obstacle detection. There has been a significant and growing interest in depth estimation from a single RGB image, due to the relatively low cost and size of monocular cameras. However, state-of-the-art single-view depth estimation algorithms are based on fairly complex deep neural networks that are too slow for real-time inference on an embedded platform, for instance, mounted on a micro aerial vehicle. In this paper, we address the problem of fast depth estimation on embedded systems. We propose an efficient and lightweight encoder-decoder network architecture and apply network pruning to further reduce computational complexity and latency. In particular, we focus on the design of a low-latency decoder. Our methodology demonstrates that it is possible to achieve similar accuracy as prior work on depth estimation, but at inference speeds that are an order of magnitude faster. Our proposed network, FastDepth, runs at 178 fps on an NVIDIA Jetson TX2 GPU and at 27 fps when using only the TX2 CPU, with active power consumption under 10 W. FastDepth achieves close to state-of-the-art accuracy on the NYU Depth v2 dataset. To the best of the authors' knowledge, this paper demonstrates real-time monocular depth estimation using a deep neural network with the lowest latency and highest throughput on an embedded platform that can be carried by a micro aerial vehicle.},
archivePrefix = {arXiv},
arxivId = {1903.03273},
author = {Wofk, Diana and Ma, Fangchang and Yang, Tien-Ju and Karaman, Sertac and Sze, Vivienne},
eprint = {1903.03273},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wofk et al. - 2019 - FastDepth Fast Monocular Depth Estimation on Embedded Systems.pdf:pdf},
title = {{FastDepth: Fast Monocular Depth Estimation on Embedded Systems}},
year = {2019}
}
@article{Kwon2014,
author = {Kwon, Junseok and Roh, Junha and Lee, Kyoung Mu and Gool, Luc Van},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kwon et al. - 2014 - LNCS 8689 - Robust Visual Tracking with Double Bounding Box Model(2).pdf:pdf},
pages = {377--392},
title = {{LNCS 8689 - Robust Visual Tracking with Double Bounding Box Model}},
year = {2014}
}
@article{Maresca2014,
author = {Maresca, Mario Edoardo and Petrosino, Alfredo},
doi = {10.1109/CVPRW.2014.128},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maresca, Petrosino - 2014 - The matrioska tracking algorithm on LTDT2014 dataset(2).pdf:pdf},
isbn = {9781479943098},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {720--725},
title = {{The matrioska tracking algorithm on LTDT2014 dataset}},
year = {2014}
}
@article{Hosang2016,
address = {Washington, DC, USA},
author = {Hosang, Jan and Benenson, Rodrigo and Dollar, Piotr and Schiele, Bernt},
doi = {10.1109/TPAMI.2015.2465908},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hosang et al. - 2016 - What Makes for Effective Detection Proposals(2).pdf:pdf},
issn = {0162-8828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = {apr},
number = {4},
pages = {814--830},
publisher = {IEEE Computer Society},
title = {{What Makes for Effective Detection Proposals?}},
url = {http://dx.doi.org/10.1109/TPAMI.2015.2465908},
volume = {38},
year = {2016}
}
@article{Ess2009b,
abstract = {In this paper, we address the problem of vision-based multi-person tracking in busy pedestrian zones using a stereo rig mounted on a mobile platform. Specifically, we are interested in the application of such a system for supporting path planning algorithms in the avoidance of dynamic obstacles. The complexity of the problem calls for an integrated solution, which extracts as much visual information as possible and combines it through cognitive feedback. We propose such an approach, which jointly estimates camera position, stereo depth, object detections, and trajectories based on visual information only. We represent the interplay between these components using a graphical model. For each frame, we first estimate the ground surface together with a set of object detections. Conditioned on these results, we then address object interactions and estimate trajectories. Finally, we employ the tracking results to predict future motion for dynamic objects and fuse this information with a static occupancy map estimated from stereo. The approach is experimentally evaluated on several long and challenging video sequences from busy inner-city locations recorded with different mobile setups. Our results show that the proposed integration makes it possible to deliver stable tracking and motion prediction performance in complex and highly dynamic scenes.},
author = {Ess, a and Leibe, B and Schindler, K and Gool, L Van},
doi = {10.1109/ROBOT.2009.5152884},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ess et al. - 2009 - Moving Obstacle Detection in Highly Dynamic Scenes(2).pdf:pdf},
isbn = {978-1-4244-2788-8},
issn = {1050-4729},
journal = {IEEE Conference on Robotics and Automation},
pages = {56--63},
title = {{Moving Obstacle Detection in Highly Dynamic Scenes}},
year = {2009}
}
@misc{BBC2016,
author = {BBC},
title = {{Artificial intelligence: Google's AlphaGo beats Go master Lee Se-dol}},
url = {http://www.bbc.com/news/technology-35785875},
year = {2016}
}
@article{Gavrila2007,
abstract = {This paper presents a multi-cue vision system for the real-time detection and tracking of pedestrians from a moving vehicle. The detection component involves a cascade of modules, each utilizing complementary visual criteria to successively narrow down the image search space, balancing robustness and efficiency considera-tions. Novel is the tight integration of the consecutive modules: (sparse) stereo-based ROI generation, shape-based detection, texture-based classification and (dense) stereo-based verification. For example, shape-based detection activates a weighted combination of texture-based classifiers, each attuned to a particular body pose. Performance of individual modules and their interaction is analyzed by means of Receiver Operator Characteristics (ROCs). A sequential optimization technique allows the successive combination of individual ROCs, providing opti-mized system parameter settings in a systematic fashion, avoiding ad-hoc parameter tuning. Application-dependent processing constraints can be incorporated in the optimization procedure. Results from extensive field tests in difficult urban traffic conditions suggest system performance is at the leading edge.},
author = {Gavrila, D. M. and Munder, S.},
doi = {10.1007/s11263-006-9038-7},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gavrila, Munder - 2007 - Multi-cue pedestrian detection and tracking from a moving vehicle(2).pdf:pdf},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Intelligent vehicles,Multiple visual cues,Pedestrian detection},
number = {1},
pages = {41--59},
title = {{Multi-cue pedestrian detection and tracking from a moving vehicle}},
volume = {73},
year = {2007}
}
@inproceedings{VandeSande2011,
author = {van de Sande, Koen E. A. and Uijlings, Jasper R. R. and Gevers, Theo and Smeulders, Arnold W. M.},
booktitle = {2011 International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126456},
isbn = {978-1-4577-1102-2},
month = {nov},
pages = {1879--1886},
publisher = {IEEE},
title = {{Segmentation as selective search for object recognition}},
url = {http://ieeexplore.ieee.org/document/6126456/},
year = {2011}
}
@inproceedings{Cheng2014,
abstract = {Training a generic objectness measure to produce a small set of candidate object windows, has been shown to speed up the classical sliding window object detection paradigm. We observe that generic objects with welldefined closed boundary, share surprisingly strong correlation in normed gradients space, when resizing their corresponding image windows into a small fixed size. Based on this observation and computational reasons, we propose to resize an image window to 8 8 and use the normed gradients as a simple 64D feature to describe it, for explicitly training a generic objectness measure. We further show how the binarized version of this feature, namely binarized normed gradients (BING), can be used for efficient objectness estimation, which requires only a few atomic operations (e.g. ADD, BITWISE SHIFT, etc.). Experiments on the challenging PASCAL VOC 2007 dataset show that our method efficiently (300fps on a single laptop CPU) generates a small set of category-independent, high quality object windows, yielding 96:2{\%} object detection rate (DR) with 1,000 proposals. With increase of the numbers of proposals and color spaces for computing BING features, our performance can be further improved to 99:5{\%} DR.},
author = {Cheng, Ming Ming and Zhang, Ziming and Lin, Wen Yan and Torr, Philip},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.414},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng et al. - 2014 - BING Binarized normed gradients for objectness estimation at 300fps(2).pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
keywords = {Objectness,binary coding,detection,proposals,realtime,saliency,visual attention},
title = {{BING: Binarized normed gradients for objectness estimation at 300fps}},
year = {2014}
}
@misc{SUBARU2016,
author = {SUBARU},
title = {{Subaru EyeSight}},
url = {http://www.subaru.com.au/eyesight-technology},
year = {2016}
}
@inproceedings{Redmon2016a,
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.91},
isbn = {978-1-4673-8851-1},
month = {jun},
pages = {779--788},
publisher = {IEEE},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://ieeexplore.ieee.org/document/7780460/},
year = {2016}
}
@techreport{Duffy2017,
author = {Duffy, Benjamin F. and Flynn, Daniel R.},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Duffy, Flynn - 2017 - A Year in Computer Vision(2).pdf:pdf},
institution = {The M Tank},
pages = {56},
title = {{A Year in Computer Vision}},
url = {http://www.themtank.org/pdfs/AYearofComputerVisionPDF.pdf},
year = {2017}
}
@article{Felzenszwalb2010,
author = {Felzenszwalb, P F and Girshick, R B and McAllester, D and Ramanan, D},
doi = {10.1109/TPAMI.2009.167},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Felzenszwalb et al. - 2010 - Object Detection with Discriminatively Trained Part-Based Models(2).pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Computer-Assisted,Discriminant Analysis,Image Enhancement,Image Interpretation,Imaging,Object recognition,PASCAL object detection,Pattern Recognition,Reproducibility of Results,Sensitivity and Specificity,Three-Dimensional,data mining,deformable models,discriminative trained part-based models,discriminative training,iterative methods,iterative training algorithm,latent SVM objective function,latent SVM.,margin-sensitive approach,multiscale deformable part models,object detection,object detection system,object recognition,pictorial structures,support vector machine,support vector machines},
month = {sep},
number = {9},
pages = {1627--1645},
publisher = {IEEE},
title = {{Object Detection with Discriminatively Trained Part-Based Models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5255236},
volume = {32},
year = {2010}
}
@article{Redmon2015,
abstract = {We present YOLO, a unified pipeline for object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is also extremely fast; YOLO processes images in real-time at 45 frames per second, hundreds to thousands of times faster than existing detection systems. Our system uses global image context to detect and localize objects, making it less prone to background errors than top detection systems like R-CNN. By itself, YOLO detects objects at unprecedented speeds with moderate accuracy. When combined with state-of-the-art detectors, YOLO boosts performance by 2-3{\%} points mAP.},
archivePrefix = {arXiv},
arxivId = {1506.02640v1},
author = {Redmon, Joseph and Girshick, Ross and Farhadi, Ali},
doi = {10.1016/j.nima.2015.05.028},
eprint = {1506.02640v1},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon, Girshick, Farhadi - 2015 - You Only Look Once Unified, Real-Time Object Detection(2).pdf:pdf},
journal = {arXiv},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
year = {2015}
}
@inproceedings{Viola2003a,
author = {Viola, P. and Jones, M. J. and Snow, D.},
booktitle = {Proceedings Ninth IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2003.1238422},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Viola, Jones, Snow - 2003 - Detecting pedestrians using patterns of motion and appearance(2).pdf:pdf},
isbn = {0-7695-1950-4},
keywords = {15 pixels,20 pixels,300 pixels,AdaBoost,Detectors,Face detection,Humans,Image resolution,Motion analysis,Motion detection,Object detection,Pattern recognition,Rain,Snow,computer vision,detection style algorithm,detector scanning,feature extraction,image intensity information,image motion analysis,image motion representation,image representation,image resolution,image sequences,low resolution images,motion appearance,motion information,motion patterns,object detection,pedestrian detection,video sequence frames,walking person},
pages = {734--741 vol.2},
publisher = {IEEE},
title = {{Detecting pedestrians using patterns of motion and appearance}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1238422},
year = {2003}
}
@article{Wu2011a,
abstract = {CENsus TRansform hISTogram (CENTRIST), a new visual descriptor for recognizing topological places or scene categories, is introduced in this paper. We show that place and scene recognition, especially for indoor environments, require its visual descriptor to possess properties that are different from other vision domains (e.g., object recognition). CENTRIST satisfies these properties and suits the place and scene recognition task. It is a holistic representation and has strong generalizability for category recognition. CENTRIST mainly encodes the structural properties within an image and suppresses detailed textural information. Our experiments demonstrate that CENTRIST outperforms the current state of the art in several place and scene recognition data sets, compared with other descriptors such as SIFT and Gist. Besides, it is easy to implement and evaluates extremely fast.},
author = {Wu, Jianxin and Rehg, Jim M.},
doi = {10.1109/TPAMI.2010.224},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Rehg - 2011 - CENTRIST A visual descriptor for scene categorization(2).pdf:pdf},
isbn = {0162-8828 VO  - 33},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Census Transform,Gist,Place recognition,SIFT,scene recognition,visual descriptor},
number = {8},
pages = {1489--1501},
pmid = {21173449},
title = {{CENTRIST: A visual descriptor for scene categorization}},
volume = {33},
year = {2011}
}
@inproceedings{YanKe2004,
author = {{Yan Ke}, Yan and Sukthankar, R.},
booktitle = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
doi = {10.1109/CVPR.2004.1315206},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yan Ke, Sukthankar - 2004 - PCA-SIFT a more distinctive representation for local image descriptors(2).pdf:pdf},
isbn = {0-7695-2158-4},
keywords = {Computer science,Computer vision,Filters,Histograms,Image registration,Image retrieval,Object detection,Object recognition,Principal component analysis,Robustness,feature extraction,image deformations,image gradient,image registration,image representation,image retrieval,image retrieval application,local feature detection,local image descriptor,object recognition,object recognition algorithms,principal component analysis,principal components analysis},
pages = {506--513},
publisher = {IEEE},
title = {{PCA-SIFT: a more distinctive representation for local image descriptors}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1315206},
volume = {2},
year = {2004}
}
@article{Redmon2016,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1612.08242},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon, Farhadi - 2016 - YOLO9000 Better, Faster, Stronger(2).pdf:pdf},
month = {dec},
title = {{YOLO9000: Better, Faster, Stronger}},
url = {http://arxiv.org/abs/1612.08242},
year = {2016}
}
@article{Zhang2014a,
abstract = {We propose a multi-expert restoration scheme to address the model drift problem in online tracking. In the proposed scheme, a tracker and its historical snapshots constitute an expert ensemble, where the best expert is selected to restore the current tracker when needed based on a minimum entropy criterion, so as to correct undesirable model updates. The base tracker in our formulation exploits an online SVM on a budget algorithm and an explicit feature mapping method for efficient model update and inference. In experiments, our tracking method achieves substantially better overall performance than 32 trackers on a benchmark dataset of 50 video sequences under various evaluation settings. In addition, in experiments with a newly collected dataset of challenging sequences, we showthat the proposed multi-expert restoration scheme significantly improves the robustness of our base tracker, especially in scenarios with frequent occlu- sions and repetitive appearance variations},
author = {Zhang, Jianming and Ma, Shugao and Sclaroff, Stan},
doi = {10.1007/978-3-319-10599-4_13},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Ma, Sclaroff - 2014 - Robust tracking via multiple experts(2).pdf:pdf},
isbn = {978-3-319-10598-7},
journal = {Eccv},
keywords = {and stan sclaroff,boston university,department of computer science,entropy minimization,experts using,jianming zhang,m,robust tracking via multiple,shugao ma,usa},
pages = {188--203},
title = {{Robust tracking via multiple experts}},
url = {http://www.cs.bu.edu/groups/ivc/software/MEEM/},
year = {2014}
}
@article{Ma2014,
abstract = {In this paper, we present an unsupervised framework for dis- covering, detecting, tracking, and reconstructing dense objects from a video sequence. The system simultaneously localizes a moving camera, and discovers a set of shape and appearance models for multiple objects, including the scene background. Each object model is represented by both a 2D and 3D level-set. This representation is used to improve detec- tion, 2D-tracking, 3D-registration and importantly subsequent updates to the level-set itself. This single framework performs dense simultaneous localization and mapping as well as unsupervised object discovery. At each iteration portions of the scene that fail to track, such as bulk outliers on moving rigid bodies, are used to either seed models for new objects or to update models of known objects. For the latter, once an object is successfully tracked in 2D with aid from a 2D level-set segmentation, the level-set is updated and then used to aid registration and evolution of a 3D level-set that captures shape information. For a known object either learned by our system or introduced from a third-party library, our framework can detect similar appearances and geometries in the scene. The system is tested using single and multiple object data sets. Results demonstrate an improved method for discovering and reconstructing 2D and 3D object models, which aid tracking even under significant occlusion or rapid motion.},
author = {Ma, Lu and Sibley, Gabe},
doi = {10.1007/978-3-319-10605-2_6},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ma, Sibley - 2014 - Unsupervised dense object discovery, detection, tracking and reconstruction(2).pdf:pdf},
isbn = {9783319106045},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {3D Reconstruction,3D Tracking,Dense Reconstruction,Learning,Level-Set Evolution,SLAM,Structure From Motion},
number = {PART 2},
pages = {80--95},
title = {{Unsupervised dense object discovery, detection, tracking and reconstruction}},
volume = {8690 LNCS},
year = {2014}
}
@article{Kendall2015,
abstract = {We present a novel deep learning framework for probabilistic pixel-wise semantic segmentation, which we term Bayesian SegNet. Pixel-wise semantic segmentation is an important step for visual scene understanding. It is a complex task requiring knowledge of support relationships and contextual information, as well as visual appearance. Our contribution is a practical system which is able to predict pixel-wise class labels with a measure of model uncertainty. We achieve this by Monte Carlo sampling with dropout at test time to generate a posterior distribution of pixel class labels. We show this Bayesian neural network provides a significant performance improvement in segmentation, with no additional parameterisation. We set a new benchmark with state-of-the-art performance on both the indoor SUN Scene Understanding and outdoor CamVid driving scenes datasets. Bayesian SegNet also performs competitively on Pascal VOC 2012 object segmentation challenge. For our web demo and source code, see http://mi.eng.cam.ac.uk/projects/segnet/},
archivePrefix = {arXiv},
arxivId = {1511.02680},
author = {Kendall, Alex and Badrinarayanan, Vijay and Cipolla, Roberto},
eprint = {1511.02680},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kendall, Badrinarayanan, Cipolla - 2015 - Bayesian SegNet model uncertainty in deep convolutional encoder-decoder architectures for s(2).pdf:pdf},
journal = {arXiv},
title = {{Bayesian SegNet: model uncertainty in deep convolutional encoder-decoder architectures for scene understanding}},
url = {http://arxiv.org/abs/1511.02680},
year = {2015}
}
@article{Wojek2009,
abstract = {Various powerful people detection methods exist. Surprisingly, most approaches rely on static image features only despite the obvious potential of motion information for people detection. This paper systematically evaluates different features and classifiers in a sliding-window framework. First, our experiments indicate that incorporating motion information improves detection performance significantly. Second, the combination of multiple and complementary feature types can also help improve performance. And third, the choice of the classifier-feature combination and several implementation details are crucial to reach best performance. In contrast to many recent papers experimental results are reported for four different datasets rather than using a single one. Three of them are taken from the literature allowing for direct comparison. The fourth dataset is newly recorded using an onboard camera driving through urban environment. Consequently this dataset is more realistic and more challenging than any currently available dataset.},
author = {Wojek, Christian and Walk, Stefan and Schiele, Bernt},
doi = {10.1109/CVPRW.2009.5206638},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wojek, Walk, Schiele - 2009 - Multi-Cue onboard pedestrian detection(2).pdf:pdf},
isbn = {9781424439935},
issn = {1063-6919},
journal = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
pages = {794--801},
title = {{Multi-Cue onboard pedestrian detection}},
year = {2009}
}
@article{Blaschko2008,
author = {Blaschko, Matthew B and Lampert, Christoph H},
doi = {10.1007/978-3-540-88682-2_2},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blaschko, Lampert - 2008 - Learning to Localize Objects with Structured Output Regression(2).pdf:pdf},
isbn = {978-3-540-88681-5},
pages = {2--15},
title = {{Learning to Localize Objects with Structured Output Regression}},
year = {2008}
}
@article{Paszke2016,
abstract = {The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18{\$}\backslashtimes{\$} faster, requires 75{\$}\backslashtimes{\$} less FLOPs, has 79{\$}\backslashtimes{\$} less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.},
archivePrefix = {arXiv},
arxivId = {1606.02147},
author = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
eprint = {1606.02147},
month = {jun},
title = {{ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation}},
url = {http://arxiv.org/abs/1606.02147},
year = {2016}
}
@article{lbp1990,
author = {{Dong-chen He} and {Li Wang}},
doi = {10.1109/TGRS.1990.572934},
issn = {0196-2892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
month = {jul},
number = {4},
pages = {509--512},
title = {{Texture Unit, Texture Spectrum, And Texture Analysis}},
url = {http://ieeexplore.ieee.org/document/572934/},
volume = {28},
year = {1990}
}
@article{Pedersoli2014,
abstract = {Face detection and facial points localization are interconnected tasks. Recently it has been shown that solving these two tasks jointly with a mixture of trees of parts (MTP) leads to state-of-the-art results. However, MTP, as most other methods for facial point localization proposed so far, requires a complete annotation of the training data at facial point level. This is used to predefine the structure of the trees and to place the parts correctly. In this work we extend the mixtures from trees to more general loopy graphs. In this way we can learn in a weakly supervised manner (using only the face location and orientation) a powerful deformable detector that implicitly aligns its parts to the detected face in the image. By attaching some reference points to the correct parts of our detector we can then localize the facial points. In terms of detection our method clearly outperforms the state-of-the-art, even if competing with methods that use facial point annotations during training. Additionally, without any facial point annotation at the level of individual training images, our method can localize facial points with an accuracy similar to fully supervised approaches.},
author = {Pedersoli, Marco and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1109/CVPR.2014.472},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pedersoli, Tuytelaars, Van Gool - 2014 - Using a deformation field model for localizing faces and facial points under weak supervisio(2).pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3694--3701},
title = {{Using a deformation field model for localizing faces and facial points under weak supervision}},
year = {2014}
}
@article{Montabone2010,
author = {Montabone, S and Soto, A},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Montabone, Soto - 2010 - Human Detection Using a Mobile Platform and Novel Features Derived From a Visual Saliency Mechanism(2).pdf:pdf},
journal = {Image and Vision Computing},
keywords = {corresponding author,human detection,moving cameras,visual features,visual saliency},
number = {3},
pages = {391--402},
title = {{Human Detection Using a Mobile Platform and Novel Features Derived From a Visual Saliency Mechanism}},
volume = {28},
year = {2010}
}
@article{Of2012,
author = {Of, State and Art, T H E},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Of, Art - 2012 - Fast and Robust Cyclist Detection for Monocular Camera Systems(2).pdf:pdf},
journal = {VISAPP 2015 - 10th International Conference on Computer Vision Theory and Applications},
pages = {3--8},
title = {{Fast and Robust Cyclist Detection for Monocular Camera Systems}},
year = {2012}
}
@article{Leonard2008,
author = {Leonard, John and How, Jonathan and Teller, Seth and Berger, Mitch and Campbell, Stefan and Fiore, Gaston and Fletcher, Luke and Frazzoli, Emilio and Huang, Albert and Karaman, Sertac and Koch, Olivier and Kuwata, Yoshiaki and Moore, David and Olson, Edwin and Peters, Steve and Teo, Justin and Truax, Robert and Walter, Matthew and Barrett, David and Epstein, Alexander and Maheloni, Keoni and Moyer, Katy and Jones, Troy and Buckley, Ryan and Antone, Matthew and Galejs, Robert and Krishnamurthy, Siddhartha and Williams, Jonathan},
doi = {10.1002/rob.20262},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leonard et al. - 2008 - A perception-driven autonomous urban vehicle(2).pdf:pdf},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {oct},
number = {10},
pages = {727--774},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
title = {{A perception-driven autonomous urban vehicle}},
url = {http://doi.wiley.com/10.1002/rob.20262},
volume = {25},
year = {2008}
}
@article{Wen-ChungChang2010,
author = {{Wen-Chung Chang} and {Chih-Wei Cho}},
doi = {10.1109/TSMCB.2009.2032527},
issn = {1083-4419},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
month = {jun},
number = {3},
pages = {892--902},
title = {{Online Boosting for Vehicle Detection}},
url = {http://ieeexplore.ieee.org/document/5325813/},
volume = {40},
year = {2010}
}
@inproceedings{Piao16,
author = {Piao, Songlin and Kiekbusch, Lisa and Schmidt, Daniel and Berns, Karsten and Hering, Daniel and Wirtz, Stefan and Hering, Nils and Weiland, J{\"{u}}rgen},
booktitle = {Commercial Vehicle Technology 2016 -- Proceedings of the Commercial Vehicle Technology Symposium (CVT 2016)},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Piao et al. - 2016 - Real-time multi-platform pedestrian detection in a heavy duty driver assistance system(2).pdf:pdf},
month = {mar},
pages = {61--70},
title = {{Real-time multi-platform pedestrian detection in a heavy duty driver assistance system}},
year = {2016}
}
@inproceedings{Cordts2016Cityscapes,
author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
booktitle = {Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{The Cityscapes Dataset for Semantic Urban Scene Understanding}},
year = {2016}
}
@article{Wangsiripitak2009,
abstract = {To work at video rate, the maps that monocular SLAM builds are bound to be sparse, making them sensitive to the erroneous inclusion of moving points and to the deletion of valid points through temporary occlusion. This paper describes the parallel implementation of monoSLAM with a 3D object tracker, allowing reasoning about moving objects and occlusion. The SLAM process provides the object tracker with information to register objects to the map's frame, and the object tracker allows the marking of features, either those on objects, or those created by their occluding edges, or those occluded by objects. Experiments are presented to verify the recovered geometry and to indicate the impact on camera pose in monoSLAM of including and avoiding moving features.},
author = {Wangsiripitak, Somkiat and Murray, David W.},
doi = {10.1109/ROBOT.2009.5152290},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wangsiripitak, Murray - 2009 - Avoiding moving outliers in visual slam by tracking moving objects(2).pdf:pdf},
isbn = {9781424427895},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {375--380},
pmid = {5152290},
title = {{Avoiding moving outliers in visual slam by tracking moving objects}},
year = {2009}
}
@article{Ge2009,
abstract = {Pedestrian detection is one of the most important components in driver-assistance systems. In this paper, we propose a monocular vision system for real-time pedestrian detection and tracking during nighttime driving with a near-infrared (NIR) camera. Three modules (region-of-interest (ROI) generation, object classification, and tracking) are integrated in a cascade, and each utilizes complementary visual features to distinguish the objects from the cluttered background in the range of 20-80 m. Based on the common fact that the objects appear brighter than the nearby background in nighttime NIR images, efficient ROI generation is done based on the dual-threshold segmentation algorithm. As there is large intraclass variability in the pedestrian class, a tree-structured, two-stage detector is proposed to tackle the problem through training separate classifiers on disjoint subsets of different image sizes and arranging the classifiers based on Haar-like and histogram-of-oriented-gradients (HOG) features in a coarse-to-fine manner. To suppress the false alarms and fill the detection gaps, template-matching-based tracking is adopted, and multiframe validation is used to obtain the final results. Results from extensive tests on both urban and suburban videos indicate that the algorithm can produce a detection rate of more than 90{\%} at the cost of about 10 false alarms/h and perform as fast as the frame rate (30 frames/s) on a Pentium IV 3.0-GHz personal computer, which also demonstrates that the proposed system is feasible for practical applications and enjoys the advantage of low implementation cost.},
author = {Ge, Junfeng and Luo, Yupin and Tei, Gyomei},
doi = {10.1109/TITS.2009.2018961},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ge, Luo, Tei - 2009 - Real-time pedestrian detection and tracking at nighttime for driver-assistance systems(2).pdf:pdf},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {AdaBoost,Histogram of oriented gradients (HOG),Kalman filter,Near infrared camera,Pedestrian detection,Template matching},
number = {2},
pages = {283--298},
title = {{Real-time pedestrian detection and tracking at nighttime for driver-assistance systems}},
volume = {10},
year = {2009}
}
@article{Urmson2008,
author = {Urmson, Chris and Anhalt, Joshua and Bagnell, Drew and Baker, Christopher and Bittner, Robert and Clark, M. N. and Dolan, John and Duggins, Dave and Galatali, Tugrul and Geyer, Chris and Gittleman, Michele and Harbaugh, Sam and Hebert, Martial and Howard, Thomas M. and Kolski, Sascha and Kelly, Alonzo and Likhachev, Maxim and McNaughton, Matt and Miller, Nick and Peterson, Kevin and Pilnick, Brian and Rajkumar, Raj and Rybski, Paul and Salesky, Bryan and Seo, Young-Woo and Singh, Sanjiv and Snider, Jarrod and Stentz, Anthony and Whittaker, William Red and Wolkowicki, Ziv and Ziglar, Jason and Bae, Hong and Brown, Thomas and Demitrish, Daniel and Litkouhi, Bakhtiar and Nickolaou, Jim and Sadekar, Varsha and Zhang, Wende and Struble, Joshua and Taylor, Michael and Darms, Michael and Ferguson, Dave},
doi = {10.1002/ROB.V25:8},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Urmson et al. - 2008 - Autonomous driving in urban environments Boss and the Urban Challenge(2).pdf:pdf},
issn = {15564959},
journal = {Journal of Field Robotics},
number = {8},
pages = {425--466},
publisher = {John Wiley and Sons Ltd.},
title = {{Autonomous driving in urban environments: Boss and the Urban Challenge}},
volume = {25},
year = {2008}
}
@article{Zuo2014,
author = {Zuo, Tianyu},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zuo - 2014 - An Efficient Vision-Based Pedestrian Detection and Tracking System for ITS Applications(2).pdf:pdf},
title = {{An Efficient Vision-Based Pedestrian Detection and Tracking System for ITS Applications}},
year = {2014}
}
@article{Modeling,
author = {Modeling, Patch-based Dynamic Appearance and Kwon, Junseok and Member, Student and Lee, Kyoung Mu},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Modeling et al. - Unknown - Highly Non-Rigid Object Tracking via(2).pdf:pdf},
issn = {0162-8828},
pages = {1--14},
title = {{Highly Non-Rigid Object Tracking via}}
}
@article{Pohlen2016,
abstract = {Semantic image segmentation is an essential component of modern autonomous driving systems, as an accurate understanding of the surrounding scene is crucial to navigation and action planning. Current state-of-the-art approaches in semantic image segmentation rely on pre-trained networks that were initially developed for classifying images as a whole. While these networks exhibit outstanding recognition performance (i.e., what is visible?), they lack localization accuracy (i.e., where precisely is something located?). Therefore, additional processing steps have to be performed in order to obtain pixel-accurate segmentation masks at the full image resolution. To alleviate this problem we propose a novel ResNet-like architecture that exhibits strong localization and recognition performance. We combine multi-scale context with pixel-level accuracy by using two processing streams within our network: One stream carries information at the full image resolution, enabling precise adherence to segment boundaries. The other stream undergoes a sequence of pooling operations to obtain robust features for recognition. The two streams are coupled at the full image resolution using residuals. Without additional processing steps and without pre-training, our approach achieves an intersection-over-union score of 71.8{\%} on the Cityscapes dataset.},
archivePrefix = {arXiv},
arxivId = {1611.08323},
author = {Pohlen, Tobias and Hermans, Alexander and Mathias, Markus and Leibe, Bastian},
eprint = {1611.08323},
month = {nov},
title = {{Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes}},
url = {http://arxiv.org/abs/1611.08323},
year = {2016}
}
@article{Kylberg2013,
author = {Kylberg, Gustaf and Sintorn, Ida-Maria},
doi = {10.1186/1687-5281-2013-17},
issn = {1687-5281},
journal = {EURASIP Journal on Image and Video Processing},
month = {dec},
number = {1},
pages = {17},
title = {{Evaluation of noise robustness for local binary pattern descriptors in texture classification}},
url = {https://jivp-eurasipjournals.springeropen.com/articles/10.1186/1687-5281-2013-17},
volume = {2013},
year = {2013}
}
@article{Franke2009,
author = {Franke, Uwe and Pfeiffer, David},
doi = {10.1007/978-3-642-03798-6},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Franke, Pfeiffer - 2009 - The Stixel World - A Compact Medium Level Representation of the 3D-World(2).pdf:pdf},
isbn = {978-3-642-03797-9},
issn = {0302-9743},
pages = {1--10},
title = {{The Stixel World - A Compact Medium Level Representation of the 3D-World}},
year = {2009}
}
@article{Conference2011,
author = {Conference, Ieee International and Processing, Image},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Conference, Processing - 2011 - FAST HUMAN DETECTION USING NODE-COMBINED PART DETECTOR Song CAO Department of Electronic Engineering,(2).pdf:pdf},
isbn = {9781457713026},
pages = {3650--3653},
title = {{FAST HUMAN DETECTION USING NODE-COMBINED PART DETECTOR Song CAO Department of Electronic Engineering, Tsinghua University, Beijing 100084, China}},
year = {2011}
}
@article{Beleznai2009,
author = {Beleznai, C and Bischof, H},
doi = {10.1109/CVPR.2009.5206564},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Beleznai, Bischof - 2009 - Fast human detection in crowded scenes by contour integration and local shape estimation(2).pdf:pdf},
isbn = {978-1-4244-3992-8},
journal = {IEEE Conference on Computer Vision and Pattern Recognition CVPR},
pages = {2246--2253},
title = {{Fast human detection in crowded scenes by contour integration and local shape estimation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206564},
year = {2009}
}
@inproceedings{TongLi2010,
author = {{Tong Li} and Cao, Xianbin and {Yanwu Xu}},
booktitle = {2010 8th World Congress on Intelligent Control and Automation},
doi = {10.1109/WCICA.2010.5554979},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tong Li, Cao, Yanwu Xu - 2010 - An effective crossing cyclist detection on a moving vehicle(2).pdf:pdf},
isbn = {978-1-4244-6712-9},
keywords = {cyclist},
mendeley-tags = {cyclist},
month = {jul},
pages = {368--372},
publisher = {IEEE},
title = {{An effective crossing cyclist detection on a moving vehicle}},
url = {http://ieeexplore.ieee.org/document/5554979/},
year = {2010}
}
@article{Liu2009,
abstract = {The focus of motion analysis has been on estimating a flow vector for every pixel by matching intensities. In my thesis, I will explore motion representations beyond the pixel level and new applications to which these representations lead. I first focus on analyzing motion from video sequences. Traditional motion analysis suffers from the inappropriate modeling of the grouping relationship of pixels and from a lack of ground-truth data. Using layers as the interface for humans to interact with videos, we build a human-assisted motion annotation system to obtain ground-truth motion, missing in the literature, for natural video sequences. Furthermore, we show that with the layer representation, we can detect and magnify small motions to make them visible to human eyes. Then we move to a contour presentation to analyze the motion for textureless objects under occlusion. We demonstrate that simultaneous boundary grouping and motion analysis can solve challenging data, where the traditional pixel-wise motion analysis fails. In the second part of my thesis, I will show the benefits of matching local image structures instead of intensity values. We propose SIFT flow that establishes dense, semantically meaningful correspondence between two images across scenes by matching pixel-wise SIFT features. Using SIFT flow, we develop a new framework for image parsing by transferring the metadata information, such as annotation, motion and depth, from the images in a large database to an unknown query image. We demonstrate this framework using new applications such as predicting motion from a single image and motion synthesis via object transfer.},
author = {Liu, Ce and Adviser-Freeman, W.T. and Adviser-Adelson, E.H.},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Adviser-Freeman, Adviser-Adelson - 2009 - Beyond pixels exploring new representations and applications for motion analysis(2).pdf:pdf},
journal = {Proceedings of the 10th European Conference on Computer Vision: Part III},
pages = {28--42},
title = {{Beyond pixels: exploring new representations and applications for motion analysis}},
year = {2009}
}
@article{Liub,
author = {Liu, Wei and Hua, Gang and Smith, John R},
doi = {10.1109/CVPR.2014.483},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Hua, Smith - Unknown - Unsupervised One-Class Learning for Automatic Outlier Removal(2).pdf:pdf},
title = {{Unsupervised One-Class Learning for Automatic Outlier Removal}}
}
@article{Bailer2014,
abstract = {Abstract. General object tracking is a challenging problem, where each tracking algorithm performs well on different sequences. This is because each of them has different strengths and weaknesses. We show that this fact can be utilized to create a fusion approach that clearly outperforms the best tracking algorithms in tracking performance. Thanks to dy- namic programming based trajectory optimization we cannot only out- perform tracking algorithms in accuracy but also in other important aspects like trajectory continuity and smoothness. Our fusion approach is very generic as it only requires frame-based tracking results in form of the object's bounding box as input and thus can work with arbitrary tracking algorithms. It is also suited for live tracking. We evaluated our approach using 29 different algorithms on 51 sequences and show the su- periority of our approach compared to state-of-the-art tracking methods},
author = {Bailer, Christian and Pagani, Alain and Stricker, Didier},
doi = {10.1007/978-3-319-10584-0_12},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bailer, Pagani, Stricker - 2014 - A superior tracking approach Building a strong tracker through fusion(2).pdf:pdf},
isbn = {9783319105833},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Data Fusion,Object Tracking},
number = {PART 7},
pages = {170--185},
title = {{A superior tracking approach: Building a strong tracker through fusion}},
volume = {8695 LNCS},
year = {2014}
}
@article{Enzweiler2008,
abstract = {This paper presents a novel focus-of-attention strategy for monocular pedestrian recognition. It uses Bayes rule to estimate the posterior for the presence of a pedestrian in a certain (rectangular) image region, based on motion parallax features. This posterior is used as a parameter to control the amount of regions of interest (ROIs) that is passed to subsequent verification stages. For the latter, we use a state-of- the-art pedestrian recognition scheme which consists of multiple modules in a cascade architecture.},
author = {Enzweiler, M and Kanter, P and Gavrila, D M},
doi = {10.1109/IVS.2008.4621169},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Enzweiler, Kanter, Gavrila - 2008 - Monocular pedestrian recognition using motion parallax(2).pdf:pdf},
isbn = {9781424425686},
journal = {IEEE Intelligent Vehicles Symposium},
pages = {792--797},
title = {{Monocular pedestrian recognition using motion parallax}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4621169},
year = {2008}
}
@article{Kuo,
author = {Kuo, Cheng-hao and Nevatia, Ram},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuo, Nevatia - Unknown - How does Person Identity Recognition Help Multi-Person Tracking(2).pdf:pdf},
title = {{How does Person Identity Recognition Help Multi-Person Tracking ?}}
}
@article{Westenberger2013,
abstract = {Automated driving applications require an environment perception that is reliable and fast. Multi-sensor fusion is a suitable means to combine the advantages of different measurement principles. However, this may lead to out-of-sequence measurements, i.e., asynchronous measurements where the original order of the measurements is lost. High-performance out-of-sequence algorithms are therefore needed that do not depend on the order of the measurements. In addition, existence probabilities can increase the reliability of the fusion system especially in safety critical applications. This paper presents a novel approach to handle out-of-sequence measurements not only in state estimation, but also in existence estimation. The method is shown to result in equal or less computational costs than state-of-the-art methods. The proposed algorithm is evaluated with real world data from crash tests.},
author = {Westenberger, Antje and Waldele, Steffen and Dora, Balaganesh and Duraisamy, Bharanidhar and Muntzinger, Marc and Dietmayer, Klaus},
doi = {10.1109/ICRA.2013.6631147},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Westenberger et al. - 2013 - Multi-sensor fusion with out-of-sequence measurements for vehicle environment perception(2).pdf:pdf},
isbn = {9781467356411},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {4042--4047},
title = {{Multi-sensor fusion with out-of-sequence measurements for vehicle environment perception}},
year = {2013}
}
@article{munder06,
author = {Munder, S and Gavrila, D M},
doi = {10.1109/TPAMI.2006.217},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Analysis of variance,Application software,Artificial Intelligence,Automated,Biometry,Cluster Analysis,Computer vision,Computer-Assisted,Feedforward neural networks,Haar transforms,Haar wavelets,Humans,Image Enhancement,Image Interpretation,Information Storage and Retrieval,Neural networks,Pattern Recognition,Pedestrian classification,Performance analysis,Principal component analysis,Reproducibility of Results,Sensitivity and Specificity,Support vector machine classification,Support vector machines,Testing,Walking,automatic bootstrapping,cascade techniques,classifier evaluation,computer vision,feature evaluation,feedforward neural nets,feedforward neural networks,image detection,image recognition,k-nearest neighbor classifier,local receptive fields,pedestrian classification,performance analysis.,principal component analysis,support vector machines,traffic engineering computing},
month = {nov},
number = {11},
pages = {1863--1868},
title = {{An Experimental Study on Pedestrian Classification}},
volume = {28},
year = {2006}
}
@article{Tomasi1991,
abstract = {The factorization method described in this series of reports requires an algorithm to track the motion of features in an image stream. Given the small inter-frame displacement made possible by the factorization approach, the best tracking method turns out to be the one proposed by Lucas and Kanade in 1981. The method defines the measure of match between fixed-size feature windows in the past and current frame as the sum of squared intensity differences over the windows. The displacement is then defined as the one that minimizes this sum. For small motions, a linearization of the image intensities leads to a Newton-Raphson style minimization. In this report, after rederiving the method in a physically intuitive way, we answer the crucial question of how to choose the feature windows that are best suited for tracking. Our selection criterion is based directly on the definition of the tracking algorithm, and expresses how well a feature can be tracked. As a result, the criterion is optimal by construction. We show by experiment that the performance of both the selection and the tracking algorithm are adequate for our factorization method, and we address the issue of how to detect occlusions. In the conclusion, we point out specific open questions for future research.},
author = {Tomasi, Carlo},
doi = {10.1016/S0031-3203(03)00234-6},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tomasi - 1991 - Detection and Tracking of Point Features Technical Report CMU-CS-91-132(2).pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Image Rochester NY},
number = {April},
pages = {1--22},
title = {{Detection and Tracking of Point Features Technical Report CMU-CS-91-132}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.131.5899{\&}rep=rep1{\&}type=pdf},
volume = {91},
year = {1991}
}
@article{Piao2010,
author = {Piao, Songlin and Kwak, Jae Ho and Kim, Whoi Yul},
doi = {10.1007/978-3-642-15399-0_36},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Piao, Kwak, Kim - 2010 - Research on eclipse based media art authoring tool for the media artist(2).pdf:pdf},
isbn = {3642153984},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Authoring,Eclipse,GMF,Media Art,RCP},
pages = {342--349},
title = {{Research on eclipse based media art authoring tool for the media artist}},
volume = {6243 LNCS},
year = {2010}
}
@article{Guo2009b,
author = {Guo, Ying Chun},
doi = {10.1109/ICMLC.2009.5212569},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo - 2009 - An integrated PSO for parameter determination and feature selection of SVR and its application in stlf(2).pdf:pdf},
isbn = {9781424437030},
issn = {15423980},
journal = {Proceedings of the 2009 International Conference on Machine Learning and Cybernetics},
keywords = {Feature selection,Parameter determination,Particle swarm optimization (PSO),Short-term load forecasting (STLF),Support vector regression (SVR)},
number = {July},
pages = {359--364},
title = {{An integrated PSO for parameter determination and feature selection of SVR and its application in stlf}},
volume = {1},
year = {2009}
}
@inproceedings{Lange2016a,
author = {Lange, Stefan and Ulbrich, Fritz and Goehring, Daniel},
booktitle = {2016 IEEE Intelligent Vehicles Symposium (IV)},
doi = {10.1109/IVS.2016.7535503},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lange, Ulbrich, Goehring - 2016 - Online vehicle detection using deep neural networks and lidar based preselected image patches(2).pdf:pdf},
isbn = {978-1-5090-1821-5},
month = {jun},
pages = {954--959},
publisher = {IEEE},
title = {{Online vehicle detection using deep neural networks and lidar based preselected image patches}},
url = {http://ieeexplore.ieee.org/document/7535503/},
year = {2016}
}
@article{Enzweiler2009,
author = {Enzweiler, M and Gavrila, D},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Enzweiler, Gavrila - 2009 - Monocular pedestrian detection Survey and experiments(2).pdf:pdf},
journal = {Pattern Analysis and Machine Intelligence},
number = {12},
pages = {2179--2195},
title = {{Monocular pedestrian detection: Survey and experiments}},
volume = {31},
year = {2009}
}
@article{Uijlings2013,
author = {Uijlings, J. R. R. and van de Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
doi = {10.1007/s11263-013-0620-5},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Uijlings et al. - 2013 - Selective Search for Object Recognition(2).pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {sep},
number = {2},
pages = {154--171},
publisher = {Springer US},
title = {{Selective Search for Object Recognition}},
url = {http://link.springer.com/10.1007/s11263-013-0620-5},
volume = {104},
year = {2013}
}
@article{Riaz2013,
author = {Riaz, Irfan and Piao, Jingchun and Shin, Hyunchul},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Riaz, Piao, Shin - 2013 - Human detection by using centrist features for thermal images(2).pdf:pdf},
isbn = {9789728939892},
journal = {International Conference Computer Graphics, Visualization, Computer Vision and Image Processing},
keywords = {centrist,hog,human detection,thermal image,vision},
number = {2},
pages = {1--11},
title = {{Human detection by using centrist features for thermal images}},
volume = {8},
year = {2013}
}
@article{Shashua2004,
abstract = { We describe the functional and architectural breakdown of a monocular pedestrian detection system. We describe in detail our approach for single-frame classification based on a novel scheme of breaking down the class variability by repeatedly training a set of relatively simple classifiers on clusters of the training set. Single-frame classification performance results and system level performance figures for daytime conditions are presented with a discussion about the remaining gap to meet a daytime normal weather condition production system.},
author = {Shashua, a. and Gdalyahu, Y. and Hayun, G.},
doi = {10.1109/IVS.2004.1336346},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shashua, Gdalyahu, Hayun - 2004 - Pedestrian detection for driving assistance systems single-frame classification and system level pe(2).pdf:pdf},
isbn = {0-7803-8310-9},
journal = {IEEE Intelligent Vehicles Symposium, 2004},
pages = {1--6},
title = {{Pedestrian detection for driving assistance systems: single-frame classification and system level performance}},
year = {2004}
}
@article{Kim2010,
abstract = {We investigate the problem of estimating the ego-motion of a multicamera rig from two positions of the rig. We describe and compare two new algorithms for finding the 6 degrees of freedom (3 for rotation and 3 for translation) of the motion. One algorithm gives a linear solution and the other is a geometric algorithm that minimizes the maximum measurement error-the optimal L{\{}infinity{\}} solution. They are described in the context of the General Camera Model (GCM), and we pay particular attention to multicamera systems in which the cameras have nonoverlapping or minimally overlapping field of view. Many nonlinear algorithms have been developed to solve the multicamera motion estimation problem. However, no linear solution or guaranteed optimal geometric solution has previously been proposed. We made two contributions: 1) a fast linear algebraic method using the GCM and 2) a guaranteed globally optimal algorithm based on the L{\{}infinity{\}} geometric error using the branch-and-bound technique. In deriving the linear method using the GCM, we give a detailed analysis of degeneracy of camera configurations. In finding the globally optimal solution, we apply a rotation space search technique recently proposed by Hartley and Kahl. Our experiments conducted on both synthetic and real data have shown excellent results.},
author = {Kim, Jae Hak and Li, Hongdong and Hartley, Richard},
doi = {10.1109/TPAMI.2009.82},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Li, Hartley - 2010 - Motion estimation for nonoverlapping multicamera rigs Linear algebraic and L geometric solutions(2).pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Branch and bound,Epipolar equation,Generalized camera,Linear programming.,Motion estimation,Multicamera rigs},
number = {6},
pages = {1044--1059},
pmid = {20431130},
title = {{Motion estimation for nonoverlapping multicamera rigs: Linear algebraic and L??? geometric solutions}},
volume = {32},
year = {2010}
}
@article{Wu2013,
author = {Wu, Yi and Lim, Jongwoo and Yang, Ming-Hsuan},
doi = {10.1109/CVPR.2013.312},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Lim, Yang - 2013 - 1- Object Tracking -Online Object Tracking A Benchmark(2).pdf:pdf},
isbn = {978-0-7695-4989-7},
journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
pages = {2411--2418},
title = {{1- Object Tracking -Online Object Tracking: A Benchmark}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619156},
year = {2013}
}
@inproceedings{Fan2016,
author = {Fan, Quanfu and Brown, Lisa and Smith, John},
booktitle = {2016 IEEE Intelligent Vehicles Symposium (IV)},
doi = {10.1109/IVS.2016.7535375},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fan, Brown, Smith - 2016 - A closer look at Faster R-CNN for vehicle detection(2).pdf:pdf},
isbn = {978-1-5090-1821-5},
keywords = {vehicle},
mendeley-tags = {vehicle},
month = {jun},
pages = {124--129},
publisher = {IEEE},
title = {{A closer look at Faster R-CNN for vehicle detection}},
url = {http://ieeexplore.ieee.org/document/7535375/},
year = {2016}
}
@article{Lampert2009,
abstract = {Most successful object recognition systems rely on binary classification, deciding only if an object is present or not, but not providing information on the actual object location. To estimate the object's location, one can take a sliding window approach, but this strongly increases the computational cost because the classifier or similarity function has to be evaluated over a large set of candidate subwindows. In this paper, we propose a simple yet powerful branch and bound scheme that allows efficient maximization of a large class of quality functions over all possible subimages. It converges to a globally optimal solution typically in linear or even sublinear time, in contrast to the quadratic scaling of exhaustive or sliding window search. We show how our method is applicable to different object detection and image retrieval scenarios. The achieved speedup allows the use of classifiers for localization that formerly were considered too slow for this task, such as SVMs with a spatial pyramid kernel or nearest-neighbor classifiers based on the $\backslash$chi;2 distance. We demonstrate state-of-the-art localization performance of the resulting systems on the UIUC Cars data set, the PASCAL VOC 2006 data set, and in the PASCAL VOC 2007 competition.},
author = {Lampert, Christoph H. and Blaschko, Matthew B. and Hofmann, Thomas},
doi = {10.1109/TPAMI.2009.144},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lampert, Blaschko, Hofmann - 2009 - Efficient subwindow search A branch and bound framework for object localization(2).pdf:pdf},
isbn = {978-1-4244-2242-5},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Branch and bound,Global optimization,Object localization,Sliding window},
number = {12},
pages = {2129--2142},
pmid = {19834136},
title = {{Efficient subwindow search: A branch and bound framework for object localization}},
volume = {31},
year = {2009}
}
@article{Babenko2011,
author = {Babenko, B. and {Ming-Hsuan Yang}, M. H. and Belongie, S.},
doi = {10.1109/TPAMI.2010.226},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Babenko, Ming-Hsuan Yang, Belongie - 2011 - Robust Object Tracking with Online Multiple Instance Learning(2).pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Adaptation model,Agriculture,Boosting,Target tracking,Training,Visual Tracking,discriminative classifier,image classification,learning (artificial intelligence),multiple instance learning,object tracking,online MIL algorithm,online boosting.,online multiple instance learning,robust object tracking,tracking by detection techniques,video clips,video signal processing},
month = {aug},
number = {8},
pages = {1619--1632},
publisher = {IEEE},
title = {{Robust Object Tracking with Online Multiple Instance Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5674053},
volume = {33},
year = {2011}
}
@article{Chen2014,
abstract = {Appearance model is one of the most important components for online visual tracking. An effective appearance model needs to strike the right balance between being adaptive, to account for appearance change, and being conservative, to re-track the object after it loses track- ing (e.g., due to occlusion). Most conventional appearance models fo- cus on one aspect out of the two, and hence are not able to achieve the right balance. In this paper, we approach this problem by a max- margin learning framework collaborating a descriptive component and a discriminative component. Particularly, the two components are for dif- ferent purposes and with different lifespans. One forms a robust object model, and the other tries to distinguish the object from the current background. Taking advantages of their complementary roles, the com- ponents improve each other and collaboratively contribute to a shared score function. Besides, for realtime implementation, we also propose a series of optimization and sample-management strategies. Experiments over 30 challenging videos demonstrate the effectiveness and robustness of the proposed tracker. Our method generally outperforms the existing state-of-the-art methods.},
author = {Chen, Dapeng and Yuan, Zejian and Hua, Gang and Wu, Yang and Zheng, Nanning},
doi = {10.1007/978-3-319-10590-1_23},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2014 - Description-discrimination collaborative tracking(2).pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Descriptive model,SVDD,collaborative tracking,discriminative model,long-term and short-term memory,structural prediction},
number = {PART 1},
pages = {345--360},
title = {{Description-discrimination collaborative tracking}},
volume = {8689 LNCS},
year = {2014}
}
@article{Hua2014,
abstract = {Object tracking is a reoccurring problem in computer vision. Tracking-by-detection approaches, in particular Struck [20], have shown to be competitive in recent evaluations. However, such approaches fail in the presence of long-term occlusions as well as severe viewpoint changes of the object. In this paper we propose a principled way to combine occlusion and motion reasoning with a tracking-by-detection approach. Occlusion and motion reasoning is based on state-of-the-art long-term trajectories which are labeled as object or background tracks with an energy-based formulation. The overlap between labeled tracks and detected regions allows to identify occlusions. The motion changes of the object between consecutive frames can be estimated robustly from the geometric relation between object trajectories. If this geometric change is significant, an additional detector is trained. Experimental results show that our tracker obtains state-of-the-art results and handles occlusion and viewpoints changes better than competing tracking methods.},
author = {Hua, Yang and Alahari, Karteek and Schmid, Cordelia},
doi = {10.1007/978-3-319-10599-4_12},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hua, Alahari, Schmid - 2014 - Occlusion and motion reasoning for long-term tracking(2).pdf:pdf},
isbn = {9783319105987},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 6},
pages = {172--187},
title = {{Occlusion and motion reasoning for long-term tracking}},
volume = {8694 LNCS},
year = {2014}
}
@misc{Krizhevsky2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks(2).pdf:pdf},
pages = {1097--1105},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks},
year = {2012}
}
@article{Porikli2006,
abstract = { We propose a simple and elegant algorithm to track nonrigid objects using a covariance based object description and a Lie algebra based update mechanism. We represent an object window as the covariance matrix of features, therefore we manage to capture the spatial and statistical properties as well as their correlation within the same representation. The covariance matrix enables efficient fusion of different types of features and modalities, and its dimensionality is small. We incorporated a model update algorithm using the Lie group structure of the positive definite matrices. The update mechanism effectively adapts to the undergoing object deformations and appearance changes. The covariance tracking method does not make any assumption on the measurement noise and the motion of the tracked objects, and provides the global optimal solution. We show that it is capable of accurately detecting the nonrigid, moving objects in non-stationary camera sequences while achieving a promising detection rate of 97.4 percent.},
author = {Porikli, Fatih and Tuzel, Oncel and Meer, Peter},
doi = {10.1109/CVPR.2006.94},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Porikli, Tuzel, Meer - 2006 - Covariance tracking using model update based on Lie algebra(2).pdf:pdf},
isbn = {0769525970},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {728--735},
title = {{Covariance tracking using model update based on Lie algebra}},
volume = {1},
year = {2006}
}
@article{Cheon2012,
author = {Cheon, Minkyu and Lee, Wonju and Yoon, Changyong and Park, Mignon},
doi = {10.1109/TITS.2012.2188630},
issn = {1524-9050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
month = {sep},
number = {3},
pages = {1243--1252},
title = {{Vision-Based Vehicle Detection System With Consideration of the Detecting Location}},
url = {http://ieeexplore.ieee.org/document/6175131/},
volume = {13},
year = {2012}
}
@inproceedings{Labayrade2002,
abstract = {Presents a road obstacle detection method able to cope with uphill and downhill gradients and dynamic pitching of the vehicle. Our approach is based on the construction and investigation of the "v-disparity" image which provides a good representation of the geometric content of the road scene. The advantage of this image is that it provides semi-global matching and is able to perform robust obstacle detection even in the case of partial occlusion or errors committed during the matching process. Furthermore, this detection is performed without any explicit extraction of coherent structures. This paper explains the construction of the "v-disparity" image, its main properties, and the obstacle detection method. The longitudinal profile of the road is estimated and the objects located above the road surface are then extracted as potential obstacles; subsequently, the accurate detection of road obstacles, in particular the position of tyre-road contact points is computed in a precise manner. The whole process is performed at frame rate with a current-day PC. Our experimental findings and comparisons with the results obtained using a flat geometry hypothesis show the benefits of our approach.},
author = {Labayrade, R. and Aubert, D. and Tarel, J.-P.},
booktitle = {Intelligent Vehicle Symposium, 2002. IEEE},
doi = {10.1109/IVS.2002.1188024},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Labayrade, Aubert, Tarel - 2002 - Real time obstacle detection in stereovision on non flat road geometry through v-disparity represen(3).pdf:pdf},
isbn = {0-7803-7346-4},
keywords = {imaging and vision enhancement,niques which are sometimes,non flat road geometry,not reliable,poor quality of,real time processing,road obstacle detection,semi-global matching,stereoscopic,vision},
pages = {646--651},
pmid = {1188024},
publisher = {IEEE},
title = {{Real time obstacle detection in stereovision on non flat road geometry through "v-disparity" representation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1188024},
volume = {2},
year = {2002}
}
@article{Zhao2003,
author = {Zhao, Tao},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao - 2003 - Model-based Segmentation and Tracking of Multiple Humans in Complexi Situations(2).pdf:pdf},
journal = {Thesis},
title = {{Model-based Segmentation and Tracking of Multiple Humans in Complexi Situations}},
year = {2003}
}
@misc{darpa2007,
author = {Darpa},
title = {{DARPA Grand Challenge (2007)}},
url = {https://en.wikipedia.org/wiki/DARPA{\_}Grand{\_}Challenge{\_}(2007)},
year = {2007}
}
@article{Collins2014,
author = {Collins, Robert T. and Carr, Peter},
doi = {10.1007/978-3-319-10605-2_20},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Collins, Carr - 2014 - Hybrid stochasticdeterministic optimization for tracking sports players and pedestrians(2).pdf:pdf},
isbn = {9783319106045},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 2},
pages = {298--313},
title = {{Hybrid stochastic/deterministic optimization for tracking sports players and pedestrians}},
volume = {8690 LNCS},
year = {2014}
}
@article{Huang2008,
abstract = {We present a detection-based three-level hierarchical association ap- proach to robustly track multiple objects in crowded environments from a single camera. At the low level, reliable tracklets (i.e. short tracks for further analysis) are generated by linking detection responses based on conservative affinity con- straints. At the middle level, these tracklets are further associated to form longer tracklets based on more complex affinity measures. The association is formulated as aMAP problem and solved by the Hungarian algorithm. At the high level, en- tries, exits and scene occluders are estimated using the already computed track- lets, which are used to refine the final trajectories. This approach is applied to the pedestrian class and evaluated on two challenging datasets. The experimental results show a great improvement in performance compared to previous methods.},
author = {Huang, Chang and Wu, Bo and Nevatia, Ramakant},
doi = {10.1007/978-3-540-88688-4-58},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang, Wu, Nevatia - 2008 - Robust object tracking by hierarchical association of detection responses(2).pdf:pdf},
isbn = {3540886850},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 2},
pages = {788--801},
title = {{Robust object tracking by hierarchical association of detection responses}},
volume = {5303 LNCS},
year = {2008}
}
@article{Benenson2012,
abstract = {Having estimated camera position and assuming plane ground, we can prepare stixel-world based on stereo camera. It is fast method used for pedestrian detection Stupid morons writes about themselves "authors of [previous article] kindly provided code..."},
author = {Benenson, Rodrigo and Mathias, Markus and Timofte, Radu and {Van Gool}, Luc},
doi = {10.1007/978-3-642-33885-4_2},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Benenson et al. - 2012 - Fast stixel computation for fast pedestrian detection(2).pdf:pdf},
isbn = {9783642338847},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 3},
pages = {11--20},
title = {{Fast stixel computation for fast pedestrian detection}},
volume = {7585 LNCS},
year = {2012}
}
@article{Piao2010a,
author = {Piao, Songlin and Kim, Whoi-yul},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Piao, Kim - 2010 - Adaptive sampling based on the motion(2).pdf:pdf},
journal = {International Conference on Modeling, Simulation and Control},
keywords = {adaptive,motion,particle filter,simulation},
pages = {336--340},
title = {{Adaptive sampling based on the motion}},
year = {2010}
}
@inproceedings{Girshick2014,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.81},
isbn = {978-1-4799-5118-5},
month = {jun},
pages = {580--587},
publisher = {IEEE},
title = {{Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}},
url = {http://ieeexplore.ieee.org/document/6909475/},
year = {2014}
}
@article{Papadopoulos2014,
author = {Papadopoulos, Dim P and Clarke, Alasdair D F and Keller, Frank and Ferrari, Vittorio},
doi = {10.1007/978-3-319-10602-1_24},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Papadopoulos et al. - 2014 - LNCS 8693 - Training Object Class Detectors from Eye Tracking Data(2).pdf:pdf},
isbn = {978-3-319-10602-1; 978-3-319-10601-4},
pages = {361--376},
title = {{LNCS 8693 - Training Object Class Detectors from Eye Tracking Data}},
year = {2014}
}
@article{Badrinarayanan2015,
abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps.We compare our proposed architecture with the fully convolutional network [2] architecture and its variants. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. The design of SegNet was primarily motivated by road scene understanding applications. Hence, it is efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than competing architectures and can be trained end-to-end using stochastic gradient descent without complex training protocols.We also benchmark the performance of SegNet on Pascal VOC12 salient object segmentation and the recent SUN RGB-D indoor scene understanding challenge. These quantitative assessments show that SegNet provides competitive performance although it is significantly smaller than other architectures.We also provide a Caffe implementation of SegNet and a webdemo at http://mi.eng.cam.ac.uk/projects/segnet/.},
archivePrefix = {arXiv},
arxivId = {1505.0729},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
doi = {10.1103/PhysRevX.5.041024},
eprint = {1505.0729},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Badrinarayanan, Kendall, Cipolla - 2015 - SegNet A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation(2).pdf:pdf},
issn = {21603308},
journal = {Cvpr 2015},
keywords = {Decoder,Deep Convolutional Neural Networks,Encoder,Pooling,Semantic Pixel-Wise Segmentation,Upsampling},
pages = {5},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
url = {http://arxiv.org/abs/1505.0729{\%}5Cnhttp://mi.eng.cam.ac.uk/projects/segnet/},
year = {2015}
}
@article{Ouyang2013,
abstract = {Feature extraction, deformation handling, occlusion handling, and classification are four important components in pedestrian detection. Existing methods learn or design these components either individually or sequentially. The interaction among these components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network architecture. By establishing automatic, mutual interaction among components, the deep model achieves a 9{\%} reduction in the average miss rate compared with the current best-performing pedestrian detection approaches on the largest Caltech benchmark dataset.},
author = {Ouyang, Wanli and Wang, Xiaogang},
doi = {10.1109/ICCV.2013.257},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ouyang, Wang - 2013 - Joint deep learning for pedestrian detection(2).pdf:pdf},
isbn = {9781479928392},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
keywords = {Pedestrian Detection,convolutional neural network,deep learning,deep neural network,deformation,feature learning,object detection,occlusion},
pages = {2056--2063},
title = {{Joint deep learning for pedestrian detection}},
year = {2013}
}
@article{Tao2015,
abstract = {Are self-driving cars in our near future? In what ways do Google's self-driving car project disrupt the auto-industry? How are the auto manufacturers addressing this challenge? What suppliers will benefit from this technological revolution? Will the standards and regulations industries be ready? This paper aims to answer some of these questions and describe an overall state of the market for self-driving vehicles.},
author = {Tao, Jiang and Srdjan, Petrovic and Uma, Ayyer and Anand, Tolani and Sajid, Husain},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tao et al. - 2015 - Self-Driving Cars Disruptive or Incremental(2).pdf:pdf},
journal = {Applied Innovation Review},
number = {1},
title = {{Self-Driving Cars: Disruptive or Incremental?}},
year = {2015}
}
@article{Sivaraman2010,
author = {Sivaraman, Sayanan and Trivedi, Mohan Manubhai},
doi = {10.1109/TITS.2010.2040177},
issn = {1524-9050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
month = {jun},
number = {2},
pages = {267--276},
title = {{A General Active-Learning Framework for On-Road Vehicle Recognition and Tracking}},
url = {http://ieeexplore.ieee.org/document/5411825/},
volume = {11},
year = {2010}
}
@incollection{Zitnick2014,
author = {Zitnick, C. Lawrence and Doll{\'{a}}r, Piotr},
doi = {10.1007/978-3-319-10602-1_26},
pages = {391--405},
publisher = {Springer, Cham},
title = {{Edge Boxes: Locating Object Proposals from Edges}},
url = {http://link.springer.com/10.1007/978-3-319-10602-1{\_}26},
year = {2014}
}
@book{stein1967,
author = {Stein, Ralph},
pages = {319},
publisher = {Cambridge Rare Books},
title = {{The Automobile Book}},
year = {1967}
}
@article{Grabner,
author = {Grabner, Helmut and Matas, Jiri and Gool, Luc Van and Cattin, Philippe},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grabner et al. - Unknown - Tracking the Invisible Learning Where the Object Might be m Carl – Track me(2).pdf:pdf},
isbn = {9781424469857},
pages = {1--8},
title = {{Tracking the Invisible : Learning Where the Object Might be m Carl – Track me}},
volume = {3}
}
@inproceedings{Franke2013,
author = {Franke, Uwe and Pfeiffer, David and Rabe, Clemens and Knoeppel, Carsten and Enzweiler, Markus and Stein, Fridtjof and Herrtwich, Ralf G.},
booktitle = {2013 IEEE International Conference on Computer Vision Workshops},
doi = {10.1109/ICCVW.2013.36},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Franke et al. - 2013 - Making Bertha See(2).pdf:pdf},
isbn = {978-1-4799-3022-7},
keywords = {2014 Mercedes-Benz S-Class vehicle,Autonomous Driving,Bertha Benz memorial route,Cameras,Germany,Global Positioning System,Intelligent Vehicles,Machine vision,Mannheim,Pforzheim,Roads,Sensors,Stereo Vision,Stereo vision,Vehicles,autonomous driving,close-to-production sensors,complex traffic situations,free-space analysis,image sensors,lane recognition,low speed highway scenarios,market introduction,mobile robots,narrow European villages,next-generation stereo vision,object recognition,object tracking,rural traffic situations,self-localization,sensing component,stereo camera system,stereo image processing,traffic engineering computing,traffic light recognition,urban traffic situations,vision algorithms},
month = {dec},
pages = {214--221},
publisher = {IEEE},
title = {{Making Bertha See}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6755901},
year = {2013}
}
@article{Zhang2014,
abstract = {In this paper, we present a simple yet fast and robust algorithm which exploits the dense spatio-temporal context for visual tracking. Our approach for- mulates the spatio-temporal relationships between the object of interest and its locally dense contexts in a Bayesian framework, which models the statistical cor- relation between the simple low-level features (i.e., image intensity and position) from the target and its surrounding regions. The tracking problem is then posed by computing a confidence map which takes into account the prior information of the target location and thereby alleviates target location ambiguity effectively. We further propose a novel explicit scale adaptation scheme, which is able to deal with target scale variations efficiently and effectively. The Fast Fourier Trans- form (FFT) is adopted for fast learning and detection in this work, which only needs 4 FFT operations. Implemented in MATLAB without code optimization, the proposed tracker runs at 350 frames per second on an i7 machine. Extensive experimental results showthat the proposed algorithm performs favorably against state-of-the-art methods in terms of efficiency, accuracy and robustness.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.1939v1},
author = {Zhang, Kaihua and Zhang, Lei and Liu, Qingshan and Zhang, David and Yang, Ming Hsuan},
doi = {10.1007/978-3-319-10602-1_9},
eprint = {arXiv:1311.1939v1},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2014 - Fast visual tracking via dense spatio-temporal context learning(2).pdf:pdf},
isbn = {978-3-319-10601-4},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 5},
pages = {127--141},
title = {{Fast visual tracking via dense spatio-temporal context learning}},
volume = {8693 LNCS},
year = {2014}
}
@article{Songb,
author = {Song, Hyun Oh and Zickler, Stefan and Altho, Tim and Girshick, Ross and Fritz, Mario and Geyer, Christopher and Felzenszwalb, Pedro and Darrell, Trevor},
doi = {10.1007/978-3-642-33709-3_57},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Song et al. - Unknown - Sparselet Models for E ffi cient Multiclass Object Detection(2).pdf:pdf},
isbn = {978-3-642-33708-6},
keywords = {deformable part models,object detection,sparse coding},
title = {{Sparselet Models for E ffi cient Multiclass Object Detection}}
}
@inproceedings{Wijnhoven2011,
author = {Wijnhoven, Rob G.J. and de With, Peter H.N.},
booktitle = {2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)},
doi = {10.1109/ICCVW.2011.6130504},
isbn = {978-1-4673-0063-6},
month = {nov},
pages = {2077--2083},
publisher = {IEEE},
title = {{Unsupervised sub-categorization for object detection: Finding cars from a driving vehicle}},
url = {http://ieeexplore.ieee.org/document/6130504/},
year = {2011}
}
@article{Sivaraman2013,
author = {Sivaraman, Sayanan and Trivedi, Mohan Manubhai},
doi = {10.1109/TITS.2013.2266661},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sivaraman, Trivedi - 2013 - Looking at Vehicles on the Road A Survey of Vision-Based Vehicle Detection, Tracking, and Behavior Analys(2).pdf:pdf},
issn = {1524-9050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {vehicle},
mendeley-tags = {vehicle},
month = {dec},
number = {4},
pages = {1773--1795},
title = {{Looking at Vehicles on the Road: A Survey of Vision-Based Vehicle Detection, Tracking, and Behavior Analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6563169},
volume = {14},
year = {2013}
}
@article{Benenson2012a,
abstract = {We present a new pedestrian detector that improves both in speed and quality over state-of-the-art. By efficiently handling different scales and transferring computation from test time to training time, detection speed is improved. When processing monocular images, our system provides high quality detections at 50 fps. We also propose a new method for exploiting geomet- ric context extracted from stereo images. On a single CPU+GPU desktop machine, we reach 135 fps, when pro- cessing street scenes, from rectified input to detections out- put.},
author = {Benenson, Rodrigo and Mathias, Markus and Timofte, Radu and {Van Gool}, Luc},
doi = {10.1109/CVPR.2012.6248017},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Benenson et al. - 2012 - Pedestrian detection at 100 frames per second(2).pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2903--2910},
title = {{Pedestrian detection at 100 frames per second}},
year = {2012}
}
@article{Wu2011b,
abstract = {Common visual codebook generation methods used in a bag of visual words model, for example, k-means or Gaussian Mixture Model, use the Euclidean distance to cluster features into visual code words. However, most popular visual descriptors are histograms of image measurements. It has been shown that with histogram features, the Histogram Intersection Kernel (HIK) is more effective than the Euclidean distance in supervised learning tasks. In this paper, we demonstrate that HIK can be used in an unsupervised manner to significantly improve the generation of visual codebooks. We propose a histogram kernel k-means algorithm which is easy to implement and runs almost as fast as the standard k-means. The HIK codebooks have consistently higher recognition accuracy over k-means codebooks by 2-4{\%} in several benchmark object and scene recognition data sets. The algorithm is also generalized to arbitrary additive kernels. Its speed is thousands of times faster than a naive implementation of the kernel k-means algorithm. In addition, we propose a one-class SVM formulation to create more effective visual code words. Finally, we show that the standard k-median clustering method can be used for visual codebook generation and can act as a compromise between the HIK / additive kernel and the k-means approaches.},
author = {Wu, Jianxin and Tan, Wc and Rehg, Jm},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Tan, Rehg - 2011 - Efficient and effective visual codebook generation using additive kernels(2).pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {additive kernel,histogram intersection kernel,visual codebook},
pages = {3097--3118},
title = {{Efficient and effective visual codebook generation using additive kernels}},
url = {http://dl.acm.org/citation.cfm?id=2078205},
volume = {12},
year = {2011}
}
@article{Kuo2011,
abstract = {We address the problem of multi-person tracking in a complex scene from a single camera. Although tracklet-association methods have shown impressive results in several challenging datasets, discriminability of the appearance model remains a limitation. Inspired by the work of person identity recognition, we obtain discriminative appearance-based affinity models by a novel framework to incorporate the merits of person identity recognition, which help multi-person tracking performance. During off-line learning, a small set of local image descriptors is selected to be used in on-line learned appearances-based affinity models effectively and efficiently. Given short but reliable track-lets generated by frame-to-frame association of detection responses, we identify them as query tracklets and gallery tracklets. For each gallery tracklet, a target-specific appearance model is learned from the on-line training samples collected by spatio-temporal constraints. Both gallery tracklets and query tracklets are fed into hierarchical association framework to obtain final tracking results. We evaluate our proposed system on several public datasets and show significant improvements in terms of tracking evaluation metrics.},
author = {Kuo, Cheng Hao and Nevatia, Ram},
doi = {10.1109/CVPR.2011.5995384},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuo, Nevatia - 2011 - How does person identity recognition help multi-person tracking(2).pdf:pdf},
isbn = {9781457703942},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1217--1224},
title = {{How does person identity recognition help multi-person tracking?}},
year = {2011}
}
@article{Alexe2012,
abstract = {We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any class. We explicitly train it to distinguish objects with a well-defined boundary in space, such as cows and telephones, from amorphous background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary. These include an innovative cue to measure the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we show this new cue to outperform a state-of-the-art saliency measure, and the combined objectness measure to perform better than any cue alone. We also compare to interest point operators, a HOG detector, and three recent works aiming at automatic object segmentation. Finally, we present two applications of objectness. In the first, we sample a small numberof windows according to their objectness probability and give an algorithm to employ them as location priors for modern class-specific object detectors. As we show experimentally, this greatly reduces the number of windows evaluated by the expensive class-specific model. In the second application, we use objectness as a complementary score in addition to the class-specific model, which leads to fewer false positives. As shown in several recent papers, objectness can act as a valuable focus of attention mechanism in many other applications operating on image windows, including weakly supervised learning of object categories, unsupervised pixelwise segmentation, and object tracking in video. Computing objectness is very efficient and takes only about 4 sec. per image. View full abstract},
author = {Alexe, Bogdan and Deselaers, Thomas and Ferrari, Vittorio},
doi = {10.1109/TPAMI.2012.28},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alexe, Deselaers, Ferrari - 2012 - Measuring the objectness of image windows(2).pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Objectness measure,object detection,object recognition},
number = {11},
pages = {2189--2202},
pmid = {22248633},
title = {{Measuring the objectness of image windows}},
volume = {34},
year = {2012}
}
@article{Hare2011,
abstract = {Adaptive tracking-by-detection methods are widely used in computer vision for tracking arbitrary objects. Current approaches treat the tracking problem as a classification task and use online learning techniques to update the object model. However, for these updates to happen one needs to convert the estimated object position into a set of labelled training examples, and it is not clear how best to perform this intermediate step. Furthermore, the objective for the classifier (label prediction) is not explicitly coupled to the objective for the tracker (accurate estimation of object position). In this paper, we present a framework for adaptive visual object tracking based on structured output prediction. By explicitly allowing the output space to express the needs of the tracker, we are able to avoid the need for an intermediate classification step. Our method uses a kernelized structured output support vector machine (SVM), which is learned online to provide adaptive tracking. To allow for real-time application, we introduce a budgeting mechanism which prevents the unbounded growth in the number of support vectors which would otherwise occur during tracking. Experimentally, we show that our algorithm is able to outperform state-of-the-art trackers on various benchmark videos. Additionally, we show that we can easily incorporate additional features and kernels into our framework, which results in increased performance.},
author = {Hare, Sam and Saffari, Amir and Torr, Philip H S},
doi = {10.1109/ICCV.2011.6126251},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hare, Saffari, Torr - 2011 - Struck Structured output tracking with kernels(2).pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {263--270},
title = {{Struck: Structured output tracking with kernels}},
year = {2011}
}
@article{Wu2011,
abstract = {A real-time and accurate human detector, C{\textless}inf{\textgreater}4{\textless}/inf{\textgreater}, is proposed in this paper. C{\textless}inf{\textgreater}4{\textless}/inf{\textgreater} achieves 20 fps speed and state-of-the-art detection accuracy, using only one processing thread without resorting to special hardwares like GPU. Real-time accurate human detection is made possible by two contributions. First, we show that contour is exactly what we should capture and signs of comparisons among neighboring pixels are the key information to capture contours. Second, we show that the CENTRIST visual descriptor is particularly suitable for human detection, because it encodes the sign information and can implicitly represent the global contour. When CENTRIST and linear classifier are used, we propose a computational method that does not need to explicitly generate feature vectors. It involves no image pre-processing or feature vector normalization, and only requires O(1) steps to test an image patch. C{\textless}inf{\textgreater}4{\textless}/inf{\textgreater} is also friendly to further hardware acceleration. In a robot with embedded 1.2GHz CPU, we also achieved accurate and 20 fps high speed human detection.},
author = {Wu, Jianxin and Geyer, Christopher and Rehg, James M.},
doi = {10.1109/ICRA.2011.5980437},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Geyer, Rehg - 2011 - Real-time human detection using contour cues(2).pdf:pdf},
isbn = {978-1-61284-380-3},
issn = {1050-4729},
journal = {2011 IEEE International Conference on Robotics and Automation},
pages = {860--867},
title = {{Real-time human detection using contour cues}},
year = {2011}
}
@article{Cho2012,
author = {Cho, Hyunggi and Rybski, Paul E and Bar-hillel, Aharon},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho, Rybski, Bar-hillel - 2012 - Real-time Pedestrian Detection with Deformable Part Models Real-time Pedestrian Detection with Defor(2).pdf:pdf},
number = {Iv},
pages = {1035--1042},
title = {{Real-time Pedestrian Detection with Deformable Part Models Real-time Pedestrian Detection with Deformable Part Models}},
year = {2012}
}
@article{Kalal2010,
abstract = {This paper proposes a novel method for tracking failure detection. The detection is based on the Forward-Backward error, i.e. the tracking is performed forward and backward in time and the discrepancies between these two trajectories are measured. We demonstrate that the proposed error enables reliable detection of tracking failures and selection of reliable trajectories in video sequences. We demonstrate that the approach is complementary to commonly used normalized cross-correlation (NCC). Based on the error, we propose a novel object tracker called Median Flow. State-of-the-art performance is achieved on challenging benchmark video sequences which include non-rigid objects.},
author = {Kalal, Zdenek and Mikolajczyk, Krystian and Matas, Jiri},
doi = {10.1109/ICPR.2010.675},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalal, Mikolajczyk, Matas - 2010 - Forward-backward error Automatic detection of tracking failures(2).pdf:pdf},
isbn = {9780769541099},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {2756--2759},
pmid = {5596017},
title = {{Forward-backward error: Automatic detection of tracking failures}},
year = {2010}
}
@article{Fuerstenberg2005,
abstract = {This paper presents a prototype system for the early detection of a car-to-pedestrian accident that has been tested in a passenger car. To obtain an absolute reliable pedestrian classification - and thus a reliable pedestrian protection - a region of no escape (RONE) is determined, where it is impossible for the pedestrian to escape the impending vehicle-pedestrian crash. Every pedestrian colliding with the car's frontal region would enter this RONE in a certain time before the crash. Therefore, the time to collision (up to 300 ms) and the point of first contact can be estimated with a high confidence level within this RONE. The results are evaluated based on a database of recorded real data of a laserscanner integrated in a passenger car.},
author = {Fuerstenberg, Kay Ch},
doi = {10.1109/ITSC.2005.1520032},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fuerstenberg - 2005 - Pedestrian protection using laserscanners(2).pdf:pdf},
isbn = {0780392159},
journal = {Proc. IEEE Conference on Intelligent Transportation Systems},
pages = {115--120},
title = {{Pedestrian protection using laserscanners}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1520032},
year = {2005}
}
@article{Gao2009,
abstract = {In this paper, a novel feature named adaptive contour feature (ACF) is proposed for human detection and segmentation. This feature consists of a chain of a number of granules in oriented granular space (OGS) that is learnt via the AdaBoost algorithm. Three operations are defined on the OGS to mine object contour feature and feature co-occurrences automatically. A heuristic learning algorithm is proposed to generate an ACF that at the same time define a weak classifier for human detection or segmentation. Experiments on two open datasets show that the ACF outperform several well-known existing features due to its stronger discriminative power rooted in the nature of its flexibility and adaptability to describe an object contour element.},
author = {Gao, Wei and Ai, Haizhou and Lao, Shihong},
doi = {10.1109/CVPRW.2009.5206762},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gao, Ai, Lao - 2009 - Adaptive contour features in oriented granular space for human detection and segmentation(2).pdf:pdf},
isbn = {9781424439935},
issn = {1063-6919},
journal = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
pages = {1786--1793},
title = {{Adaptive contour features in oriented granular space for human detection and segmentation}},
year = {2009}
}
@inproceedings{Bergh2013,
author = {Bergh, Michael Van Den and Roig, Gemma and Boix, Xavier and Manen, Santiago and Gool, Luc Van},
booktitle = {2013 IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.54},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergh et al. - 2013 - Online Video SEEDS for Temporal Window Objectness(2).pdf:pdf},
isbn = {978-1-4799-2840-8},
keywords = {Color,Electron tubes,Histograms,Noise,Optimization,Partitioning algorithms,SEEDS super pixels,Streaming media,image sequence,image sequences,image window objectness,object recognition,online video SEEDS,real-time video super pixel algorithm,temporal consistency,temporal window objectness algorithms,video sequence,video signal processing},
month = {dec},
pages = {377--384},
publisher = {IEEE},
title = {{Online Video SEEDS for Temporal Window Objectness}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6751156},
year = {2013}
}
@article{Ulrich2009,
abstract = {This paper provides a method for recognizing 3D objects in a single camera image and for determining their 3D poses. A model is trained solely based on the geometry information of a 3D CAD model of the object. We do not rely on texture or reflectance information of the object's surface, making this approach useful for a wide range of industrial and robot ap- plications and complementary to descriptor-based approaches. A view-based approach that does not show the drawbacks of previous methods is applied: It is robust to noise, occlusions, clutter, and contrast changes. Furthermore, the 3D pose is determined with high accuracy. The high robustness of an exhaustive search is combined with an efficient hierarchical search, a high percentage of which can be computed offline, making our method suitable even for time-critical applications. The method is especially suited for, but not limited to, the recognition of untextured objects like metal parts, which are often used in industrial environments. It allows, for example, 3D pin picking in robot applications. Tracking approaches can use it for initialization},
author = {Ulrich, Markus and Wiedemann, Christian and Steger, Carsten},
doi = {10.1109/ROBOT.2009.5152511},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ulrich, Wiedemann, Steger - 2009 - CAD-based recognition of 3D objects in monocular images(2).pdf:pdf},
isbn = {978-1-4244-2788-8},
issn = {1050-4729},
journal = {2009 IEEE International Conference on Robotics and Automation},
pages = {1191--1198},
title = {{CAD-based recognition of 3D objects in monocular images}},
year = {2009}
}
@article{Dollar2012,
abstract = {Cascades help make sliding window object detection fast, nevertheless, computational demands remain prohibitive for numerous applications. Currently, evaluation of adjacent windows proceeds independently; this is suboptimal as detector responses at nearby locations and scales are correlated. We propose to exploit these correlations by tightly coupling detector evaluation of nearby windows. We introduce two opposing mechanisms: detector excitation of promising neighbors and inhibition of inferior neighbors. By enabling neighboring detectors to communicate, crosstalk cascades achieve major gains (4-30× speedup) over cascades evaluated independently at each image location. Combined with recent advances in fast multi-scale feature computation, for which we provide an optimized implementation, our approach runs at 35-65 fps on 640×480 images while attaining state-of-the-art accuracy.},
author = {Doll{\'{a}}r, Piotr and Appel, Ron and Kienzle, Wolf},
doi = {10.1007/978-3-642-33709-3_46},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doll{\'{a}}r, Appel, Kienzle - 2012 - Crosstalk cascades for frame-rate pedestrian detection(2).pdf:pdf},
isbn = {9783642337086},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 2},
pages = {645--659},
title = {{Crosstalk cascades for frame-rate pedestrian detection}},
volume = {7573 LNCS},
year = {2012}
}
@book{Calonder2010,
author = {Calonder, Michael and Lepetit, Vincent and Strecha, Christoph and Fua, Pascal},
booktitle = {Proceedings of the 11th European conference on Computer vision: Part IV},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Calonder et al. - 2010 - BRIEF binary robust independent elementary features(2).pdf:pdf},
isbn = {3-642-15560-X, 978-3-642-15560-4},
pages = {778--792},
publisher = {Springer-Verlag},
title = {{BRIEF: binary robust independent elementary features}},
year = {2010}
}
@inproceedings{Yang2014,
author = {Yang, Kai and Liu, Chao and Zheng, Jiang Yu and Christopher, Lauren and Chen, Yaobin},
booktitle = {17th International IEEE Conference on Intelligent Transportation Systems (ITSC)},
doi = {10.1109/ITSC.2014.6957928},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2014 - Bicyclist detection in large scale naturalistic driving video(2).pdf:pdf},
isbn = {978-1-4799-6078-1},
keywords = {cyclist},
mendeley-tags = {cyclist},
month = {oct},
pages = {1638--1643},
publisher = {IEEE},
title = {{Bicyclist detection in large scale naturalistic driving video}},
url = {http://ieeexplore.ieee.org/document/6957928/},
year = {2014}
}
@article{Goehring2014,
abstract = {In the following paper, we present a framework for quickly training 2D object detectors for robotic perception. Our method can be used by robotics practitioners to quickly (under 30 seconds per object) build a large-scale real-time perception system. In particular, we show how to create new detectors on the fly using large-scale internet image databases, thus allowing a user to choose among thousands of available categories to build a detection system suitable for the particular robotic application. Furthermore, we show how to adapt these models to the current environment with just a few in-situ images. Experiments on existing 2D benchmarks evaluate the speed, accuracy, and flexibility of our system.},
author = {Goehring, Daniel and Hoffman, Judy and Rodner, Erik and Saenko, Kate and Darrell, Trevor},
doi = {10.1109/ICRA.2014.6907018},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goehring et al. - 2014 - Interactive adaptation of real-time object detectors(2).pdf:pdf},
isbn = {978-1-4799-3685-4},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1282--1289},
title = {{Interactive adaptation of real-time object detectors}},
year = {2014}
}
@inproceedings{Leutenegger2011,
author = {Leutenegger, Stefan and Chli, Margarita and Siegwart, Roland Y.},
booktitle = {2011 International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126542},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leutenegger, Chli, Siegwart - 2011 - BRISK Binary Robust invariant scalable keypoints(2).pdf:pdf},
isbn = {978-1-4577-1102-2},
keywords = {BRISK method,Boats,Brightness,Complexity theory,Detectors,Feature extraction,Kernel,Robustness,SIFT algorithm,SURF algorithm,binary robust invariant scalable keypoints,bit-string descriptor,computer vision,computer vision application,feature extraction,image matching,image transformation,keypoint description,keypoint detection,keypoint generation,keypoint matching,scale-space FAST-based detector,transforms},
month = {nov},
pages = {2548--2555},
publisher = {IEEE},
title = {{BRISK: Binary Robust invariant scalable keypoints}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126542},
year = {2011}
}
@article{Broggi2014,
abstract = {The presence of autonomous vehicles on public roads is becoming a reality. In the last 10 years, autonomous prototypes have been confined in controlled or isolated environments, but new traffic regulations for testing and direct automotive companies interests are moving autonomous vehicles tests on real roads. This paper presents a test on public urban roads and freeways that was held in Parma on July 12, 2013. This was the first test in open public urban roads with nobody behind the steering wheel: the vehicle had to cope with roundabouts, junctions, pedestrian crossings, freeway junctions, traffic lights, and regular traffic. The vehicle setup, the software architecture, and the route are here presented together with some results and possible future improvements.},
author = {Broggi, Alberto and Cerri, Pietro and Debattisti, Stefano and Laghi, Maria Chiara and Medici, Paolo and Panciroli, Matteo and Prioletti, Antonio},
doi = {10.1109/IVS.2014.6856478},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Broggi et al. - 2014 - PROUD-public road urban driverless test Architecture and results(2).pdf:pdf},
isbn = {9781479936380},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {Iv},
pages = {648--654},
title = {{PROUD-public road urban driverless test: Architecture and results}},
year = {2014}
}
@article{Zhang2012,
author = {Zhang, Jianming and {Lo Presti}, Liliana and Sclaroff, Stan},
doi = {10.1109/AVSS.2012.51},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Lo Presti, Sclaroff - 2012 - Online multi-person tracking by tracker hierarchy(2).pdf:pdf},
isbn = {9780769547978},
journal = {Proceedings - 2012 IEEE 9th International Conference on Advanced Video and Signal-Based Surveillance, AVSS 2012},
pages = {379--385},
title = {{Online multi-person tracking by tracker hierarchy}},
year = {2012}
}
@inproceedings{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
isbn = {9781467369640},
issn = {10636919},
pmid = {24920543},
title = {{Going deeper with convolutions}},
year = {2015}
}
@article{Bourdev2005,
abstract = { We describe a method for training object detectors using a generalization of the cascade architecture, which results in a detection rate and speed comparable to that of the best published detectors while allowing for easier training and a detector with fewer features. In addition, the method allows for quickly calibrating the detector for a target detection rate, false positive rate or speed. One important advantage of our method is that it enables systematic exploration of the ROC surface, which characterizes the trade-off between accuracy and speed for a given classifier.},
author = {Bourdev, Lubomir and Brandt, Jonathan},
doi = {10.1109/CVPR.2005.310},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bourdev, Brandt - 2005 - Robust object detection via soft cascade(2).pdf:pdf},
isbn = {0769523722},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {236--243},
title = {{Robust object detection via soft cascade}},
volume = {2},
year = {2005}
}
@inproceedings{Leibe2007,
author = {Leibe, Bastian and Cornelis, Nico and Cornelis, Kurt and {Van Gool}, Luc},
booktitle = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2007.383146},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leibe et al. - 2007 - Dynamic 3D Scene Analysis from a Moving Vehicle(2).pdf:pdf},
isbn = {1-4244-1179-3},
keywords = {2D object detection,3D localization,3D scene analysis,Cameras,Geometry,Image analysis,Layout,Object detection,Object recognition,Streaming media,Trajectory,Vehicle dynamics,Vehicles,automatic scene geometry estimation,motion estimation,moving vehicle,object detection,object recognition,structure-from-motion,traffic engineering computing,trajectory estimation,vehicle dynamics},
month = {jun},
pages = {1--8},
publisher = {IEEE},
title = {{Dynamic 3D Scene Analysis from a Moving Vehicle}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4270171},
year = {2007}
}
@inproceedings{Ren2013,
abstract = {Facial expression recognition remains a challenging problem especially when the face is partially corrupted or occluded. We propose using a new classification method, termed Sparse Representation based Classification (SRC), to accurately recognize expressions under these conditions. A test vector is representable as a linear combination of vectors from its own class and so its representation as a linear combination of all available training vectors is sparse. Efficient methods have been developed in the area of compressed sensing to recover this sparse representation. SRC gives state of the art performance on clean and noise corrupted images matching the recognition rate obtained using Gabor based features. When test images are occluded by square black blocks, SRC improves significantly on the performance obtained using Gabor features; SRC increases the recognition rate by 6.6{\%} when the block occlusion length is 30 and by 11.2{\%} when the block length is 40.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Ren, Xiaofeng and Ramanan, Deva},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2013.417},
eprint = {9411012},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren, Ramanan - 2013 - Histograms of sparse codes for object detection(2).pdf:pdf},
isbn = {9781424442966},
issn = {10636919},
keywords = {Feature Learning,Object Detection,Sparse Coding,Supervised Training},
month = {jun},
pages = {3246--3253},
pmid = {8637596},
primaryClass = {chao-dyn},
publisher = {IEEE},
title = {{Histograms of sparse codes for object detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619261},
year = {2013}
}
@article{Huang2007,
abstract = {In recent years, boosting has been successfully applied to many practical problems in pattern recognition and computer vision fields such as object detection and tracking. As boosting is an offline training process with beforehand collected data, once learned, it cannot make use of any newly arriving ones. However, an offline boosted detector is to be exploited online and inevitably there must be some special cases that are not covered by those beforehand collected training data. As a result, the inadaptable detector often performs badly in diverse and changeful environments which are ordinary for many real-life applications. To alleviate this problem, this paper proposes an incremental learning algorithm to effectively adjust a boosted strong classifier with domain-partitioning weak hypotheses to online samples, which adopts a novel approach to efficient estimation of training losses received from offline samples. By this means, the offline learned general-purpose detectors can be adapted to special online situations at a low extra cost, and still retains good generalization ability for common environments. The experiments show convincing results of our incremental learning approach on challenging face detection problems with partial occlusions and extreme illuminations.},
author = {Huang, Chang and Ai, Haizhou and Yamashita, Takayoshi and Lao, Shihong and Kawade, Masato},
doi = {10.1109/ICCV.2007.4408850},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2007 - Incremental learning of boosted face detector(2).pdf:pdf},
isbn = {978-1-4244-1631-8},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
title = {{Incremental learning of boosted face detector}},
year = {2007}
}
@article{Krizhevsky,
author = {Krizhevsky, Alex and Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
journal = {ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS},
title = {{Imagenet classification with deep convolutional neural networks}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.299.205},
year = {2012}
}
@inproceedings{Scaramuzza2006,
author = {Scaramuzza, D. and Martinelli, A. and Siegwart, R.},
booktitle = {Fourth IEEE International Conference on Computer Vision Systems (ICVS'06)},
doi = {10.1109/ICVS.2006.3},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scaramuzza, Martinelli, Siegwart - 2006 - A Flexible Technique for Accurate Omnidirectional Camera Calibration and Structure from Mot(2).pdf:pdf},
isbn = {0-7695-2506-7},
keywords = {Calibration,Cameras,Cognitive robotics,Computer vision,Layout,Lenses,Machine vision,Mirrors,Motion estimation,Robot vision systems},
pages = {45--45},
publisher = {IEEE},
title = {{A Flexible Technique for Accurate Omnidirectional Camera Calibration and Structure from Motion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1578733},
year = {2006}
}
@incollection{Piao14,
address = {Aachen},
annote = {{\{}ISBN-13{\}}: 978-3-8440-2753-2},
author = {Piao, Songlin and Berns, Karsten},
booktitle = {Field and Assistive Robotics - Advances in Systems and Algorithms},
editor = {Berns, Karsten and Mehdi, Syed Atif and Muhammad, Abubakr},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Piao, Berns - 2014 - Multi-Object Tracking Based on Tracking-Learning-Detection Framework(2).pdf:pdf},
pages = {74--87},
publisher = {Shaker Verlag},
title = {{Multi-Object Tracking Based on Tracking-Learning-Detection Framework}},
year = {2014}
}
@misc{Matlab,
author = {Matlab},
title = {{What is Camera Calibration?}},
url = {http://de.mathworks.com/help/vision/ug/camera-calibration.html{\#}buvr2qb-2},
year = {2016}
}
@article{Gandhi2005,
abstract = {Omnidirectional cameras that give a 360° panoramic view of the surroundings have recently been used in many applications such as robotics, navigation, and surveillance. This paper describes the application of parametric ego-motion estimation for vehicle detection to perform surround analysis using an automobile-mounted camera. For this purpose, the parametric planar motion model is integrated with the transformations to compensate distortion in omnidirectional images. The framework is used to detect objects with independent motion or height above the road. Camera calibration as well as the approximate vehicle speed obtained from a CAN bus are integrated with the motion information from spatial and temporal gradients using a Bayesian approach. The approach is tested for various configurations of an automobile-mounted omni camera as well as a rectilinear camera. Successful detection and tracking of moving vehicles and generation of a surround map are demonstrated for application to intelligent driver support.},
author = {Gandhi, Tarak and Trivedi, Mohan},
doi = {10.1007/s00138-004-0168-z},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gandhi, Trivedi - 2005 - Parametric ego-motion estimation for vehicle surround analysis using an omnidirectional camera(2).pdf:pdf},
isbn = {0001250000},
issn = {09328092},
journal = {Machine Vision and Applications},
keywords = {Collision avoidance,Driver support systems,Intelligent vehicles,Motion estimation,Panoramic vision},
number = {2},
pages = {85--95},
title = {{Parametric ego-motion estimation for vehicle surround analysis using an omnidirectional camera}},
volume = {16},
year = {2005}
}
@article{Braid2006,
author = {Braid, Deborah and Broggi, Alberto and Schmiedel, Gary},
doi = {10.1002/rob.20140},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Braid, Broggi, Schmiedel - 2006 - The TerraMax autonomous vehicle(2).pdf:pdf},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {sep},
number = {9},
pages = {693--708},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
title = {{The TerraMax autonomous vehicle}},
url = {http://doi.wiley.com/10.1002/rob.20140},
volume = {23},
year = {2006}
}
@article{Nam2014a,
abstract = {Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1406.1134},
author = {Nam, Woonhyun and Doll{\'{a}}r, Piotr and Han, Joon Hee},
eprint = {1406.1134},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nam, Doll{\'{a}}r, Han - 2014 - Local Decorrelation For Improved Detection(2).pdf:pdf},
issn = {10495258},
journal = {Nips},
pages = {1--9},
title = {{Local Decorrelation For Improved Detection}},
url = {http://arxiv.org/abs/1406.1134v1{\%}5Cnhttp://arxiv.org/abs/1406.1134},
year = {2014}
}
@inproceedings{Rabe2007,
author = {Rabe, Clemens and Franke, Uwe and Gehrig, Stefan},
booktitle = {2007 IEEE Intelligent Vehicles Symposium},
doi = {10.1109/IVS.2007.4290147},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rabe, Franke, Gehrig - 2007 - Fast detection of moving objects in complex scenarios(2).pdf:pdf},
isbn = {1-4244-1067-3},
issn = {1931-0587},
month = {jun},
pages = {398--403},
publisher = {IEEE},
title = {{Fast detection of moving objects in complex scenarios}},
url = {http://ieeexplore.ieee.org/document/4290147/},
year = {2007}
}
@techreport{Ainhauser2013,
abstract = {The presentation discusses if and to what extent ROS can serve as a platform for future autonomous driving functions.},
author = {Ainhauser, Christoph and Bulwahn, Lukas and Hildisch, Andreas and Holder, Stefan and Lazarevych, Olexiy and Mohr, Daniel and Ochs, Tilmann and Rudorfer, Michael and Scheickl, Oliver and Schumm, Tillmann and Sedlmeier, Felix},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ainhauser et al. - 2013 - The presentation discusses if and to what extent ROS can serve as a platform for future autonomous driving (2).pdf:pdf},
title = {{The presentation discusses if and to what extent ROS can serve as a platform for future autonomous driving functions. It presents ideas from joint work of several people at BMW Car IT}},
year = {2013}
}
@article{Vlist2002,
abstract = {Evolutionary trends in human body form provide important context for interpreting variation among modem populations. Average body mass in living humans is smaller than it was during most of the Pleistocene,, possibly owing to technological improvements during the past 50,000 years that no longer favored large body size. Sexual dimorphism in body size reached modem levels at least 150,000 years ago and probably earlier. Geographic variation in both body size and shape in earlier humans paralleled latitudinal clines observed today. Climatic adaptation is the most likely primary cause for these gradients, overlain in more recent populations by nutritional effects on growth. Thus, to distinguish growth disturbances, it is necessary to partition out the (presumably genetic) long-term differences in body form between populations that have resulted from climatic selection. An example is given from a study of Inupiat children, using a new index of body shape to assess relative body mass.},
author = {van der Vlist, Bram},
doi = {DOI 10.1146/annurev.anthro.31.040402.085407},
isbn = {0084-6570},
issn = {0084-6570},
journal = {Annual Review of Anthropology},
keywords = {adaptation,arctic populations,australopithecus-afarensis,climate,cross-sectional geometry,early hominid,growth,hominin,human-evolution,middle paleolithic humans,nutrition,pleistocene human tibia,postcranial robusticity,secular trends,sexual dimorphism},
month = {oct},
number = {1},
pages = {211--232},
title = {{Variation in human body size and shape}},
volume = {31},
year = {2002}
}
@article{Milan2013,
author = {Milan, Anton and Schindler, K and Roth, Stefan},
doi = {10.1109/CVPR.2013.472},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Milan, Schindler, Roth - 2013 - Detection-and Trajectory-Level Exclusion in Multiple Object Tracking(2).pdf:pdf},
isbn = {978-0-7695-4989-7},
journal = {Statistics},
number = {June},
pages = {3682--3689},
title = {{Detection-and Trajectory-Level Exclusion in Multiple Object Tracking}},
url = {http://www.gris.tu-darmstadt.de/{~}aandriye/files/cvpr2013/cvpr2013-poster.pdf},
year = {2013}
}
@misc{Google,
author = {Google},
title = {{Google Self-Driving Car Project}},
url = {https://www.google.com/selfdrivingcar},
year = {2016}
}
@article{Andriyenko2011,
abstract = {We present a principled model for occlusion reasoning in complex scenarios with frequent inter-object occlusions, and its application to multi-target tracking. To compute the putative overlap between pairs of targets, we represent each target with a Gaussian. Conveniently, this leads to an analytical form for the relative overlap - another Gaussian - which is combined with a sigmoidal term for modeling depth relations. Our global occlusion model bears several advantages: Global target visibility can be computed efficiently in closed-form, and varying degrees of partial occlusion can be naturally accounted for. Moreover, the dependence of the occlusion on the target locations - i.e. the gradient of the overlap - can also be computed in closed-form, which makes it possible to efficiently include the proposed occlusion model in a continuous energy minimization framework. Experimental results on seven datasets confirm that the proposed formulation consistently reduces missed targets and lost trajectories, especially in challenging scenarios with crowds and severe inter-object occlusions.},
author = {Andriyenko, Anton and Roth, Stefan and Schindler, Konrad},
doi = {10.1109/ICCVW.2011.6130472},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andriyenko, Roth, Schindler - 2011 - An analytical formulation of global occlusion reasoning for multi-target tracking(2).pdf:pdf},
isbn = {9781467300629},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
number = {November},
pages = {1839--1846},
title = {{An analytical formulation of global occlusion reasoning for multi-target tracking}},
year = {2011}
}
@article{Wojek2008,
author = {Wojek, Christian and Schiele, Bernt},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wojek, Schiele - 2008 - A Performance Evaluation of Single and Multi-feature People Detection(2).pdf:pdf},
pages = {82--91},
title = {{A Performance Evaluation of Single and Multi-feature People Detection}},
year = {2008}
}
@article{Tuzel2008,
abstract = {We present a new algorithm to detect pedestrian in still images utilizing covariance matrices as object descriptors. Since the descriptors do not form a vector space, well known machine learning techniques are not well suited to learn the classifiers. The space of d-dimensional nonsingular covariance matrices can be represented as a connected Riemannian manifold. The main contribution of the paper is a novel approach for classifying points lying on a connected Riemannian manifold using the geometry of the space. The algorithm is tested on INRIA and DaimlerChrysler pedestrian datasets where superior detection rates are observed over the previous approaches.},
author = {Tuzel, Oncel and Porikli, Fatih and Meer, Peter},
doi = {10.1109/TPAMI.2008.75},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tuzel, Porikli, Meer - 2008 - Pedestrian detection via classification on Riemannian manifolds(2).pdf:pdf},
isbn = {2007100703},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Boosting,Classification,Object descriptors,Pedestrian detection,Riemannian manifolds,Symmetric positive definite matrices},
number = {10},
pages = {1713--1727},
pmid = {18703826},
title = {{Pedestrian detection via classification on Riemannian manifolds}},
volume = {30},
year = {2008}
}
@article{Hu2014,
author = {Hu, Ronghang and Wang, Ruiping and Shan, Shiguang and Chen, Xilin},
doi = {10.1109/ICPR.2014.482},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - 2014 - Robust head-shoulder detection using a two-stage cascade framework(2).pdf:pdf},
isbn = {9781479952083},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {2796--2801},
title = {{Robust head-shoulder detection using a two-stage cascade framework}},
year = {2014}
}
@article{Redmon2015a,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
eprint = {1506.02640},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Detection(2).pdf:pdf},
month = {jun},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@inproceedings{Hirschmuller2005,
author = {Hirschmuller, H. and Heiko},
booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
doi = {10.1109/CVPR.2005.56},
isbn = {0-7695-2372-2},
pages = {807--814},
publisher = {IEEE},
title = {{Accurate and Efficient Stereo Processing by Semi-Global Matching and Mutual Information}},
url = {http://ieeexplore.ieee.org/document/1467526/},
volume = {2},
year = {2005}
}
@techreport{Heeguen2015,
author = {Heeguen, Chae},
institution = {HYUNDAI RESEARCH},
title = {{New trend of car evolution (Korean)}},
year = {2015}
}
@article{Lepetit2005,
abstract = {Many applications require tracking of complex 3D objects. These include visual servoing of robotic arms on specific target objects, Aug- mented Reality systems that require real-time registration of the object to be augmented, and head tracking systems that sophisticated inter- faces can use. Computer Vision offers solutions that are cheap, practical and non-invasive. This survey reviews the different techniques and approaches that have been developed by industry and research. First, important math- ematical tools are introduced: Camera representation, robust estima- tion and uncertainty estimation. Then a comprehensive study is given of the numerous approaches developed by the Augmented Reality and Robotics communities, beginning with those that are based on point or planar fiducial marks and moving on to those that avoid the need to engineer the environment by relying on natural features such as edges, texture or interest. Recent advances that avoid manual initialization and failures due to fast motion are also presented. The survery con- cludes with the different possible choices that should be made when implementing a 3D tracking system and a discussion of the future of vision-based 3D tracking. Because it encompasses many computer vision techniques from low- level vision to 3D geometry and includes a comprehensive study of the massive literature on the subject, this survey should be the handbook of the student, the researcher, or the engineer who wants to implement a 3D tracking system.},
author = {Lepetit, Vincent and Fua, Pascal},
doi = {10.1561/0600000001},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lepetit, Fua - 2005 - Monocular Model-Based 3D Tracking of Rigid Objects(2).pdf:pdf},
isbn = {1933019034},
issn = {1572-2740},
journal = {Foundations and Trends in Computer Graphics and Vision},
number = {1},
pages = {1--89},
title = {{Monocular Model-Based 3D Tracking of Rigid Objects}},
url = {http://www.nowpublishers.com/product.aspx?product=CGV{\&}doi=0600000001},
volume = {1},
year = {2005}
}
@inproceedings{Fuchs2015,
author = {Fuchs, Christian and Neuhaus, Frank and Paulus, Dietrich},
booktitle = {2015 IEEE Intelligent Vehicles Symposium (IV)},
doi = {10.1109/IVS.2015.7225688},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fuchs, Neuhaus, Paulus - 2015 - Advanced 3-D trailer pose estimation for articulated vehicles(2).pdf:pdf},
isbn = {978-1-4673-7266-4},
month = {jun},
pages = {211--216},
publisher = {IEEE},
title = {{Advanced 3-D trailer pose estimation for articulated vehicles}},
url = {http://ieeexplore.ieee.org/document/7225688/},
year = {2015}
}
@article{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick - 2015 - Fast R-CNN(2).pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {978-1-4673-8391-2},
title = {{Fast R-CNN}},
url = {http://arxiv.org/abs/1504.08083},
year = {2015}
}
@article{Manen2014,
author = {Manen, Santiago and Kwon, Junseok and Guillaumin, Matthieu and {Van Gool}, Luc},
doi = {10.1007/978-3-319-10602-1_11},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manen et al. - 2014 - Appearances can be deceiving Learning visual tracking from few trajectory annotations(2).pdf:pdf},
isbn = {9783319106021},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Event modelling,Motion learning,Visual tracking},
number = {PART 5},
pages = {157--172},
title = {{Appearances can be deceiving: Learning visual tracking from few trajectory annotations}},
volume = {8693 LNCS},
year = {2014}
}
@article{Joachims1999b,
abstract = {Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, off-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.},
author = {Joachims, Thorsten and Dortmund, Universitat and Joachimscsuni-dortmundde, Thorsten},
doi = {10.1109/ICEMI.2009.5274151},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joachims, Dortmund, Joachimscsuni-dortmundde - 1999 - Making Large-Scale SVM Learning Practical(2).pdf:pdf},
isbn = {9781424438631},
issn = {15279995},
journal = {Advances in Kernel Methods - Support Vector Learning},
pages = {41--56},
title = {{Making Large-Scale SVM Learning Practical}},
url = {http://svmlight.joachims.org/{\%}5Cnhttps://eldorado.uni-dortmund.de/handle/2003/2596},
year = {1999}
}
@article{Ren2015,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolu-tional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. The code will be released.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v1},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1016/j.nima.2015.05.028},
eprint = {arXiv:1506.01497v1},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks(2).pdf:pdf},
journal = {ArXiv 2015},
pages = {1--10},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
year = {2015}
}
@article{Everingham2010,
author = {Everingham, Mark and {Van Gool}, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
doi = {10.1007/s11263-009-0275-4},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {jun},
number = {2},
pages = {303--338},
publisher = {Springer US},
title = {{The Pascal Visual Object Classes (VOC) Challenge}},
url = {http://link.springer.com/10.1007/s11263-009-0275-4},
volume = {88},
year = {2010}
}
@inproceedings{Piao2014,
author = {Piao, Songlin and Berns, Karsten},
booktitle = {Proceedings of the 13the International Conference on Intelligent Autonomous Systems (IAS-13)},
doi = {10.1007/978-3-319-08338-4_38},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Piao, Berns - 2014 - Vision-Based Person Detection for Safe Navigation of Commercial Vehicle(2).pdf:pdf},
pages = {513--524},
publisher = {Springer International Publishing},
title = {{Vision-Based Person Detection for Safe Navigation of Commercial Vehicle}},
url = {http://link.springer.com/10.1007/978-3-319-08338-4{\_}38},
year = {2014}
}
@misc{Mallick2016,
author = {Mallick, Satya},
title = {{Histogram of Oriented Gradients-tutorial}},
url = {https://www.learnopencv.com/histogram-of-oriented-gradients/},
year = {2016}
}
@article{Prisacariu2009,
abstract = {We formulate a probabilistic framework for simultaneous 2D segmentation and 2D– 3D pose tracking, using a known 3D model (of arbitrary shape) of the segmented object. Our technique is region-based; at each frame we maximise the discrimination between statistical foreground and background models, by adjusting the pose parameters iteratively. Unlike all previous work in 3D tracking, we use posterior membership probabilities for foreground and background pixels, rather than pixel likelihoods, and during periods of stable tracking we allow adaptation of the statistical foreground and background models. We support our ideas with a real-time implementation, and use this to generate experimental results on both real and artificial video sequences, with a number of 3D models, to showcase the qualities of our tracker, and to demonstrate the benefit of using pixel-wise posteriors rather than likelihoods.},
author = {Prisacariu, V.a. and Reid, I.D.},
doi = {10.5244/C.23.47},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Prisacariu, Reid - 2009 - PWP3D Real-time segmentation and tracking of 3D objects(2).pdf:pdf},
isbn = {1-901725-39-1},
journal = {British Machine Vision Conference},
pages = {1--10},
title = {{PWP3D: Real-time segmentation and tracking of 3D objects}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:PWP3D:+Real-time+segmentation+and+tracking+of+3D+objects{\#}0},
year = {2009}
}
@article{Cai2015,
abstract = {The design of complexity-aware cascaded detectors, combining features of very different complexities, is considered. A new cascade design procedure is introduced, by formulating cascade learning as the Lagrangian optimization of a risk that accounts for both accuracy and complexity. A boosting algorithm, denoted as complexity aware cascade training (CompACT), is then derived to solve this optimization. CompACT cascades are shown to seek an optimal trade-off between accuracy and complexity by pushing features of higher complexity to the later cascade stages, where only a few difficult candidate patches remain to be classified. This enables the use of features of vastly different complexities in a single detector. In result, the feature pool can be expanded to features previously impractical for cascade design, such as the responses of a deep convolutional neural network (CNN). This is demonstrated through the design of a pedestrian detector with a pool of features whose complexities span orders of magnitude. The resulting cascade generalizes the combination of a CNN with an object proposal mechanism: rather than a pre-processing stage, CompACT cascades seamlessly integrate CNNs in their stages. This enables state of the art performance on the Caltech and KITTI datasets, at fairly fast speeds.},
archivePrefix = {arXiv},
arxivId = {1507.05348},
author = {Cai, Zhaowei and Saberian, Mohammad and Vasconcelos, Nuno},
doi = {10.1109/ICCV.2015.384},
eprint = {1507.05348},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cai, Saberian, Vasconcelos - 2015 - Learning Complexity-Aware Cascades for Deep Pedestrian Detection(2).pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {978-1-4673-8391-2},
journal = {Iccv},
pages = {3361--3369},
title = {{Learning Complexity-Aware Cascades for Deep Pedestrian Detection}},
url = {http://arxiv.org/abs/1507.05348},
year = {2015}
}
@article{Berkeley,
author = {Berkeley, U C and Zitnick, C Lawrence and Doll, Piotr},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berkeley, Zitnick, Doll - Unknown - Detecting Objects using Deformation Dictionaries(2).pdf:pdf},
title = {{Detecting Objects using Deformation Dictionaries}}
}
@inproceedings{Rublee2011,
author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
booktitle = {2011 International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126544},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rublee et al. - 2011 - ORB An efficient alternative to SIFT or SURF(2).pdf:pdf},
isbn = {978-1-4577-1102-2},
keywords = {BRIEF,Boats,ORB,SIFT,SURF,binary descriptor,computer vision,feature matching,image matching,noise resistance,object detection,object recognition,patch-tracking,smart phone,tracking,transforms},
month = {nov},
pages = {2564--2571},
publisher = {IEEE},
title = {{ORB: An efficient alternative to SIFT or SURF}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126544},
year = {2011}
}
@article{Wu2013a,
abstract = {A real-time and accurate object detection framework, C(4), is proposed in this paper. C(4) achieves 20 fps speed and the state-of-the-art detection accuracy, using only one processing thread without resorting to special hardware such as GPU. The real-time accurate object detection is made possible by two contributions. First, we conjecture (with supporting experiments) that contour is what we should capture and signs of comparisons among neighboring pixels are the key information to capture contour cues. Second, we show that the CENTRIST visual descriptor is suitable for contour based object detection, because it encodes the sign information and can implicitly represent the global contour. When CENTRIST and linear classifier are used, we propose a computational method that does not need to explicitly generate feature vectors. It involves no image preprocessing or feature vector normalization, and only requires O(1) steps to test an image patch. C(4) is also friendly to further hardware acceleration. It has been applied to detect objects such as pedestrians, faces, and cars on benchmark data sets. It has comparable detection accuracy with state-of-the-art methods, and has a clear advantage in detection speed.},
author = {Wu, Jianxin and Liu, Nini and Geyer, Christopher and Rehg, James M},
doi = {10.1109/TIP.2013.2270111},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2013 - C4 a real-time object detection framework(2).pdf:pdf},
issn = {1941-0042},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
keywords = {Algorithms,Automobiles,Databases, Factual,Face,Face: anatomy {\&} histology,Humans,Image Processing, Computer-Assisted,Image Processing, Computer-Assisted: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Software,Video Recording,Walking},
number = {10},
pages = {4096--107},
pmid = {23797259},
title = {{C4: a real-time object detection framework.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23797259},
volume = {22},
year = {2013}
}
@article{Comaniciu2002,
author = {Comaniciu, Dorinn and Meer, Peter},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Comaniciu, Meer - 2002 - Mean shift A robust approach toward feature space analysis(2).pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {5},
pages = {603--619},
title = {{Mean shift: A robust approach toward feature space analysis}},
volume = {24},
year = {2002}
}
@incollection{Andrews2003,
author = {Andrews, Stuart and Tsochantaridis, Ioannis and Hofmann, Thomas},
booktitle = {Advances in Neural Information Processing Systems 15},
doi = {10.1.1.86.8281},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrews, Tsochantaridis, Hofmann - 2003 - Support Vector Machines for Multiple-Instance Learning(2).pdf:pdf},
pages = {577--584},
publisher = {MIT Press},
title = {{Support Vector Machines for Multiple-Instance Learning}},
url = {http://papers.nips.cc/paper/2232-support-vector-machines-for-multiple-instance-learning.pdf},
year = {2003}
}
@article{Wang,
author = {Wang, Xiaoyu and Hua, Gang and Han, Tony X},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Hua, Han - Unknown - Discriminative Tracking by Metric Learning.pdf(2).pdf:pdf},
pages = {1--14},
title = {{Discriminative Tracking by Metric Learning.pdf}}
}
@article{Zeng2010,
abstract = {Robustly counting the number of people for surveillance systems has widespread applications. In this paper, we propose a robust and rapid head-shoulder detector for people counting. By combining the multilevel HOG (Histograms of Oriented Gradients) with the multilevel LBP (Local Binary Pattern) as the feature set, we can detect the head-shoulders of people robustly, even though there are partial occlusions occurred. To further improve the detection performance, Principal Components Analysis (PCA) is used to reduce the dimension of the multilevel HOG-LBP feature set. Our experiments show that the PCA based multilevel HOG-LBP descriptors are more discriminative, more robust than the state-of-the-art algorithms. For the application of the real-time people-flow estimation, we also incorporate our detector into the particle filter tracking and achieve convincing accuracy.},
author = {Zeng, Chengbin and Ma, Huadong},
doi = {10.1109/ICPR.2010.509},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeng, Ma - 2010 - Robust head-shoulder detection by PCA-based multilevel HOG-LBP detector for people counting(2).pdf:pdf},
isbn = {9780769541099},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
keywords = {Head-shoulder detection,Multilevel HOG-LBP detector,PCA,People counting},
pages = {2069--2072},
pmid = {5597274},
title = {{Robust head-shoulder detection by PCA-based multilevel HOG-LBP detector for people counting}},
year = {2010}
}
@misc{boss2007,
author = {Boss},
title = {{Boss at DARPA}},
url = {https://paulstamatiou.com/carnegie-mellon-succeeds-at-darpa-event/},
year = {2007}
}
@article{Hall2014,
author = {Hall, David and Perona, Pietro},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall, Perona - 2014 - Online, Real-Time Tracking Using a Category-to-Individual Detector(2).pdf:pdf},
isbn = {978-3-319-10590-1; 978-3-319-10589-5},
journal = {Eccv},
pages = {361--376},
title = {{Online, Real-Time Tracking Using a Category-to-Individual Detector}},
volume = {8689},
year = {2014}
}
@article{Hinterstoisser,
abstract = {We propose a framework for automatic modeling, detection, and tracking of 3D objects with a Kinect. The detection part is mainly based on the recent template-based LINEMOD approach [1] for object detection. We show how to build the templates automatically from 3D models, and how to estimate the 6 degrees-of-freedom pose accurately and in real-time. The pose estimation and the color information allow us to check the detection hypotheses and improves the correct detec-tion rate by 13{\%} with respect to the original LINEMOD. These many improvements make our framework suitable for object manipulation in Robotics applications. Moreover we propose a new dataset made of 15 registered, 1100+ frame video sequences of 15 various objects for the evaluation of future competing methods. Fig. 1. 15 different texture-less 3D objects are simultaneously detected with our ap-proach under different poses on heavy cluttered background with partial occlusion. Each detected object is augmented with its 3D model. We also show the corresponding coordinate systems.},
author = {Hinterstoisser, Stefan and Lepetit, Vincent and Ilic, Slobodan and Holzer, Stefan and Konolige, Kurt and Navab, Nassir},
doi = {10.1007/978-3-642-37331-2_42},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinterstoisser et al. - Unknown - Model Based Training , Detection and Pose Estimation of Texture-Less 3D Objects in Heavily Cluttere(2).pdf:pdf},
isbn = {978-3-642-37330-5 978-3-642-37331-2},
issn = {03029743},
pages = {1--14},
title = {{Model Based Training , Detection and Pose Estimation of Texture-Less 3D Objects in Heavily Cluttered Scenes}}
}
@inproceedings{Chen2014a,
author = {Chen, Han-Hsuan and {Chun-Cheng Lin} and {Wei-Yu Wu} and Chan, Yi-Ming and Fu, Li-Chen and Hsiao, Pei-Yung},
booktitle = {17th International IEEE Conference on Intelligent Transportation Systems (ITSC)},
doi = {10.1109/ITSC.2014.6957716},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2014 - Integrating appearance and edge features for on-road bicycle and motorcycle detection in the nighttime(2).pdf:pdf},
isbn = {978-1-4799-6078-1},
keywords = {cyclist},
mendeley-tags = {cyclist},
month = {oct},
pages = {354--359},
publisher = {IEEE},
title = {{Integrating appearance and edge features for on-road bicycle and motorcycle detection in the nighttime}},
url = {http://ieeexplore.ieee.org/document/6957716/},
year = {2014}
}
@article{He2014,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image. This requirement is "artificial" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102x faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank {\#}2 in object detection and {\#}3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1007/978-3-319-10578-9_23},
eprint = {1406.4729},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2014 - Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(3).pdf:pdf},
month = {jun},
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}},
url = {http://arxiv.org/abs/1406.4729 http://dx.doi.org/10.1007/978-3-319-10578-9{\_}23},
year = {2014}
}
@article{Bae2014a,
abstract = {... In this paper, we propose a robust  online  multi - object  tracking method in consideration of the aforementioned limitations of ... [3, 5, 11] devise on - line  learning methods, but their ... Unlike these previous works, the proposed online  learning method is designed in consideration of two ... $\backslash$n},
author = {Bae, Seung Hwan and Yoon, Kuk Jin},
doi = {10.1109/CVPR.2014.159},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bae, Yoon - 2014 - Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning(2).pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1218--1225},
pmid = {24801247},
title = {{Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning}},
year = {2014}
}
@article{Geronimo2010b,
abstract = {Advanced driver assistance systems (ADASs), and particularly pedestrian protection systems (PPSs), have become an active research area aimed at improving traffic safety. The major challenge of PPSs is the development of reliable on-board pedestrian detection systems. Due to the varying appearance of pedestrians (e.g., different clothes, changing size, aspect ratio, and dynamic shape) and the unstructured environment, it is very difficult to cope with the demanded robustness of this kind of system. Two problems arising in this research area are the lack of public benchmarks and the difficulty in reproducing many of the proposed methods, which makes it difficult to compare the approaches. As a result, surveying the literature by enumerating the proposals one--after-another is not the most useful way to provide a comparative point of view. Accordingly, we present a more convenient strategy to survey the different approaches. We divide the problem of detecting pedestrians from images into different processing steps, each with attached responsibilities. Then, the different proposed methods are analyzed and classified with respect to each processing stage, favoring a comparative viewpoint. Finally, discussion of the important topics is presented, putting special emphasis on the future needs and challenges.},
author = {Ger{\'{o}}nimo, David and L{\'{o}}pez, Antonio M. and Sappa, Angel D. and Graf, Thorsten},
doi = {10.1109/TPAMI.2009.122},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ger{\'{o}}nimo et al. - 2010 - Survey of pedestrian detection for advanced driver assistance systems(2).pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {ADAS,On-board vision,Pedestrian detection,Survey},
number = {7},
pages = {1239--1258},
pmid = {20489227},
title = {{Survey of pedestrian detection for advanced driver assistance systems}},
volume = {32},
year = {2010}
}
@article{Franke2005,
abstract = {Obstacle avoidance is one of the most important challenges for mobile robots as well as future vision based driver assistance systems. This task requires a precise extraction of depth and the robust and fast detection of moving objects. In order to reach these goals, this paper considers vision as a process in space and time. It presents a powerful fusion of depth and motion information for image sequences taken from a moving observer. 3D-position and 3D-motion for a large number of image points are estimated simultaneously by means of Kalman-Filters. There is no need of prior error-prone segmentation. Thus, one gets a rich 6D representation that allows the detection of moving obstacles even in the presence of partial occlusion of foreground or background.},
author = {Franke, Uwe and Rabe, Clemens and Gehrig, Stefan},
doi = {10.1007/11550518_27},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Franke, Rabe, Gehrig - 2005 - 6D-Vision Fusion of Stereo and Motion for Robust Environment Perception(2).pdf:pdf},
isbn = {3540287035},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
pages = {216},
title = {{6D-Vision : Fusion of Stereo and Motion for Robust Environment Perception}},
url = {http://www.springerlink.com/index/3pabb7l8965wr3l4.pdf},
volume = {3663},
year = {2005}
}
@article{Namdev2012,
abstract = {Motion segmentation is an inevitable component for mobile robotic systems such as the case with robots per- forming SLAM and collision avoidance in dynamic worlds. This paper proposes an incremental motion segmentation system that efficiently segments multiple moving objects and simultaneously build the map of the environment using visual SLAM modules. Multiple cues based on optical flow and two view geometry are integrated to achieve this segmentation. A dense optical flow algorithm is used for dense tracking of features. Motion potentials based on geometry are computed for each of these dense tracks. These geometric potentials along with the optical flow potentials are used to form a graph like structure. A graph based segmentation algorithm then clusters together nodes of similar potentials to form the eventual motion segments. Experimental results of high quality segmentation on different publicly available datasets demonstrate the effectiveness of our method. I.},
author = {Namdev, Rahul Kumar and Kundu, Abhijit and Krishna, K Madhava and Jawahar, C. V.},
doi = {10.1109/ICRA.2012.6224800},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Namdev et al. - 2012 - Motion segmentation of multiple objects from a freely moving monocular camera(2).pdf:pdf},
isbn = {978-1-4673-1405-3},
issn = {1050-4729},
journal = {Icra},
pages = {4092--4099},
title = {{Motion segmentation of multiple objects from a freely moving monocular camera}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6224800},
year = {2012}
}
@article{Zhang2015b,
author = {Zhang, Shilin and Zhang, Xunyuan},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhang - 2015 - Locate and Detect Persons in Crowded Scenes Aided by Objectiveness Measure(2).pdf:pdf},
journal = {International Journal of u- and e- Service, Science and Technology},
keywords = {bing feature,deformable part-based models,multi-,pedestrian detection},
number = {6},
pages = {249--256},
title = {{Locate and Detect Persons in Crowded Scenes Aided by Objectiveness Measure}},
volume = {8},
year = {2015}
}
@incollection{zeiler2014,
author = {Zeiler, Matthew D. and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler, Fergus - 2014 - Visualizing and Understanding Convolutional Networks(2).pdf:pdf},
pages = {818--833},
publisher = {Springer, Cham},
title = {{Visualizing and Understanding Convolutional Networks}},
url = {http://link.springer.com/10.1007/978-3-319-10590-1{\_}53},
year = {2014}
}
@inproceedings{Kennedy1995,
author = {Kennedy, J. and Eberhart, R.},
booktitle = {Proceedings of ICNN'95 - International Conference on Neural Networks},
doi = {10.1109/ICNN.1995.488968},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kennedy, Eberhart - 1995 - Particle swarm optimization(2).pdf:pdf},
isbn = {0-7803-2768-3},
keywords = {Artificial neural networks,Birds,Educational institutions,Genetic algorithms,Humans,Marine animals,Optimization methods,Particle swarm optimization,Performance evaluation,Testing,artificial intelligence,artificial life,evolution,genetic algorithms,multidimensional search,neural nets,neural network,nonlinear functions,optimization,particle swarm,search problems,simulation,social metaphor},
pages = {1942--1948},
publisher = {IEEE},
title = {{Particle swarm optimization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=488968},
volume = {4},
year = {1995}
}
@article{Labayrade2002a,
abstract = {Presents a road obstacle detection method able to cope with uphill and downhill gradients and dynamic pitching of the vehicle. Our approach is based on the construction and investigation of the "v-disparity" image which provides a good representation of the geometric content of the road scene. The advantage of this image is that it provides semi-global matching and is able to perform robust obstacle detection even in the case of partial occlusion or errors committed during the matching process. Furthermore, this detection is performed without any explicit extraction of coherent structures. This paper explains the construction of the "v-disparity" image, its main properties, and the obstacle detection method. The longitudinal profile of the road is estimated and the objects located above the road surface are then extracted as potential obstacles; subsequently, the accurate detection of road obstacles, in particular the position of tyre-road contact points is computed in a precise manner. The whole process is performed at frame rate with a current-day PC. Our experimental findings and comparisons with the results obtained using a flat geometry hypothesis show the benefits of our approach.},
author = {Labayrade, R. and Aubert, D. and Tarel, J.-P.},
doi = {10.1109/IVS.2002.1188024},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Labayrade, Aubert, Tarel - 2002 - Real time obstacle detection in stereovision on non flat road geometry through v-disparity represen(4).pdf:pdf},
isbn = {0-7803-7346-4},
issn = {19449224},
journal = {Intelligent Vehicle Symposium, 2002. IEEE},
keywords = {imaging and vision enhancement,niques which are sometimes,non flat road geometry,not reliable,poor quality of,real time processing,road obstacle detection,semi-global matching,stereoscopic,vision},
number = {January},
pages = {646--651},
pmid = {1188024},
title = {{Real time obstacle detection in stereovision on non flat road geometry through "v-disparity" representation}},
volume = {2},
year = {2002}
}
@article{lenet5,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day},
author = {Lecun, Y and Bottou, L and Bengio, Y and Haffner, P},
doi = {10.1109/5.726791},
institution = {Speech {\&} Image Process. Services Lab., AT{\&}T Bell Labs., Red Bank, NJ, USA},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {cnn,lenet-5},
month = {nov},
number = {11},
pages = {2278--2324},
publisher = {IEEE},
title = {{Gradient-based learning applied to document recognition}},
url = {http://dx.doi.org/10.1109/5.726791},
volume = {86},
year = {1998}
}
@article{Zhu2014,
author = {Zhu, Feng and Wang, Xiaogang and Yu, Nenghai},
doi = {10.1007/978-3-319-10599-4_10},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu, Wang, Yu - 2014 - Crowd tracking with dynamic evolution of group structures(2).pdf:pdf},
isbn = {9783319105987},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 6},
pages = {139--154},
title = {{Crowd tracking with dynamic evolution of group structures}},
volume = {8694 LNCS},
year = {2014}
}
@article{Dollar2012a,
abstract = {Pedestrian detection is a key problem in computer vision, with several applications that have the potential to positively impact quality of life. In recent years, the number of approaches to detecting pedestrians in monocular images has grown steadily. However, multiple data sets and widely varying evaluation protocols are used, making direct comparisons difficult. To address these shortcomings, we perform an extensive evaluation of the state of the art in a unified framework. We make three primary contributions: 1) We put together a large, well-annotated, and realistic monocular pedestrian detection data set and study the statistics of the size, position, and occlusion patterns of pedestrians in urban scenes, 2) we propose a refined per-frame evaluation methodology that allows us to carry out probing and informative comparisons, including measuring performance in relation to scale and occlusion, and 3) we evaluate the performance of sixteen pretrained state-of-the-art detectors across six data sets. Our study allows us to assess the state of the art and provides a framework for gauging future efforts. Our experiments show that despite significant progress, performance still has much room for improvement. In particular, detection is disappointing at low resolutions and for partially occluded pedestrians.},
author = {Doll{\'{a}}r, Piotr and Wojek, Christian and Schiele, Bernt and Perona, Pietro},
doi = {10.1109/TPAMI.2011.155},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doll{\'{a}}r et al. - 2012 - Pedestrian detection An evaluation of the state of the art(2).pdf:pdf},
isbn = {0769523722},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Caltech Pedestrian data set,Pedestrian detection,benchmark,data set,evaluation,object detection},
number = {4},
pages = {743--761},
pmid = {21808091},
title = {{Pedestrian detection: An evaluation of the state of the art}},
volume = {34},
year = {2012}
}
@inproceedings{stixelNet2015,
author = {Levi, Dan and Garnett, Noa and Fetaya, Ethan},
booktitle = {Proceedings of the British Machine Vision Conference (BMVC)},
doi = {10.5244/C.29.109},
editor = {{Xianghua Xie Mark W. Jones} and Tam, Gary K L},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levi, Garnett, Fetaya - 2015 - StixelNet A Deep Convolutional Network for Obstacle Detection and Road Segmentation(2).pdf:pdf},
isbn = {1-901725-53-7},
month = {sep},
pages = {109.1--109.12},
publisher = {BMVA Press},
title = {{StixelNet: A Deep Convolutional Network for Obstacle Detection and Road Segmentation}},
url = {https://dx.doi.org/10.5244/C.29.109},
year = {2015}
}
@misc{Zhang2016,
author = {Zhang, Shanshan and Benenson, Rodrigo and Omran, Mohamed and Hosang, Jan and Schiele, Bernt},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2016 - How Far are We from Solving Pedestrian Detection(2).pdf:pdf},
title = {{How Far are We from Solving Pedestrian Detection?}},
year = {2016}
}
@article{Benfold2011,
abstract = {The majority of existing pedestrian trackers concentrate on maintaining the identities of targets, however systems for remote biometric analysis or activity recognition in surveillance video often require stable bounding-boxes around pedestrians rather than approximate locations. We present a multi-target tracking system that is designed specifically for the provision of stable and accurate head location estimates. By performing data association over a sliding window of frames, we are able to correct many data association errors and fill in gaps where observations are missed. The approach is multi-threaded and combines asynchronous HOG detections with simultaneous KLT tracking and Markov-Chain Monte-Carlo Data Association (MCM-CDA) to provide guaranteed real-time tracking in high definition video. Where previous approaches have used ad-hoc models for data association, we use a more principled approach based on a Minimal Description Length (MDL) objective which accurately models the affinity between observations. We demonstrate by qualitative and quantitative evaluation that the system is capable of providing precise location estimates for large crowds of pedestrians in real-time. To facilitate future performance comparisons, we make a new dataset with hand annotated ground truth head locations publicly available.},
author = {Benfold, Ben and Reid, Ian},
doi = {10.1109/CVPR.2011.5995667},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Benfold, Reid - 2011 - Stable multi-target tracking in real-time surveillance video(2).pdf:pdf},
isbn = {9781457703942},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3457--3464},
title = {{Stable multi-target tracking in real-time surveillance video}},
year = {2011}
}
@article{Gavrila2004a,
author = {Gavrila, D. M. and Gavrila, D. M. and Giebel, J. and Munder, S.},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gavrila et al. - 2004 - Vision-based pedestrian detection The PROTECTOR system(2).pdf:pdf},
journal = {IN IEEE INTELLIGENT VEHICLES SYMPOSIUM},
pages = {13----18},
title = {{Vision-based pedestrian detection: The PROTECTOR system}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.97.9079},
year = {2004}
}
@article{Raphael2011,
author = {Raphael, Eric and Kiefer, Raymond and Reisman, Pini and Hayon, Gaby},
doi = {10.4271/2011-01-0579},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raphael et al. - 2011 - Development of a Camera-Based Forward Collision Alert System(2).pdf:pdf},
issn = {1946-4002},
journal = {SAE International Journal of Passenger Cars - Mechanical Systems},
number = {1},
pages = {2011--01--0579},
title = {{Development of a Camera-Based Forward Collision Alert System}},
url = {http://papers.sae.org/2011-01-0579/},
volume = {4},
year = {2011}
}
@article{Levinson2011,
abstract = {In order to achieve autonomous operation of a vehicle in urban situations with unpredictable traffic, several realtime systems must interoperate, including environment perception, localization, planning, and control. In addition, a robust vehicle platform with appropriate sensors, computational hardware, networking, and software infrastructure is essential.},
author = {Levinson, Jesse and Askeland, Jake and Becker, Jan and Dolson, Jennifer and Held, David and Kammel, Soeren and Kolter, J. Zico and Langer, Dirk and Pink, Oliver and Pratt, Vaughan and Sokolsky, Michael and Stanek, Ganymed and Stavens, David and Teichman, Alex and Werling, Moritz and Thrun, Sebastian},
doi = {10.1109/IVS.2011.5940562},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levinson et al. - 2011 - Towards fully autonomous driving Systems and algorithms(2).pdf:pdf},
isbn = {9781457708909},
issn = {1931-0587},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {Iv},
pages = {163--168},
title = {{Towards fully autonomous driving: Systems and algorithms}},
year = {2011}
}
@article{Torralba2006b,
abstract = {We consider the problem of detecting a large number of different classes of objects in cluttered scenes. We present a learning procedure, based on boosted decision stumps, that reduces the computational and sample complexity, by finding common features that can be shared across the classes (and/or views). Shared features, emerge in a model of object recognition trained to detect many object classes efficiently and robustly, and are preferred over class-specific features. Although that class-specific features achieve a more compact representation for a single category, the whole set of shared features is able to provide more efficient and robust representations when the system is trained to detect many object classes than the set of class-specific features. Classifiers based on shared features need less training data, since many classes share similar features (e.g., computer screens and posters can both be distinguished from the background by looking for the feature “edges in a rectangular arrangement”).},
author = {Torralba, a and Murphy, K P and Freeman, W T},
doi = {10.1007/11957959},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Torralba, Murphy, Freeman - 2006 - Shared Features for Multiclass Object Detection(2).pdf:pdf},
isbn = {978-3-540-68794-8},
journal = {Toward Category-Level Object Recognition},
number = {Lecture Notes in Computer Science},
pages = {345--361},
title = {{Shared Features for Multiclass Object Detection}},
volume = {4170},
year = {2006}
}
@article{Stein,
author = {Stein, Gideon P and Mano, Ofer},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stein, Mano - Unknown - Vision based ACC with a Single Camera Bounds on Range and Range Rate Accuracy.pdf(2).pdf:pdf},
title = {{Vision based ACC with a Single Camera Bounds on Range and Range Rate Accuracy.pdf}}
}
@article{Grisleri2010,
author = {Grisleri, Paolo and Fedriga, Isabella},
doi = {10.3182/20100906-3-IT-2019.00086},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grisleri, Fedriga - 2010 - The BRAiVE Autonomous Ground Vehicle Platform(2).pdf:pdf},
issn = {14746670},
journal = {IFAC Proceedings Volumes},
number = {16},
pages = {497--502},
title = {{The BRAiVE Autonomous Ground Vehicle Platform}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1474667016351060},
volume = {43},
year = {2010}
}
@article{Enzweiler2011,
author = {Enzweiler, M. and Gavrila, D. M.},
doi = {10.1109/TIP.2011.2142006},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Enzweiler, Gavrila - 2011 - A Multilevel Mixture-of-Experts Framework for Pedestrian Classification(2).pdf:pdf},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
keywords = {Automated,Cameras,Chamfer shape matching,Cluster Analysis,Computational modeling,Computer-Assisted,Fuzzy Logic,HOG,Humans,Image Processing,LBP,MLP,Mixture-of-experts,Optical imaging,Pattern Recognition,Pixel,Shape,Support Vector Machines,Support vector machines,Training,depth,flow,histograms of oriented gradients,image classification,image intensity,image matching,linSVM,linear support vector machines,local binary patterns,multilayer perceptrons,multilevel mixture-of-experts framework,multimodality dataset,object detection,pedestrian classification,pedestrian recognition,shape recognition,support vector machines,traffic engineering computing},
month = {oct},
number = {10},
pages = {2967--2979},
publisher = {IEEE},
title = {{A Multilevel Mixture-of-Experts Framework for Pedestrian Classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5749283},
volume = {20},
year = {2011}
}
@article{Ziegler2014,
abstract = {125 years after Bertha Benz completed the first overland journey in automotive history, the Mercedes Benz S-Class S 500 INTELLIGENT DRIVE followed the same route from Mannheim to Pforzheim, Germany, in fully autonomous manner. The autonomous vehicle was equipped with close-to-production sensor hardware and relied solely on vision and radar sensors in combination with accurate digital maps to obtain a comprehensive understanding of complex traffic situations. The historic Bertha Benz Memorial Route is particularly challenging for autonomous driving. The course taken by the autonomous vehicle had a length of 103 km and covered rural roads, 23 small villages and major cities (e.g. downtown Mannheim and Heidelberg). The route posed a large variety of difficult traffic scenarios including intersections with and without traffic lights, roundabouts, and narrow passages with oncoming traffic. This paper gives an overview of the autonomous vehicle and presents details on vision and radar-based perception, digital road maps and video-based self-localization, as well as motion planning in complex urban scenarios.},
author = {Ziegler, Julius and Bender, Philipp and Schreiber, Markus and Lategahn, Henning and Strauss, Tobias and Stiller, Christoph and Dang, Thao and Franke, Uwe and Appenrodt, Nils and Keller, Christoph G. and Kaus, Eberhard and Herrtwich, Ralf G. and Rabe, Clemens and Pfeiffer, David and Lindner, Frank and Stein, Fridtjof and Erbs, Friedrich and Enzweiler, Markus and Knoppel, Carsten and Hipp, Jochen and Haueis, Martin and Trepte, Maximilian and Brenk, Carsten and Tamke, Andreas and Ghanaat, Mohammad and Braun, Markus and Joos, Armin and Fritz, Hans and Mock, Horst and Hein, Martin and Zeeb, Eberhard},
doi = {10.1109/MITS.2014.2306552},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ziegler et al. - 2014 - Making bertha drive-an autonomous journey on a historic route(2).pdf:pdf},
isbn = {1939-1390},
issn = {15249050},
journal = {IEEE Intelligent Transportation Systems Magazine},
number = {2},
pages = {8--20},
title = {{Making bertha drive-an autonomous journey on a historic route}},
volume = {6},
year = {2014}
}
@article{Zhao2009,
abstract = {Ground plane detection plays an important role in stereo vision based obstacle detection methods. Recently, V-disparity image has been widely used for ground plane detection. The existing approach based on V-disparity image can detect flat ground successfully but have difficulty in detecting non-flat ground. In this paper, we discuss the representation of non-flat ground in V-disparity image, based on which we propose a method to detect non-flat ground using V-disparity image.},
author = {Zhao, Jun and Whitty, Mark and Katupitiya, Jayantha},
doi = {10.1109/IROS.2009.5354207},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Whitty, Katupitiya - 2009 - Detection of non-flat ground surfaces using V-disparity images(2).pdf:pdf},
isbn = {9781424438044},
journal = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009},
keywords = {Correlation,Stereo vision,V-disparity image},
pages = {4584--4589},
title = {{Detection of non-flat ground surfaces using V-disparity images}},
year = {2009}
}
@inproceedings{Cho2010,
abstract = {Bicycles that share the road with intelligent vehicles present particular challenges for automated perception systems. Bicycle detection is important because bicycles share the road with vehicles and can move at comparable speeds in urban environments. From a computer vision standpoint, bicycle detection is challenging as bicycle's appearance can change dramatically between viewpoints and a person riding on the bicycle is a non-rigid object. In this paper, we present a vision-based framework to detect and track bicycles that takes into account these issues. A mixture model of multiple viewpoints is defined and trained via a Support Vector Machine (SVM) to detect bicycles under a variety of circumstances. Each component of the model uses a part-based representation and known geometric context is used to improve overall detection efficiency. An extended Kalman filter (EKF) is used to estimate the position and velocity of the bicycle in vehicle coordinates. We demonstrate the effectiveness of this approach through a series of experiments run on video data of moving bicycles captured from a vehicle-mounted camera.},
author = {Cho, Hyunggi and Rybski, Paul E. and Zhang, Wende},
booktitle = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
doi = {10.1109/ITSC.2010.5624993},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho, Rybski, Zhang - 2010 - Vision-based bicycle detection and tracking using a deformable part model and an EKF algorithm(2).pdf:pdf},
isbn = {9781424476572},
issn = {2153-0009},
keywords = {cyclist},
mendeley-tags = {cyclist},
month = {sep},
pages = {1875--1880},
publisher = {IEEE},
title = {{Vision-based bicycle detection and tracking using a deformable part model and an EKF algorithm}},
url = {http://ieeexplore.ieee.org/document/5624993/},
year = {2010}
}
@misc{talos2007,
author = {Talos},
title = {{Talos at DARPA}},
url = {https://www.ll.mit.edu/publications/labnotes/automation.html},
year = {2007}
}
@article{Hong2014,
author = {Hong, Zhibin and Wang, Chaohui and Mei, Xue and Prokhorov, Danil and Tao, Dacheng},
doi = {10.1007/978-3-319-10599-4_11},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hong et al. - 2014 - Tracking Using Multilevel Quantizations(2).pdf:pdf},
isbn = {978-3-319-10598-7},
issn = {16113349},
journal = {Eccv},
keywords = {conditional random fields,multilevel quantizations,non-rigid object tracking,online random forests,tracking},
pages = {155--171},
title = {{Tracking Using Multilevel Quantizations}},
url = {http://link.springer.com/10.1007/978-3-319-10599-4{\%}5Cnhttp://link.springer.com/10.1007/978-3-319-10599-4{\_}11},
volume = {8694},
year = {2014}
}
@inproceedings{Tian2015a,
author = {Tian, Wei and Lauer, Martin},
booktitle = {2015 IEEE 18th International Conference on Intelligent Transportation Systems},
doi = {10.1109/ITSC.2015.211},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tian, Lauer - 2015 - Fast Cyclist Detection by Cascaded Detector and Geometric Constraint(2).pdf:pdf},
isbn = {978-1-4673-6596-3},
keywords = {cyclist},
mendeley-tags = {cyclist},
month = {sep},
pages = {1286--1291},
publisher = {IEEE},
title = {{Fast Cyclist Detection by Cascaded Detector and Geometric Constraint}},
url = {http://ieeexplore.ieee.org/document/7313303/},
year = {2015}
}
@inproceedings{Dragon2014,
author = {Dragon, Ralf and Gool, Luc Van},
booktitle = {Cvpr2014},
doi = {10.1109/CVPR.2014.442},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dragon, Gool - 2014 - Ground Plane Estimation using a Hidden Markov Model(2).pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
pages = {4026--4033},
title = {{Ground Plane Estimation using a Hidden Markov Model}},
year = {2014}
}
@misc{mobileye,
author = {MOBILEYE},
title = {{MobileEye Vision Technology - Pedestrian Detection}},
url = {http://www.mobileye.com},
year = {1999}
}
@article{Huval2015,
abstract = {Numerous groups have applied a variety of deep learning techniques to computer vision problems in highway perception scenarios. In this paper, we presented a number of empirical evaluations of recent deep learning advances. Computer vision, combined with deep learning, has the potential to bring about a relatively inexpensive, robust solution to autonomous driving. To prepare deep learning for industry uptake and practical applications, neural networks will require large data sets that represent all possible driving environments and scenarios. We collect a large data set of highway data and apply deep learning and computer vision algorithms to problems such as car and lane detection. We show how existing convolutional neural networks (CNNs) can be used to perform lane and vehicle detection while running at frame rates required for a real-time system. Our results lend credence to the hypothesis that deep learning holds promise for autonomous driving.},
archivePrefix = {arXiv},
arxivId = {1504.01716},
author = {Huval, Brody and Wang, Tao and Tandon, Sameep and Kiske, Jeff and Song, Will and Pazhayampallil, Joel and Andriluka, Mykhaylo and Rajpurkar, Pranav and Migimatsu, Toki and Cheng-Yue, Royce and Mujica, Fernando and Coates, Adam and Ng, Andrew Y},
eprint = {1504.01716},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huval et al. - 2015 - An Empirical Evaluation of Deep Learning on Highway Driving(2).pdf:pdf},
journal = {arXiv},
pages = {1--7},
title = {{An Empirical Evaluation of Deep Learning on Highway Driving}},
url = {http://arxiv.org/abs/1504.01716},
year = {2015}
}
@article{Wu2012,
abstract = {We present a novel framework for multiple object tracking in which the problems of object detection and data association are expressed by a single objective function. The framework follows the Lagrange dual decomposition strategy, taking advantage of the often complementary nature of the two subproblems. Our coupling formulation avoids the problem of error propagation from which traditional “detection-tracking approaches” to multiple object tracking suffer. We also eschew common heuristics such as “nonmaximum suppression” of hypotheses by modeling the joint image likelihood as opposed to applying independent likelihood assumptions. Our coupling algorithm is guaranteed to converge and can handle partial or even complete occlusions. Furthermore, our method does not have any severe scalability issues but can process hundreds of frames at the same time. Our experiments involve challenging, notably distinct datasets and demonstrate that our method can achieve results comparable to those of state-of-art approaches, even without a heavily trained object detector.},
author = {Wu, Zheng and Thangali, Ashwin and Sclaroff, Stan and Betke, Margrit},
doi = {10.1109/CVPR.2012.6247896},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2012 - Coupling detection and data association for multiple object tracking(2).pdf:pdf},
isbn = {978-1-4673-1228-8},
issn = {1063-6919},
journal = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
keywords = {Couplings,Detectors,Dictionaries,Joints,Lagrange dual decomposition strategy,Markov processes,Minimization,Object detection,coupling detection,data association,error propagation,joint image likelihood,multiple object tracking,object tracking,sensor fusion},
pages = {1948--1955},
title = {{Coupling detection and data association for multiple object tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6247896},
year = {2012}
}
@article{Kalal2012,
abstract = {This paper investigates long-term tracking of unknown objects in a video stream. The object is defined by its location and extent in a single frame. In every frame that follows, the task is to determine the object's location and extent or indicate that the object is not present. We propose a novel tracking framework (TLD) that explicitly decomposes the long-term tracking task into tracking, learning and detection. The tracker follows the object from frame to frame. The detector localizes all appearances that have been observed so far and corrects the tracker if necessary. The learning estimates detector's errors and updates it to avoid these errors in the future. We study how to identify detector's errors and learn from them. We develop a novel learning method (P-N learning) which estimates the errors by a pair of "experts'': (i) P-expert estimates missed detections, and (ii) N-expert estimates false alarms. The learning process is modeled as a discrete dynamical system and the conditions under which the learning guarantees improvement are found. We describe our real-time implementation of the TLD framework and the P-N learning. We carry out an extensive quantitative evaluation which shows a significant improvement over state-of-the-art approaches.},
author = {Kalal, Zdenek and Mikolajczyk, Krystian and Matas, Jiri},
doi = {10.1109/TPAMI.2011.239},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalal, Mikolajczyk, Matas - 2012 - Tracking-learning-detection(2).pdf:pdf},
isbn = {2011030153},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Long-term tracking,bootstrapping,learning from video,real time,semi-supervised learning},
number = {7},
pages = {1409--1422},
pmid = {22156098},
title = {{Tracking-learning-detection}},
volume = {34},
year = {2012}
}
@article{Lin2014,
abstract = {Hashing has proven a valuable tool for large-scale information retrieval. Despite much success, existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest---multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures. The resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. To solve the StructHash optimization problem, we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.},
archivePrefix = {arXiv},
arxivId = {1407.1151},
author = {Lin, Guosheng and Shen, Chunhua and Wu, Jianxin},
doi = {10.1007/978-3-319-10578-9_40},
eprint = {1407.1151},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin, Shen, Wu - 2014 - Optimizing ranking measures for compact binary code learning(2).pdf:pdf},
isbn = {9783319105772},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 3},
pages = {613--627},
title = {{Optimizing ranking measures for compact binary code learning}},
volume = {8691 LNCS},
year = {2014}
}
@article{Walk2010,
abstract = {Pedestrian detection is an important problem in computer vision due to its importance for applications such as visual surveillance, robotics, and automotive safety. This paper pushes the state-of-the-art of pedestrian detection in two ways. First, we},
author = {Walk, Stefan and Schindler, Konrad and Schiele, Bernt},
doi = {10.1007/978-3-642-15567-3_14},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Walk, Schindler, Schiele - 2010 - Disparity statistics for pedestrian detection Combining appearance, motion and stereo(2).pdf:pdf},
isbn = {3642155669},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 6},
pages = {182--195},
title = {{Disparity statistics for pedestrian detection: Combining appearance, motion and stereo}},
volume = {6316 LNCS},
year = {2010}
}
@inproceedings{Tang2015,
author = {Tang, Siyu and Andres, Bjoern and Andriluka, Mykhaylo and Schiele, Bernt},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7299138},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2015 - Subgraph decomposition for multi-target tracking(2).pdf:pdf},
isbn = {978-1-4673-6964-0},
keywords = {Analytical models,Image edge detection,Joining processes,Linear programming,Optimization,Random variables,Target tracking,computer vision,disjoint path constraints,graph theory,hard optimization problem,minimum cost subgraph multicut problem,multitarget tracking,optimal links,optimisation,subgraph decomposition,target tracking},
month = {jun},
pages = {5033--5041},
publisher = {IEEE},
title = {{Subgraph decomposition for multi-target tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7299138},
year = {2015}
}
@article{Zhang2015a,
author = {Zhang, Shanshan and Klein, Dominik A. and Bauckhage, Christian and Cremers, Armin B.},
doi = {10.1007/s11042-015-2571-z},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2015 - Fast moving pedestrian detection based on motion segmentation and new motion features(2).pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Histograms of oriented gradients,Motion segmentation,Motion self difference features,Pedestrian detection,Support vector machines},
title = {{Fast moving pedestrian detection based on motion segmentation and new motion features}},
year = {2015}
}
@incollection{Choi2010,
abstract = {Tracking multiple objects is important in many application domains. We propose a novel algorithm for multi-object tracking that is capable of working under very challenging conditions such as minimal hardware equipment, uncalibrated monocular camera, occlusions and severe background clutter. To address this problem we propose a new method that jointly estimates object tracks, estimates corresponding 2D/3D temporal trajectories in the camera reference system as well as estimates the model parameters (pose, focal length, etc) within a coherent probabilistic formulation. Since our goal is to estimate stable and robust tracks that can be univocally associated to the object IDs, we propose to include in our formulation an interaction (attraction and repulsion) model that is able to model multiple 2D/3D trajectories in space-time and handle situations where objects occlude each other. We use a MCMC particle filtering algorithm for parameter inference and propose a solution that enables accurate and efficient tracking and camera model estimation. Qualitative and quantitative experimental results obtained using our own dataset and the publicly available ETH dataset shows very promising tracking and camera estimation results.},
author = {Choi, Wongun and Savarese, Silvio},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15561-1_40},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Savarese - 2010 - Multiple Target Tracking in World Coordinate with Single, Minimally Calibrated Camera(2).pdf:pdf},
isbn = {364215560X},
issn = {03029743},
number = {PART 4},
pages = {553--567},
title = {{Multiple Target Tracking in World Coordinate with Single, Minimally Calibrated Camera}},
url = {http://link.springer.com/10.1007/978-3-642-15561-1{\_}40},
volume = {6314 LNCS},
year = {2010}
}
@article{Zhao2017,
archivePrefix = {arXiv},
arxivId = {1704.08545},
author = {Zhao, Hengshuang and Qi, Xiaojuan and Shen, Xiaoyong and Shi, Jianping and Jia, Jiaya},
eprint = {1704.08545},
file = {:home/songlin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2017 - ICNet for Real-Time Semantic Segmentation on High-Resolution Images(2).pdf:pdf},
journal = {CoRR},
title = {{ICNet for Real-Time Semantic Segmentation on High-Resolution Images}},
url = {http://arxiv.org/abs/1704.08545},
volume = {abs/1704.0},
year = {2017}
}
@article{Sivak1996,
abstract = {The literature contains numerous claims that 90{\%} of all the information used in driving is visual. This article presents a theoretical discussion, a citation search, and a review of evidence concerning such claims. The findings indicate that not only do we lack data from which to derive an accurate numerical estimate, but we lack a measurement system within which any numerical estimate would be meaningful. Consequently, although the information relevant to driving is likely to be predominantly visual, any claims about the precise percentage attributable to vision are premature. The proliferation of such claims in the absence of direct evidence is a reminder that researchers should be careful about assuring the validity of the claims they are passing on.},
annote = {PMID: 8983048},
author = {Sivak, Michael},
doi = {10.1068/p251081},
journal = {Perception},
number = {9},
pages = {1081--1089},
title = {{The Information That Drivers Use: Is it Indeed 90{\%} Visual?}},
url = {https://doi.org/10.1068/p251081},
volume = {25},
year = {1996}
}
@article{Werling2010OptimalTG,
author = {Werling, Moritz and Ziegler, Julius and Kammel, S{\"{o}}ren and Thrun, Sebastian},
file = {::},
journal = {2010 IEEE International Conference on Robotics and Automation},
pages = {987--993},
title = {{Optimal trajectory generation for dynamic street scenarios in a Fren{\'{e}}t Frame}},
year = {2010}
}
@article{7490340,
author = {Paden, B and {\v{C}}{\'{a}}p, M and Yong, S Z and Yershov, D and Frazzoli, E},
doi = {10.1109/TIV.2016.2578706},
file = {::},
issn = {2379-8904},
journal = {IEEE Transactions on Intelligent Vehicles},
keywords = {Autonomous automobiles,Decision making,Feedback control,Intelligent vehicles,Motion planning,Path planning,Trajectory,automotive transportation accessibility,automotive transportation efficiency,control algorithms,dynamic environment,feedback,feedback control,mobile robots,motion control,motion planning,path planning,pedestrians,road safety,road vehicles,safety enhancement,safety-critical tasks,self-driving urban vehicles,vehicle mobility model},
number = {1},
pages = {33--55},
title = {{A Survey of Motion Planning and Control Techniques for Self-Driving Urban Vehicles}},
volume = {1},
year = {2016}
}
